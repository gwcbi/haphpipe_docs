{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About HAPHPIPE HA plotype and PH ylodynamics pipeline for viral assembly, population genetics, and phylodynamics. In this User Guide, we assume basic familiarity with conda environments, basic bash knowledge and knowledge of general next-generation sequencing (NGS) concepts. For more information regarding all of these, see helpful links and FAQ . Remember that a directory is a simply a folder nesting system, similar to what you see on your computer. Where you see \"folder\" below you can also say \"directory.\" HAPHPIPE is intended only for Linux and Mac OS X platforms. If you are a Windows user, see this section on our install page . This User Guide was developed jointly by undergraduate and graduate students to be accessible for users at all stages. Margaret C. Steiner, Keylie M. Gibson, Matthew L. Bendall, Uzma Rentia, and Pengfei Li all contributed to developing this User Guide and testing HAPHPIPE. See our protocol paper (link coming soon) for more information and this paper (link coming soon) for our validation study. Citing HAPHPIPE When using HAPHPIPE, please cite our article when it is available. For now, reference the GitHub website: https://github.com/gwcbi/haphpipe The HAPHPIPE suite Each stage can be run on its own. Stages are grouped into 4 categories: hp_reads , hp_assemble , hp_haplotype , and hp_annotate . More detailed description of command line options for each stage are available in the below sections. To view all available stages in HAPHPIPE, run: haphpipe -h Output will look like: Program: haphpipe (haplotype and phylodynamics pipeline) Version: 0.8.1 Commands: -- Reads sample_reads subsample reads using seqtk trim_reads trim reads using Trimmomatic join_reads join reads using FLASh ec_reads error correct reads using SPAdes -- Assemble assemble_denovo assemble reads denovo assemble_amplicons assemble contigs to amplicon regions assemble_scaffold assemble contigs to genome align_reads align reads to reference call_variants call variants vcf_to_consensus create consensus sequence from VCF refine_assembly iterative refinement: align - variants - consensus finalize_assembly finalize consensus sequence -- Haplotype predict_haplo assemble haplotypes with PredictHaplo ph_parser parse output from PredictHaplo. -- Annotate pairwise_align align consensus to an annotated reference extract_pairwise extract sequence regions from pairwise alignment annotate_from_ref annotate consensus from reference annotation -- Miscellaneous demo setup demo directory and test data HAPHPIPE consists of a suite of sub-commands under each stage that are invoked as follows: haphpipe [stage] [sub-command] [options] For example, to join paired end reads, one would invoke the following: haphpipe join_reads --fq1 trimmed_1.fastq --fq2 trimmed_2.fastq","title":"Home"},{"location":"#about-haphpipe","text":"HA plotype and PH ylodynamics pipeline for viral assembly, population genetics, and phylodynamics. In this User Guide, we assume basic familiarity with conda environments, basic bash knowledge and knowledge of general next-generation sequencing (NGS) concepts. For more information regarding all of these, see helpful links and FAQ . Remember that a directory is a simply a folder nesting system, similar to what you see on your computer. Where you see \"folder\" below you can also say \"directory.\" HAPHPIPE is intended only for Linux and Mac OS X platforms. If you are a Windows user, see this section on our install page . This User Guide was developed jointly by undergraduate and graduate students to be accessible for users at all stages. Margaret C. Steiner, Keylie M. Gibson, Matthew L. Bendall, Uzma Rentia, and Pengfei Li all contributed to developing this User Guide and testing HAPHPIPE. See our protocol paper (link coming soon) for more information and this paper (link coming soon) for our validation study.","title":"About HAPHPIPE"},{"location":"#citing-haphpipe","text":"When using HAPHPIPE, please cite our article when it is available. For now, reference the GitHub website: https://github.com/gwcbi/haphpipe","title":"Citing HAPHPIPE"},{"location":"#the-haphpipe-suite","text":"Each stage can be run on its own. Stages are grouped into 4 categories: hp_reads , hp_assemble , hp_haplotype , and hp_annotate . More detailed description of command line options for each stage are available in the below sections. To view all available stages in HAPHPIPE, run: haphpipe -h Output will look like: Program: haphpipe (haplotype and phylodynamics pipeline) Version: 0.8.1 Commands: -- Reads sample_reads subsample reads using seqtk trim_reads trim reads using Trimmomatic join_reads join reads using FLASh ec_reads error correct reads using SPAdes -- Assemble assemble_denovo assemble reads denovo assemble_amplicons assemble contigs to amplicon regions assemble_scaffold assemble contigs to genome align_reads align reads to reference call_variants call variants vcf_to_consensus create consensus sequence from VCF refine_assembly iterative refinement: align - variants - consensus finalize_assembly finalize consensus sequence -- Haplotype predict_haplo assemble haplotypes with PredictHaplo ph_parser parse output from PredictHaplo. -- Annotate pairwise_align align consensus to an annotated reference extract_pairwise extract sequence regions from pairwise alignment annotate_from_ref annotate consensus from reference annotation -- Miscellaneous demo setup demo directory and test data HAPHPIPE consists of a suite of sub-commands under each stage that are invoked as follows: haphpipe [stage] [sub-command] [options] For example, to join paired end reads, one would invoke the following: haphpipe join_reads --fq1 trimmed_1.fastq --fq2 trimmed_2.fastq","title":"The HAPHPIPE suite"},{"location":"adv/","text":"Making your own pipelines with Bash This is an example of how to make your own pipeline with bash. This example uses four NGS samples for COVID-19 from NCBI and puts them through whole genome de novo assembly. SRA accession numbers: SRR11140744 SRR11140746 SRR11140748 SRR11140750 Step 0 - Obtaining samples. We will assume that you have downloaded the reads from NCBI or know how to use fastq-dump. We will show the fastq-dump command here that we used to download the files. This part is not included in the pipeline. for sra in SRR11140744 SRR11140746 SRR11140748 SRR11140750; do fastq-dump --outdir ${sra} --split-files --origfmt ${sra} done Output is: Read 503344 spots for SRR11140744 Written 503344 spots for SRR11140744 Read 358971 spots for SRR11140746 Written 358971 spots for SRR11140746 Read 421395 spots for SRR11140748 Written 421395 spots for SRR11140748 Read 17657 spots for SRR11140750 Written 17657 spots for SRR11140750 You're starting directory should look like for this example, whether the reads were obtained manually or downloaded with fastq-dump: . \u251c\u2500\u2500 SRR11140744 | \u251c\u2500\u2500 SRR11140744_1.fastq | \u2514\u2500\u2500 SRR11140744_2.fastq \u251c\u2500\u2500 SRR11140746 | \u251c\u2500\u2500 SRR11140746_1.fastq | \u2514\u2500\u2500 SRR11140746_2.fastq \u251c\u2500\u2500 SRR11140748 | \u251c\u2500\u2500 SRR11140748_1.fastq | \u2514\u2500\u2500 SRR11140748_2.fastq \u2514\u2500\u2500 SRR11140750 \u251c\u2500\u2500 SRR11140750_1.fastq \u2514\u2500\u2500 SRR11140750_2.fastq We also know we will need a reference genome to help scaffold the contigs. We have downloaded the COVID19 reference genome here as a FASTA file. Step 1 - Evaluate which modules you want to use. View all the module options using haphpipe -h . We have decided that we want to sample the reads ( sample_reads ), trim the reads ( trim_reads ), error correct the reads ( ec_reads ). We want to do genome de novo assembly, so we want to use assemble_denovo and assemble_scaffold . We then want to do refinement of the assembly ( refine_assembly ) and finalize the assembly ( finalize_assembly ). Step 2 - Document files and options needed for each module. As we go through each module, we will take note of what we need and which input is specific for an individual sample (i.e., the input fastq reads will be different per sample, but we probably want the same number of reads for each sample.) We know all modules have the option for a logfile. We'll include that in the bash scripting, but don't need to make notes of a logfile for each module. -- Part 2A - Sample reads. Upon viewing the haphpipe sample_reads -h we see that we need to provide fastq reads 1 and 2, an output directory, and number of reads desired. Sample specific options: fastq read 1 fastq read 2 output directory Not sample specific options: number of reads desired -- Part 2B - Trim reads. Upon viewing the haphpipe trim_reads -h we see that we need to provide fastq reads 1 and 2, an output directory, number of reads, and number of CPUs desired. There is an option to change the timming commands, but we'll keep default. Sample specific options: fastq read 1 -- we will want these to be the subsampled read file fastq read 2 -- we will want these to be the subsampled read file output directory Not sample specific options: number of reads desired ncpu -- Part 2C - Error correct reads. Upon viewing the haphpipe ec_reads -h we see that we need to provide fastq reads 1 and 2, an output directory, and number of CPUs desired. Sample specific options: fastq read 1 -- we will want these to be the trimmed read file fastq read 2 -- we will want these to be the trimmed read file output directory Not sample specific options: ncpu -- Part 2D - De novo assembly. Upon viewing the haphpipe assemble_denovo -h we see that we need to provide fastq reads 1 and 2, an output directory, and number of CPUs desired. Because we previously completed error correction. We will not want to include it here (i.e., we will invoke the option command --no_error_correction ). Sample specific options: fastq read 1 -- we will want these to be the error corrected reads fastq read 2 -- we will want these to be the error corrected reads output directory Not sample specific options: ncpu no error correction option -- Part 2E - Assemble scaffold. Upon viewing the haphpipe assemble_scaffold -h we see that we need to provide fasta file containing assembled contigs, an output directory, name to append to scaffold sequence and reference fasta desired. There is an option to change the timming commands, but we'll keep default. Sample specific options: assembled contigs file output directory sequence name Not sample specific options: reference fasta -- Part 2F - Refine assembly. Upon viewing the haphpipe refine_assembly -h we see that we need to provide fastq reads 1 and 2, an output directory, a reference sequence to refine, a sample ID and a maximum number of refinement steps, and number of CPUs desired. Sample specific options: fastq read 1 -- we will want these to be the error corrected reads fastq read 2 -- we will want these to be the error corrected reads output directory reference fasta -- we will want this to be the sample's assembled scaffold Sample ID Not sample specific options: ncpu maximum number of refinement steps (we'll do 3 for sake of simplicity and time) -- Part 2G - Finalize assembly. Upon viewing the haphpipe finalize_assembly -h we see that we need to provide fastq reads 1 and 2, an output directory, a reference sequence to finalize, a sample ID and a maximum number of refinement steps, and number of CPUs desired. We could replace the preset bowtie2 option, but we will leave it as default for this sample pipeline. Sample specific options: fastq read 1 -- we will want these to be the error corrected reads fastq read 2 -- we will want these to be the error corrected reads output directory reference fasta -- we will want this to be the sample's refined fasta sequence Sample ID Not sample specific options: ncpu -- Part 2H - Gather the needed initial input for each sample. We need to gather the necessary files that we will need to input for each sample so that we can make a script that takes in input files. We know from the User Guide that we can look at the output/input file types and names here . Because each module has a standard output file name, we will only need to be specific about the input for the raw fastq files for each sample. Therefore we need: input raw fastq read 1 input raw fastq read 2 reference genome to scaffold against (this will be a covid19 reference) output directory for each sample sample name The other options we can code into the bash script for each module, since they will be the same for every sample in this analysis. Step 3 - Create a bash script for each module. Now we will format a bash script for each module. -- Part 3A - Input options. Because we have a list of needed input options (specified by the user), we need to make a bash command to take in the inputs. First, we will specify the script name. We can do this one of two ways. i) explicitly or 2) through a command. i) SN='covid_genome' # this sets the script name (SN variable) to covid_genome ii) SN=$(basename $0) # this sets the script name (SN variable) to whatever the script filename is. If the script's file name is covid.sh then it is set as that. If the file name is this_is_file it will be set as that. Because we are in charge of this script, we will explicitly set it. Second, we want to set some input information for the user. We can do this as such: read -r -d '' USAGE EOF USAGE: $SN [read1] [read2] [reference_fasta] [samp_id] outdir ----- COVID19 Genome Assembly Pipeline ----- This pipeline implements genome assembly using a denovo approach. Reads are error-corrected and used to refine the scaffolded assembly, with up to 3 refinement steps. This pipeline is used as an example for advanced usage - making own pipeline in the User Guide. Input: read1: Fastq file for read 1. May be compressed (.gz) read2: Fastq file for read 2. May be compressed (.gz) reference_fasta: Reference sequence (fasta) samp_id: Sample ID outdir: Output directory (default is [sample_dir]/$SN) EOF Third, we want to provide help information for the script. We can do that using the avoce code, which has been saved in the variable $USAGE . Therefore, #--- Read command line args and if arg is -h, provide the usage information [[ -n $1 ]] [[ $1 == '-h' ]] echo $USAGE exit 0 If pipeline.sh -h is invoked here, then the output is: USAGE: covid_genome_assembly [read1] [read2] [reference_fasta] [samp_id] outdir ----- COVID19 Genome Assembly Pipeline ----- This pipeline implements genome assembly using a denovo approach. Reads are error-corrected and used to refine the scaffolded assembly, with up to 3 refinement steps. This pipeline is used as an example for advanced usage - making own pipeline in the User Guide. Input: read1: Fastq file for read 1. May be compressed (.gz) read2: Fastq file for read 2. May be compressed (.gz) reference_fasta: Reference sequence (fasta) samp_id: Sample ID outdir: Output directory (default is [sample_dir]/covid_genome_assembly) Note that this output looks identical to the information we put into $USAGE variable above in the second part of this section. Fourth, we want to read in the command with the provided input options. Because we are using a bash script, the position of the input files is imparative. #--- Read command line args [[ -n $1 ]] raw1= $1 [[ -n $2 ]] raw2= $2 [[ -n $3 ]] refFA= $3 [[ -n $4 ]] sampid= $4 [[ -n $5 ]] outdir= $5 Fifth, we want to check that the input files are provided and are not empty. We also want to set the outdirectory. #--- Check that files are provided and exist [[ -z ${raw1+x} ]] echo FAILED: read1 is not set echo $USAGE exit 1 [[ ! -e $raw1 ]] echo [---$SN---] ($(date)) FAILED: file $raw1 does not exist exit 1 [[ -z ${raw2+x} ]] echo FAILED: read2 is not set echo $USAGE exit 1 [[ ! -e $raw2 ]] echo [---$SN---] ($(date)) FAILED: file $raw2 does not exist exit 1 [[ -z ${refFA+x} ]] echo FAILED: refFA is not set echo $USAGE exit 1 [[ ! -e $refFA ]] echo [---$SN---] ($(date)) FAILED: file $refFA does not exist exit 1 [[ -z ${sampid+x} ]] echo FAILED: sampid is not set echo $USAGE exit 1 #--- Set outdirectory [[ -z ${outdir+x} ]] outdir=$(dirname $raw1)/$SN mkdir -p $outdir Sixth, we want to set the number of CPUs to use throughout the script. #--- Determine CPUs to use # First examines NCPU environment variable, then nproc, finally sets to 1 [[ -n $NCPU ]] ncpu=$NCPU [[ -z $ncpu ]] ncpu=$(nproc 2 /dev/null) [[ -z $ncpu ]] ncpu=1 Seventh, we want to print out the variables and files. echo [---$SN---] ($(date)) read1: $raw1 echo [---$SN---] ($(date)) read2: $raw2 echo [---$SN---] ($(date)) reference_fasta: $refFA echo [---$SN---] ($(date)) samp_id: $sampid echo [---$SN---] ($(date)) outdir: $outdir echo [---$SN---] ($(date)) num CPU: $ncpu Finally, because here at GWU CBI, we like to time everything, we include a line of code to start a timer and end the timer: #--- Start the timer t1=$(date + %s ) #### Put haphpipe module scripts here #---Complete job t2=$(date + %s ) diff=$(($t2-$t1)) echo [---$SN---] ($(date)) $(($diff / 60)) minutes and $(($diff % 60)) seconds elapsed. echo [---$SN---] ($(date)) $SN COMPLETE. After Part 3A, our pipeline file should read as such: #!/usr/bin/env bash ############################################################################### # This pipeline implements genome assembly using a denovo approach. Reads are # error-corrected and used to refine the scaffolded assembly, with up to 3 # refinement steps. This pipeline is used as an example for advanced usage # - making own pipeline in the User Guide. ############################################################################### SN='covid_genome_assembly' read -r -d '' USAGE EOF USAGE: $SN [read1] [read2] [reference_fasta] [samp_id] outdir ----- COVID19 Genome Assembly Pipeline ----- This pipeline implements genome assembly using a denovo approach. Reads are error-corrected and used to refine the scaffolded assembly, with up to 3 refinement steps. This pipeline is used as an example for advanced usage - making own pipeline in the User Guide. Input: read1: Fastq file for read 1. May be compressed (.gz) read2: Fastq file for read 2. May be compressed (.gz) reference_fasta: Reference sequence (fasta) samp_id: Sample ID outdir: Output directory (default is [sample_dir]/$SN) EOF #--- Read command line args and if arg is -h, provide the usage information [[ -n $1 ]] [[ $1 == '-h' ]] echo $USAGE exit 0 #--- Read command line args [[ -n $1 ]] raw1= $1 [[ -n $2 ]] raw2= $2 [[ -n $3 ]] refFA= $3 [[ -n $4 ]] refGTF= $4 [[ -n $5 ]] sampid= $5 [[ -n $6 ]] outdir= $6 #--- Check that files are provided and exist [[ -z ${raw1+x} ]] echo FAILED: read1 is not set echo $USAGE exit 1 [[ ! -e $raw1 ]] echo [---$SN---] ($(date)) FAILED: file $raw1 does not exist exit 1 [[ -z ${raw2+x} ]] echo FAILED: read2 is not set echo $USAGE exit 1 [[ ! -e $raw2 ]] echo [---$SN---] ($(date)) FAILED: file $raw2 does not exist exit 1 [[ -z ${refFA+x} ]] echo FAILED: refFA is not set echo $USAGE exit 1 [[ ! -e $refFA ]] echo [---$SN---] ($(date)) FAILED: file $refFA does not exist exit 1 [[ -z ${refGTF+x} ]] echo FAILED: refGTF is not set echo $USAGE exit 1 [[ ! -e $refGTF ]] echo [---$SN---] ($(date)) FAILED: file $refGTF does not exist exit 1 [[ -z ${sampid+x} ]] echo FAILED: sampid is not set echo $USAGE exit 1 #--- Set outdirectory [[ -z ${outdir+x} ]] outdir=$(dirname $raw1)/$SN mkdir -p $outdir #--- Determine CPUs to use # First examines NCPU environment variable, then nproc, finally sets to 1 [[ -n $NCPU ]] ncpu=$NCPU [[ -z $ncpu ]] ncpu=$(nproc 2 /dev/null) [[ -z $ncpu ]] ncpu=1 echo [---$SN---] ($(date)) read1: $raw1 echo [---$SN---] ($(date)) read2: $raw2 echo [---$SN---] ($(date)) reference_fasta: $refFA echo [---$SN---] ($(date)) reference_gtf: $refGTF echo [---$SN---] ($(date)) samp_id: $sampid echo [---$SN---] ($(date)) outdir: $outdir echo [---$SN---] ($(date)) num CPU: $ncpu #--- Start the timer t1=$(date + %s ) #### Put haphpipe module scripts here #---Complete job t2=$(date + %s ) diff=$(($t2-$t1)) echo [---$SN---] ($(date)) $(($diff / 60)) minutes and $(($diff % 60)) seconds elapsed. echo [---$SN---] ($(date)) $SN COMPLETE. -- Part 3B - Sample reads. Now we will begin constructing bash scripts for each module. Each module will follow a similar trend. We will set the stage name. We will echo stage name to terminal. We will check to make sure the files that are the output of the stage are not already present. If they are present, we will skip the stage and continue on (no need to repeat the stage). If the files are not present, we will complete the stage and call the command. If the command completes, we will print to terminal. If command fails, we will print that to terminal and quit the script. Remember, you can find the output file names here . The ouput names for this stage are: sample_1.fastq and sample_2.fastq . We have also decided to subsample the number of reads to 50,000 reads for each sample. Now this in the input for the base haphpipe command for this stage: haphpipe sample_reads\\ --fq1 $raw1\\ --fq2 $raw2\\ --nreads 50000\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} The entire stage's bash script is here: stage= sample_reads echo -e \\n[---$SN---] ($(date)) Stage: $stage # if the sampled files are present, skip this stage. Otherwise, call sample_reads if [[ -e $outdir/sample_1.fastq -e ${outdir}/sample_2.fastq ]]; then echo [---$SN---] ($(date)) EXISTS: $stage sample_1.fastq, sample_2.fastq else # this reads in the sample_reads command and saves it in the variable cmd read -r -d '' cmd EOF haphpipe sample_reads\\ --fq1 $raw1\\ --fq2 $raw2\\ --nreads 50000\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi -- Part 3C - Trim reads. Remember, you can find the output file names here . The ouput names for this stage are: trimmed_1.fastq and trimmed_2.fastq . Now, because we are doing trimming after the sampled reads module, we need to change the input fastq reads for this module to be the fastq reads output from the previous step (sample reads). Therefore, the input fastq files are named sample_1.fastq and sample_2.fastq . Also remember that these files are now contained in the outdirectory specified by the input, so we have to list the path to the input fastq files. Now this in the input for the base haphpipe command for this stage: haphpipe trim_reads\\ --ncpu $ncpu\\ --fq1 ${outdir}/sample_1.fastq\\ --fq2 ${outdir}/sample_2.fastq\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} The entire stage's bash script is here: stage= trim_reads echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/trimmed_1.fastq -e ${outdir}/trimmed_2.fastq ]]; then echo [---$SN---] ($(date)) EXISTS: $stage trimmed_1.fastq,trimmed_2.fastq else read -r -d '' cmd EOF haphpipe trim_reads\\ --ncpu $ncpu\\ --fq1 ${outdir}/sample_1.fastq\\ --fq2 ${outdir}/sample_2.fastq\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi -- Part 3D - Error correct reads. Remember, you can find the output file names here . The ouput names for this stage are: corrected_1.fastq and corrected_2.fastq . Now, because we are doing error correction after the trimming module, we need to change the input fastq reads for this module to be the fastq reads output from the previous step (trimmed reads). Therefore, the input fastq files are named trimmed_1.fastq and trimmed_2.fastq . Also remember that these files are now contained in the outdirectory specified by the input (just like the sampled reads in the previous step), so we have to list the path to the input fastq files. Now this in the input for the base haphpipe command for this stage: haphpipe ec_reads\\ --ncpu $ncpu\\ --fq1 ${outdir}/trimmed_1.fastq\\ --fq2 ${outdir}/trimmed_2.fastq\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} The entire stage's bash script is here: stage= ec_reads echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/corrected_1.fastq -e $outdir/corrected_2.fastq ]]; then echo [---$SN---] ($(date)) EXISTS: $stage corrected_1.fastq,corrected_2.fastq else read -r -d '' cmd EOF haphpipe ec_reads\\ --ncpu $ncpu\\ --fq1 ${outdir}/trimmed_1.fastq\\ --fq2 ${outdir}/trimmed_2.fastq\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi -- Part 3E - De novo assembly. Remember, you can find the output file names here . The ouput name for this stage is denovo_contigs.fna . Now, because we are doing denovo assembly after the error correction module, we need to change the input fastq reads for this module to be the fastq reads output from the previous step (error corrected reads). Therefore, the input fastq files are named corrected_1.fastq and corrected_2.fastq . Also remember that these files are now contained in the outdirectory specified by the input, so we have to list the path to the input fastq files. Finally, remember we want to specify that we do NOT want to do another round of error correction. Now this in the input for the base haphpipe command for this stage: haphpipe assemble_denovo\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --no_error_correction\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} The entire stage's bash script is here: stage= assemble_denovo echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/denovo_contigs.fna ]]; then echo [---$SN---] ($(date)) EXISTS: $stage denovo_contigs.fna else read -r -d '' cmd EOF haphpipe assemble_denovo\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --no_error_correction\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi -- Part 3F - Assemble scaffold. Remember, you can find the output file names here . The ouput name for this stage is scaffold_assembly.fa . There are other outputs, but we focus on this one for further use in the refinement and finalize modules. Now, because we are doing scaffold assembly after the denovo assembly module, we need to specify that the input file is the output contig file ( denovo_contigs.fna ). Again, remember that this file is now contained in the outdirectory specified by the input, so we have to list the path too. We also have to use our input reference fasta file and input sampleID from the script command. Now this in the input for the base haphpipe command for this stage: haphpipe assemble_scaffold\\ --contigs_fa ${outdir}/denovo_contigs.fna\\ --ref_fa ${refFA}\\ --seqname ${sampid}\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} The entire stage's bash script is here: stage= assemble_scaffold echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/scaffold_assembly.fa ]]; then echo [---$SN---] ($(date)) EXISTS: $stage scaffold_assembly.fa else read -r -d '' cmd EOF haphpipe assemble_scaffold\\ --contigs_fa ${outdir}/denovo_contigs.fna\\ --ref_fa ${refFA}\\ --seqname ${sampid}\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi -- Part 3G - Refine assembly. Remember, you can find the output file names here . The ouput name for this stage is refined.fna . Now, because we are doing refining the assembly after the scaffold assembly module, we need to change the input fastq reads for this module to be the fastq reads output from the error correction step (error corrected reads). Therefore, the input fastq files are named corrected_1.fastq and corrected_2.fastq . Also remember that these files are now contained in the outdirectory specified by the input, so we have to list the path to the input fastq files. Finally, we need to specify that the input reference file is the scaffold_assembly.fa file from the previous scaffold assembly step (above), which is also located in the outdirectory. Now this in the input for the base haphpipe command for this stage: hp_refine_assembly\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --ref_fa ${outdir}/scaffold_assembly.fa\\ --sample_id ${sampid}\\ --max_step 5\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} The entire stage's bash script is here: stage= refine_assembly echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e ${outdir}/refined.fna ]]; then echo [---$SN---] ($(date)) EXISTS: $stage refined.fna else read -r -d '' cmd EOF hp_refine_assembly\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --ref_fa ${outdir}/scaffold_assembly.fa\\ --sample_id ${sampid}\\ --max_step 5\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi -- Part 3H - Finalize assembly. Remember, you can find the output file names here . The ouput files for this stage are: final.fna , final.bam , final.vcf.gz . Now, because we are doing finalizing the assembly after the refinement module, we need to change the input fastq reads for this module to be the fastq reads output from the error correction step (error corrected reads). Therefore, the input fastq files are named corrected_1.fastq and corrected_2.fastq . Also remember that these files are now contained in the outdirectory specified by the input, so we have to list the path to the input fastq files. Finally, we need to specify that the input reference file is the refined.fna file from the previous refinement step (above), which is also located in the outdirectory. Now this in the input for the base haphpipe command for this stage: hp_finalize_assembly\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --sample_id ${sampid}\\ --ref_fa ${outdir}/refined.fna\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} The entire stage's bash script is here: stage= finalize_assembly echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e ${outdir}/final.fna -e ${outdir}/final.bam -e ${outdir}/final.vcf.gz ]]; then echo [---$SN---] ($(date)) EXISTS: $stage final.fna,final.bam,final.vcf.gz else read -r -d '' cmd EOF hp_finalize_assembly\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --sample_id ${sampid}\\ --ref_fa ${outdir}/refined.fna\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi -- Part 3I - Gather all the individual module scripts into the final pipeline script. For readable code, we will separate each module with ### like so: ############################################################################### # Step #: description here ############################################################################### insert code here .. Once we concatenate all our code, we end up with this: ############################################################################### # Step 1: Sample Reads ############################################################################### stage= sample_reads echo -e \\n[---$SN---] ($(date)) Stage: $stage # if the sampled files are present, skip this stage. Otherwise, call sample_reads if [[ -e $outdir/sample_1.fastq -e ${outdir}/sample_2.fastq ]]; then echo [---$SN---] ($(date)) EXISTS: $stage sample_1.fastq, sample_2.fastq else # this reads in the sample_reads command and saves it in the variable cmd read -r -d '' cmd EOF haphpipe sample_reads\\ --fq1 $raw1\\ --fq2 $raw2\\ --nreads 50000\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi ############################################################################### # Step 2: Trim Reads ############################################################################### stage= trim_reads echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/trimmed_1.fastq -e ${outdir}/trimmed_2.fastq ]]; then echo [---$SN---] ($(date)) EXISTS: $stage trimmed_1.fastq,trimmed_2.fastq else read -r -d '' cmd EOF haphpipe trim_reads\\ --ncpu $ncpu\\ --fq1 $raw1\\ --fq2 $raw2\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi ############################################################################### # Step 3: Error correction using Spades ############################################################################### stage= ec_reads echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/corrected_1.fastq -e $outdir/corrected_2.fastq ]]; then echo [---$SN---] ($(date)) EXISTS: $stage corrected_1.fastq,corrected_2.fastq else read -r -d '' cmd EOF haphpipe ec_reads\\ --ncpu $ncpu\\ --fq1 ${outdir}/trimmed_1.fastq\\ --fq2 ${outdir}/trimmed_2.fastq\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi ############################################################################### # Step 4: Denovo assembly ############################################################################### stage= assemble_denovo echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/denovo_contigs.fna ]]; then echo [---$SN---] ($(date)) EXISTS: $stage denovo_contigs.fna else read -r -d '' cmd EOF haphpipe assemble_denovo\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --no_error_correction\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi ############################################################################### # Step 5: Scaffold assembly ############################################################################### stage= assemble_scaffold echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/scaffold_assembly.fa ]]; then echo [---$SN---] ($(date)) EXISTS: $stage scaffold_assembly.fa else read -r -d '' cmd EOF haphpipe assemble_scaffold\\ --contigs_fa ${outdir}/denovo_contigs.fna\\ --ref_fa ${refFA}\\ --seqname ${sampid}\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi ############################################################################### # Step 5: Refine assembly ############################################################################### stage= refine_assembly echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e ${outdir}/refined.fna ]]; then echo [---$SN---] ($(date)) EXISTS: $stage refined.fna else read -r -d '' cmd EOF hp_refine_assembly\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --ref_fa ${outdir}/scaffold_assembly.fa\\ --sample_id ${sampid}\\ --max_step 5\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi ############################################################################### # Step 5: Finalize assembly ############################################################################### stage= finalize_assembly echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e ${outdir}/final.fna -e ${outdir}/final.bam -e ${outdir}/final.vcf.gz ]]; then echo [---$SN---] ($(date)) EXISTS: $stage final.fna,final.bam,final.vcf.gz else read -r -d '' cmd EOF hp_finalize_assembly\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --sample_id ${sampid}\\ --ref_fa ${outdir}/refined.fna\\ ${quiet} --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi Step 4 - Combine both the input code and bash scripts for each module into a single script. We named this script covid_genome_assembly.sh and the entire code is: #!/usr/bin/env bash ############################################################################### # This pipeline implements genome assembly using a denovo approach. Reads are # error-corrected and used to refine the scaffolded assembly, with up to 3 # refinement steps. This pipeline is used as an example for advanced usage # - making own pipeline in the User Guide. ############################################################################### SN='covid_genome_assembly' read -r -d '' USAGE EOF USAGE: $SN [read1] [read2] [reference_fasta] [samp_id] outdir ----- COVID19 Genome Assembly Pipeline ----- This pipeline implements genome assembly using a denovo approach. Reads are error-corrected and used to refine the scaffolded assembly, with up to 3 refinement steps. This pipeline is used as an example for advanced usage - making own pipeline in the User Guide. Input: read1: Fastq file for read 1. May be compressed (.gz) read2: Fastq file for read 2. May be compressed (.gz) reference_fasta: Reference sequence (fasta) samp_id: Sample ID outdir: Output directory (default is [sample_dir]/$SN) EOF #--- Read command line args and if arg is -h, provide the usage information [[ -n $1 ]] [[ $1 == '-h' ]] echo $USAGE exit 0 #--- Read command line args [[ -n $1 ]] raw1= $1 [[ -n $2 ]] raw2= $2 [[ -n $3 ]] refFA= $3 [[ -n $4 ]] sampid= $4 [[ -n $5 ]] outdir= $5 #--- Check that files are provided and exist [[ -z ${raw1+x} ]] echo FAILED: read1 is not set echo $USAGE exit 1 [[ ! -e $raw1 ]] echo [---$SN---] ($(date)) FAILED: file $raw1 does not exist exit 1 [[ -z ${raw2+x} ]] echo FAILED: read2 is not set echo $USAGE exit 1 [[ ! -e $raw2 ]] echo [---$SN---] ($(date)) FAILED: file $raw2 does not exist exit 1 [[ -z ${refFA+x} ]] echo FAILED: refFA is not set echo $USAGE exit 1 [[ ! -e $refFA ]] echo [---$SN---] ($(date)) FAILED: file $refFA does not exist exit 1 [[ -z ${sampid+x} ]] echo FAILED: sampid is not set echo $USAGE exit 1 #--- Set outdirectory [[ -z ${outdir+x} ]] outdir=$(dirname $raw1)/$SN mkdir -p $outdir #--- Determine CPUs to use # First examines NCPU environment variable, then nproc, finally sets to 1 [[ -n $NCPU ]] ncpu=$NCPU [[ -z $ncpu ]] ncpu=$(nproc 2 /dev/null) [[ -z $ncpu ]] ncpu=1 echo [---$SN---] ($(date)) read1: $raw1 echo [---$SN---] ($(date)) read2: $raw2 echo [---$SN---] ($(date)) reference_fasta: $refFA echo [---$SN---] ($(date)) samp_id: $sampid echo [---$SN---] ($(date)) outdir: $outdir echo [---$SN---] ($(date)) num CPU: $ncpu #--- Start the timer t1=$(date + %s ) ############################################################################### # Step 1: Sample Reads ############################################################################### stage= sample_reads echo -e \\n[---$SN---] ($(date)) Stage: $stage # if the sampled files are present, skip this stage. Otherwise, call sample_reads if [[ -e $outdir/sample_1.fastq -e ${outdir}/sample_2.fastq ]]; then echo [---$SN---] ($(date)) EXISTS: $stage sample_1.fastq, sample_2.fastq else # this reads in the sample_reads command and saves it in the variable cmd read -r -d '' cmd EOF haphpipe sample_reads\\ --fq1 $raw1\\ --fq2 $raw2\\ --nreads 50000\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi ############################################################################### # Step 2: Trim Reads ############################################################################### stage= trim_reads echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/trimmed_1.fastq -e ${outdir}/trimmed_2.fastq ]]; then echo [---$SN---] ($(date)) EXISTS: $stage trimmed_1.fastq,trimmed_2.fastq else read -r -d '' cmd EOF haphpipe trim_reads\\ --ncpu $ncpu\\ --fq1 $raw1\\ --fq2 $raw2\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi ############################################################################### # Step 3: Error correction using Spades ############################################################################### stage= ec_reads echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/corrected_1.fastq -e $outdir/corrected_2.fastq ]]; then echo [---$SN---] ($(date)) EXISTS: $stage corrected_1.fastq,corrected_2.fastq else read -r -d '' cmd EOF haphpipe ec_reads\\ --ncpu $ncpu\\ --fq1 ${outdir}/trimmed_1.fastq\\ --fq2 ${outdir}/trimmed_2.fastq\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi ############################################################################### # Step 4: Denovo assembly ############################################################################### stage= assemble_denovo echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/denovo_contigs.fna ]]; then echo [---$SN---] ($(date)) EXISTS: $stage denovo_contigs.fna else read -r -d '' cmd EOF haphpipe assemble_denovo\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --no_error_correction\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi ############################################################################### # Step 5: Scaffold assembly ############################################################################### stage= assemble_scaffold echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/scaffold_assembly.fa ]]; then echo [---$SN---] ($(date)) EXISTS: $stage scaffold_assembly.fa else read -r -d '' cmd EOF haphpipe assemble_scaffold\\ --contigs_fa ${outdir}/denovo_contigs.fna\\ --ref_fa ${refFA}\\ --seqname ${sampid}\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi ############################################################################### # Step 5: Refine assembly ############################################################################### stage= refine_assembly echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e ${outdir}/refined.fna ]]; then echo [---$SN---] ($(date)) EXISTS: $stage refined.fna else read -r -d '' cmd EOF hp_refine_assembly\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --ref_fa ${outdir}/scaffold_assembly.fa\\ --sample_id ${sampid}\\ --max_step 5\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi ############################################################################### # Step 5: Finalize assembly ############################################################################### stage= finalize_assembly echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e ${outdir}/final.fna -e ${outdir}/final.bam -e ${outdir}/final.vcf.gz ]]; then echo [---$SN---] ($(date)) EXISTS: $stage final.fna,final.bam,final.vcf.gz else read -r -d '' cmd EOF hp_finalize_assembly\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --sample_id ${sampid}\\ --ref_fa ${outdir}/refined.fna\\ ${quiet} --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi #### Put haphpipe module scripts here #---Complete job t2=$(date + %s ) diff=$(($t2-$t1)) echo [---$SN---] ($(date)) $(($diff / 60)) minutes and $(($diff % 60)) seconds elapsed. echo [---$SN---] ($(date)) $SN COMPLETE. Step 5 - Executing the script. Example to run: bash covid_genome_assembly.sh SRR11140750/SRR11140750_1.fastq SRR11140750/SRR11140750_2.fastq SARSCoV2.NC_045512.COVID19.fasta SRR11140750 Example to run in a loop over all the samples: for sra in SRR11140744 SRR11140746 SRR11140748 SRR11140750; do bash covid_genome_assembly.sh ${sra}/${sra}_1.fastq ${sra}/${sra}_2.fastq SARSCoV2.NC_045512.COVID19.fasta ${sra} covid_genome_assembly done Directories should look like such after running this script: . \u251c\u2500\u2500 SRR11140744 | \u251c\u2500\u2500 SRR11140744_1.fastq | \u251c\u2500\u2500 SRR11140744_2.fastq | \u2514\u2500\u2500 covid_genome_assembly | \u251c\u2500\u2500 corrected_1.fastq | \u251c\u2500\u2500 corrected_2.fastq | \u251c\u2500\u2500 corrected_U.fastq | \u251c\u2500\u2500 denovo_contigs.fna | \u251c\u2500\u2500 denovo_summary.txt | \u251c\u2500\u2500 haphpipe.out | \u251c\u2500\u2500 final.bam | \u251c\u2500\u2500 final.bam.bai | \u251c\u2500\u2500 final_bt2.out | \u251c\u2500\u2500 final.fna | \u251c\u2500\u2500 final.vcf.gz | \u251c\u2500\u2500 final.vcf.gz.tbi | \u251c\u2500\u2500 refined.01.fna | \u251c\u2500\u2500 refined_bt2.01.out | \u251c\u2500\u2500 refined.fna | \u251c\u2500\u2500 refined_bt2.out | \u251c\u2500\u2500 refined_summary.out | \u251c\u2500\u2500 sample_1.fastq | \u251c\u2500\u2500 sample_2.fastq | \u251c\u2500\u2500 scaffold_aligned.fa | \u251c\u2500\u2500 scaffold_assembly.fa | \u251c\u2500\u2500 scaffold_imputed.fa | \u251c\u2500\u2500 scaffold_padded.out | \u251c\u2500\u2500 trimmed_1.fastq | \u251c\u2500\u2500 trimmed_2.fastq | \u251c\u2500\u2500 trimmed_U.fastq | \u2514\u2500\u2500 trimmomatic_summary.out \u251c\u2500\u2500 SRR11140746 | \u251c\u2500\u2500 SRR11140746_1.fastq | \u251c\u2500\u2500 SRR11140746_2.fastq | \u2514\u2500\u2500 covid_genome_assembly .... Step 6 - Optional inclusions: PredictHaplo, multiple alignment, and building a phylogenetic tree. Here are some option modules to include within the pipeline. -- Adding PredictHaplo as a stage. If you desire PredictHaplo, you can either utilize the option --interval_txt or you can rerun the pipeline with this gtf file. It is easier for PredictHaplo to run on smaller regions than the entire genome like we implemented above. $ cat SARSCoV2.NC_045512.COVID19.gtf SARSCoV2.NC_045512.COVID19 NCBI_refseq amplicon 265 13482 . + 0 name orf1a ; SARSCoV2.NC_045512.COVID19 NCBI_refseq amplicon 21562 25383 . + 0 name surface ; SARSCoV2.NC_045512.COVID19 NCBI_refseq amplicon 26244 26471 . + 0 name envelope ; Remember, you can find the output file names here . The ouput name for this stage is PH0#.best*.fas . We are doing this stage after the finalize_assembly stage, doing refining the assembly after the scaffold assembly module. Therefore, the input fastq files are named corrected_1.fastq and corrected_2.fastq . Also remember that these files are now contained in the outdirectory specified by the input, so we have to list the path to the input fastq files. Finally, we need to specify that the input reference file is the final.fna file from the finalize assembly stage, which is also located in the outdirectory. We have to do loops for the PredictHaplo and parser stages because there are multiple haplotype regions. Now this in the input for the base haphpipe command for this stage: hp_predict_haplo --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --ref_fa ${outdir}/final.fna\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} The entire stage's bash script is here: stage= predict_haplo echo -e \\n[---$SN---] ($(date)) Stage: $stage for PH in ${outdir}/PH*; do if [[ -e ${PH} ]]; then for PHbest in ${outdir}/PH*/*best*.fas; do if [[ -e ${PHbest} ]]; then echo [---$SN---] ($(date)) EXISTS: $stage $PHbest fi done else read -r -d '' cmd EOF hp_predict_haplo --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --ref_fa ${outdir}/final.fna\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi done -- Following up PredictHaplo with ph_parser In order to use the output from PredictHaplo (see description here ), we need to run the ph_parser stage. hp_ph_parser --haplotypes_fa ${outdir}/${PH}/*best*.fas\\ --prefix ${sampID}_${PH}\\ --logfile ${outdir}/${PH}/haphpipe.out\\ --outdir ${outdir} Now we have to do a loop, because there are multiple regions (i.e., PH0# directories). The entire stage's bash script is here: stage= ph_parser echo -e \\n[---$SN---] ($(date)) Stage: $stage for PH in ${outdir}/PH*/ph_haplotypes.fna; do #if [[ -e ${PH} ]]; then # echo [---$SN---] ($(date)) EXISTS: $stage $PH #else read -r -d '' cmd EOF hp_ph_parser --haplotypes_fa $(dirname $PH)/*best*.fas\\ --prefix ${sampID}_$(dirname $(basename $PH))\\ --logfile ${outdir}/haphpipe.out\\ --outdir $(dirname $PH) EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) #fi done -- Adding multiple_align as a stage Remember, you can find the output file names here . The ouput name for this stage is alignment.fasta . We first need to make a text file that has a list of directories that contin final.fna . Because this is a genome assembly, we want to use the --alignall option to align the entire region and not use a GTF file. We also choose to output a phylip file using the option --phyipout . Now this in the input for the base haphpipe command for this stage: haphpipe multiple_align --ncpu $ncpu\\ --dir_list dir_list.txt\\ --phylipout\\ --alignall\\ --logfile haphpipe.out The entire stage's bash script is here: stage= multiple_align echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/multiple_align/alignment.fasta ]] [[ -e $outdir/multiple_align/alignment.phy ]]; then echo [---$SN---] ($(date)) EXISTS: $stage alignment.fasta,alignment.phy else read -r -d '' cmd EOF haphpipe multiple_align\\ --ncpu $ncpu\\ --dir_list dir_list.txt\\ --phylipout\\ --alignall\\ --logfile haphpipe.out EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi -- Adding model_test as a stage Remember, you can find the output file names here . The ouput name for this stage is modeltest_results.out . The input will be the alignment file from multiple_align . We'll also give the output an id of covid19_genome so the output file will be covid19_genome_modeltest_results.out . We will also chose the template of the output as raxml, since the next module build_tree uses RAxML. Now this in the input for the base haphpipe command for this stage: haphpipe model_test --seqs alignment.fasta\\ --run_id covid_genome\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir}\\ --template raxml\\ --ncpu ${ncpu} The entire stage's bash script is here: stage= model_test echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/covid19_genome_modeltest_results.out ]]; then echo [---$SN---] ($(date)) EXISTS: $stage covid19_genome_modeltest_results.out else read -r -d '' cmd EOF haphpipe model_test --seqs alignment.fasta\\ --run_id covid_genome\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir}\\ --template raxml\\ --ncpu ${ncpu} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi","title":"Advanced Users"},{"location":"adv/#making-your-own-pipelines-with-bash","text":"This is an example of how to make your own pipeline with bash. This example uses four NGS samples for COVID-19 from NCBI and puts them through whole genome de novo assembly. SRA accession numbers: SRR11140744 SRR11140746 SRR11140748 SRR11140750 Step 0 - Obtaining samples. We will assume that you have downloaded the reads from NCBI or know how to use fastq-dump. We will show the fastq-dump command here that we used to download the files. This part is not included in the pipeline. for sra in SRR11140744 SRR11140746 SRR11140748 SRR11140750; do fastq-dump --outdir ${sra} --split-files --origfmt ${sra} done Output is: Read 503344 spots for SRR11140744 Written 503344 spots for SRR11140744 Read 358971 spots for SRR11140746 Written 358971 spots for SRR11140746 Read 421395 spots for SRR11140748 Written 421395 spots for SRR11140748 Read 17657 spots for SRR11140750 Written 17657 spots for SRR11140750 You're starting directory should look like for this example, whether the reads were obtained manually or downloaded with fastq-dump: . \u251c\u2500\u2500 SRR11140744 | \u251c\u2500\u2500 SRR11140744_1.fastq | \u2514\u2500\u2500 SRR11140744_2.fastq \u251c\u2500\u2500 SRR11140746 | \u251c\u2500\u2500 SRR11140746_1.fastq | \u2514\u2500\u2500 SRR11140746_2.fastq \u251c\u2500\u2500 SRR11140748 | \u251c\u2500\u2500 SRR11140748_1.fastq | \u2514\u2500\u2500 SRR11140748_2.fastq \u2514\u2500\u2500 SRR11140750 \u251c\u2500\u2500 SRR11140750_1.fastq \u2514\u2500\u2500 SRR11140750_2.fastq We also know we will need a reference genome to help scaffold the contigs. We have downloaded the COVID19 reference genome here as a FASTA file. Step 1 - Evaluate which modules you want to use. View all the module options using haphpipe -h . We have decided that we want to sample the reads ( sample_reads ), trim the reads ( trim_reads ), error correct the reads ( ec_reads ). We want to do genome de novo assembly, so we want to use assemble_denovo and assemble_scaffold . We then want to do refinement of the assembly ( refine_assembly ) and finalize the assembly ( finalize_assembly ). Step 2 - Document files and options needed for each module. As we go through each module, we will take note of what we need and which input is specific for an individual sample (i.e., the input fastq reads will be different per sample, but we probably want the same number of reads for each sample.) We know all modules have the option for a logfile. We'll include that in the bash scripting, but don't need to make notes of a logfile for each module. -- Part 2A - Sample reads. Upon viewing the haphpipe sample_reads -h we see that we need to provide fastq reads 1 and 2, an output directory, and number of reads desired. Sample specific options: fastq read 1 fastq read 2 output directory Not sample specific options: number of reads desired -- Part 2B - Trim reads. Upon viewing the haphpipe trim_reads -h we see that we need to provide fastq reads 1 and 2, an output directory, number of reads, and number of CPUs desired. There is an option to change the timming commands, but we'll keep default. Sample specific options: fastq read 1 -- we will want these to be the subsampled read file fastq read 2 -- we will want these to be the subsampled read file output directory Not sample specific options: number of reads desired ncpu -- Part 2C - Error correct reads. Upon viewing the haphpipe ec_reads -h we see that we need to provide fastq reads 1 and 2, an output directory, and number of CPUs desired. Sample specific options: fastq read 1 -- we will want these to be the trimmed read file fastq read 2 -- we will want these to be the trimmed read file output directory Not sample specific options: ncpu -- Part 2D - De novo assembly. Upon viewing the haphpipe assemble_denovo -h we see that we need to provide fastq reads 1 and 2, an output directory, and number of CPUs desired. Because we previously completed error correction. We will not want to include it here (i.e., we will invoke the option command --no_error_correction ). Sample specific options: fastq read 1 -- we will want these to be the error corrected reads fastq read 2 -- we will want these to be the error corrected reads output directory Not sample specific options: ncpu no error correction option -- Part 2E - Assemble scaffold. Upon viewing the haphpipe assemble_scaffold -h we see that we need to provide fasta file containing assembled contigs, an output directory, name to append to scaffold sequence and reference fasta desired. There is an option to change the timming commands, but we'll keep default. Sample specific options: assembled contigs file output directory sequence name Not sample specific options: reference fasta -- Part 2F - Refine assembly. Upon viewing the haphpipe refine_assembly -h we see that we need to provide fastq reads 1 and 2, an output directory, a reference sequence to refine, a sample ID and a maximum number of refinement steps, and number of CPUs desired. Sample specific options: fastq read 1 -- we will want these to be the error corrected reads fastq read 2 -- we will want these to be the error corrected reads output directory reference fasta -- we will want this to be the sample's assembled scaffold Sample ID Not sample specific options: ncpu maximum number of refinement steps (we'll do 3 for sake of simplicity and time) -- Part 2G - Finalize assembly. Upon viewing the haphpipe finalize_assembly -h we see that we need to provide fastq reads 1 and 2, an output directory, a reference sequence to finalize, a sample ID and a maximum number of refinement steps, and number of CPUs desired. We could replace the preset bowtie2 option, but we will leave it as default for this sample pipeline. Sample specific options: fastq read 1 -- we will want these to be the error corrected reads fastq read 2 -- we will want these to be the error corrected reads output directory reference fasta -- we will want this to be the sample's refined fasta sequence Sample ID Not sample specific options: ncpu -- Part 2H - Gather the needed initial input for each sample. We need to gather the necessary files that we will need to input for each sample so that we can make a script that takes in input files. We know from the User Guide that we can look at the output/input file types and names here . Because each module has a standard output file name, we will only need to be specific about the input for the raw fastq files for each sample. Therefore we need: input raw fastq read 1 input raw fastq read 2 reference genome to scaffold against (this will be a covid19 reference) output directory for each sample sample name The other options we can code into the bash script for each module, since they will be the same for every sample in this analysis. Step 3 - Create a bash script for each module. Now we will format a bash script for each module. -- Part 3A - Input options. Because we have a list of needed input options (specified by the user), we need to make a bash command to take in the inputs. First, we will specify the script name. We can do this one of two ways. i) explicitly or 2) through a command. i) SN='covid_genome' # this sets the script name (SN variable) to covid_genome ii) SN=$(basename $0) # this sets the script name (SN variable) to whatever the script filename is. If the script's file name is covid.sh then it is set as that. If the file name is this_is_file it will be set as that. Because we are in charge of this script, we will explicitly set it. Second, we want to set some input information for the user. We can do this as such: read -r -d '' USAGE EOF USAGE: $SN [read1] [read2] [reference_fasta] [samp_id] outdir ----- COVID19 Genome Assembly Pipeline ----- This pipeline implements genome assembly using a denovo approach. Reads are error-corrected and used to refine the scaffolded assembly, with up to 3 refinement steps. This pipeline is used as an example for advanced usage - making own pipeline in the User Guide. Input: read1: Fastq file for read 1. May be compressed (.gz) read2: Fastq file for read 2. May be compressed (.gz) reference_fasta: Reference sequence (fasta) samp_id: Sample ID outdir: Output directory (default is [sample_dir]/$SN) EOF Third, we want to provide help information for the script. We can do that using the avoce code, which has been saved in the variable $USAGE . Therefore, #--- Read command line args and if arg is -h, provide the usage information [[ -n $1 ]] [[ $1 == '-h' ]] echo $USAGE exit 0 If pipeline.sh -h is invoked here, then the output is: USAGE: covid_genome_assembly [read1] [read2] [reference_fasta] [samp_id] outdir ----- COVID19 Genome Assembly Pipeline ----- This pipeline implements genome assembly using a denovo approach. Reads are error-corrected and used to refine the scaffolded assembly, with up to 3 refinement steps. This pipeline is used as an example for advanced usage - making own pipeline in the User Guide. Input: read1: Fastq file for read 1. May be compressed (.gz) read2: Fastq file for read 2. May be compressed (.gz) reference_fasta: Reference sequence (fasta) samp_id: Sample ID outdir: Output directory (default is [sample_dir]/covid_genome_assembly) Note that this output looks identical to the information we put into $USAGE variable above in the second part of this section. Fourth, we want to read in the command with the provided input options. Because we are using a bash script, the position of the input files is imparative. #--- Read command line args [[ -n $1 ]] raw1= $1 [[ -n $2 ]] raw2= $2 [[ -n $3 ]] refFA= $3 [[ -n $4 ]] sampid= $4 [[ -n $5 ]] outdir= $5 Fifth, we want to check that the input files are provided and are not empty. We also want to set the outdirectory. #--- Check that files are provided and exist [[ -z ${raw1+x} ]] echo FAILED: read1 is not set echo $USAGE exit 1 [[ ! -e $raw1 ]] echo [---$SN---] ($(date)) FAILED: file $raw1 does not exist exit 1 [[ -z ${raw2+x} ]] echo FAILED: read2 is not set echo $USAGE exit 1 [[ ! -e $raw2 ]] echo [---$SN---] ($(date)) FAILED: file $raw2 does not exist exit 1 [[ -z ${refFA+x} ]] echo FAILED: refFA is not set echo $USAGE exit 1 [[ ! -e $refFA ]] echo [---$SN---] ($(date)) FAILED: file $refFA does not exist exit 1 [[ -z ${sampid+x} ]] echo FAILED: sampid is not set echo $USAGE exit 1 #--- Set outdirectory [[ -z ${outdir+x} ]] outdir=$(dirname $raw1)/$SN mkdir -p $outdir Sixth, we want to set the number of CPUs to use throughout the script. #--- Determine CPUs to use # First examines NCPU environment variable, then nproc, finally sets to 1 [[ -n $NCPU ]] ncpu=$NCPU [[ -z $ncpu ]] ncpu=$(nproc 2 /dev/null) [[ -z $ncpu ]] ncpu=1 Seventh, we want to print out the variables and files. echo [---$SN---] ($(date)) read1: $raw1 echo [---$SN---] ($(date)) read2: $raw2 echo [---$SN---] ($(date)) reference_fasta: $refFA echo [---$SN---] ($(date)) samp_id: $sampid echo [---$SN---] ($(date)) outdir: $outdir echo [---$SN---] ($(date)) num CPU: $ncpu Finally, because here at GWU CBI, we like to time everything, we include a line of code to start a timer and end the timer: #--- Start the timer t1=$(date + %s ) #### Put haphpipe module scripts here #---Complete job t2=$(date + %s ) diff=$(($t2-$t1)) echo [---$SN---] ($(date)) $(($diff / 60)) minutes and $(($diff % 60)) seconds elapsed. echo [---$SN---] ($(date)) $SN COMPLETE. After Part 3A, our pipeline file should read as such: #!/usr/bin/env bash ############################################################################### # This pipeline implements genome assembly using a denovo approach. Reads are # error-corrected and used to refine the scaffolded assembly, with up to 3 # refinement steps. This pipeline is used as an example for advanced usage # - making own pipeline in the User Guide. ############################################################################### SN='covid_genome_assembly' read -r -d '' USAGE EOF USAGE: $SN [read1] [read2] [reference_fasta] [samp_id] outdir ----- COVID19 Genome Assembly Pipeline ----- This pipeline implements genome assembly using a denovo approach. Reads are error-corrected and used to refine the scaffolded assembly, with up to 3 refinement steps. This pipeline is used as an example for advanced usage - making own pipeline in the User Guide. Input: read1: Fastq file for read 1. May be compressed (.gz) read2: Fastq file for read 2. May be compressed (.gz) reference_fasta: Reference sequence (fasta) samp_id: Sample ID outdir: Output directory (default is [sample_dir]/$SN) EOF #--- Read command line args and if arg is -h, provide the usage information [[ -n $1 ]] [[ $1 == '-h' ]] echo $USAGE exit 0 #--- Read command line args [[ -n $1 ]] raw1= $1 [[ -n $2 ]] raw2= $2 [[ -n $3 ]] refFA= $3 [[ -n $4 ]] refGTF= $4 [[ -n $5 ]] sampid= $5 [[ -n $6 ]] outdir= $6 #--- Check that files are provided and exist [[ -z ${raw1+x} ]] echo FAILED: read1 is not set echo $USAGE exit 1 [[ ! -e $raw1 ]] echo [---$SN---] ($(date)) FAILED: file $raw1 does not exist exit 1 [[ -z ${raw2+x} ]] echo FAILED: read2 is not set echo $USAGE exit 1 [[ ! -e $raw2 ]] echo [---$SN---] ($(date)) FAILED: file $raw2 does not exist exit 1 [[ -z ${refFA+x} ]] echo FAILED: refFA is not set echo $USAGE exit 1 [[ ! -e $refFA ]] echo [---$SN---] ($(date)) FAILED: file $refFA does not exist exit 1 [[ -z ${refGTF+x} ]] echo FAILED: refGTF is not set echo $USAGE exit 1 [[ ! -e $refGTF ]] echo [---$SN---] ($(date)) FAILED: file $refGTF does not exist exit 1 [[ -z ${sampid+x} ]] echo FAILED: sampid is not set echo $USAGE exit 1 #--- Set outdirectory [[ -z ${outdir+x} ]] outdir=$(dirname $raw1)/$SN mkdir -p $outdir #--- Determine CPUs to use # First examines NCPU environment variable, then nproc, finally sets to 1 [[ -n $NCPU ]] ncpu=$NCPU [[ -z $ncpu ]] ncpu=$(nproc 2 /dev/null) [[ -z $ncpu ]] ncpu=1 echo [---$SN---] ($(date)) read1: $raw1 echo [---$SN---] ($(date)) read2: $raw2 echo [---$SN---] ($(date)) reference_fasta: $refFA echo [---$SN---] ($(date)) reference_gtf: $refGTF echo [---$SN---] ($(date)) samp_id: $sampid echo [---$SN---] ($(date)) outdir: $outdir echo [---$SN---] ($(date)) num CPU: $ncpu #--- Start the timer t1=$(date + %s ) #### Put haphpipe module scripts here #---Complete job t2=$(date + %s ) diff=$(($t2-$t1)) echo [---$SN---] ($(date)) $(($diff / 60)) minutes and $(($diff % 60)) seconds elapsed. echo [---$SN---] ($(date)) $SN COMPLETE. -- Part 3B - Sample reads. Now we will begin constructing bash scripts for each module. Each module will follow a similar trend. We will set the stage name. We will echo stage name to terminal. We will check to make sure the files that are the output of the stage are not already present. If they are present, we will skip the stage and continue on (no need to repeat the stage). If the files are not present, we will complete the stage and call the command. If the command completes, we will print to terminal. If command fails, we will print that to terminal and quit the script. Remember, you can find the output file names here . The ouput names for this stage are: sample_1.fastq and sample_2.fastq . We have also decided to subsample the number of reads to 50,000 reads for each sample. Now this in the input for the base haphpipe command for this stage: haphpipe sample_reads\\ --fq1 $raw1\\ --fq2 $raw2\\ --nreads 50000\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} The entire stage's bash script is here: stage= sample_reads echo -e \\n[---$SN---] ($(date)) Stage: $stage # if the sampled files are present, skip this stage. Otherwise, call sample_reads if [[ -e $outdir/sample_1.fastq -e ${outdir}/sample_2.fastq ]]; then echo [---$SN---] ($(date)) EXISTS: $stage sample_1.fastq, sample_2.fastq else # this reads in the sample_reads command and saves it in the variable cmd read -r -d '' cmd EOF haphpipe sample_reads\\ --fq1 $raw1\\ --fq2 $raw2\\ --nreads 50000\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi -- Part 3C - Trim reads. Remember, you can find the output file names here . The ouput names for this stage are: trimmed_1.fastq and trimmed_2.fastq . Now, because we are doing trimming after the sampled reads module, we need to change the input fastq reads for this module to be the fastq reads output from the previous step (sample reads). Therefore, the input fastq files are named sample_1.fastq and sample_2.fastq . Also remember that these files are now contained in the outdirectory specified by the input, so we have to list the path to the input fastq files. Now this in the input for the base haphpipe command for this stage: haphpipe trim_reads\\ --ncpu $ncpu\\ --fq1 ${outdir}/sample_1.fastq\\ --fq2 ${outdir}/sample_2.fastq\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} The entire stage's bash script is here: stage= trim_reads echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/trimmed_1.fastq -e ${outdir}/trimmed_2.fastq ]]; then echo [---$SN---] ($(date)) EXISTS: $stage trimmed_1.fastq,trimmed_2.fastq else read -r -d '' cmd EOF haphpipe trim_reads\\ --ncpu $ncpu\\ --fq1 ${outdir}/sample_1.fastq\\ --fq2 ${outdir}/sample_2.fastq\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi -- Part 3D - Error correct reads. Remember, you can find the output file names here . The ouput names for this stage are: corrected_1.fastq and corrected_2.fastq . Now, because we are doing error correction after the trimming module, we need to change the input fastq reads for this module to be the fastq reads output from the previous step (trimmed reads). Therefore, the input fastq files are named trimmed_1.fastq and trimmed_2.fastq . Also remember that these files are now contained in the outdirectory specified by the input (just like the sampled reads in the previous step), so we have to list the path to the input fastq files. Now this in the input for the base haphpipe command for this stage: haphpipe ec_reads\\ --ncpu $ncpu\\ --fq1 ${outdir}/trimmed_1.fastq\\ --fq2 ${outdir}/trimmed_2.fastq\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} The entire stage's bash script is here: stage= ec_reads echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/corrected_1.fastq -e $outdir/corrected_2.fastq ]]; then echo [---$SN---] ($(date)) EXISTS: $stage corrected_1.fastq,corrected_2.fastq else read -r -d '' cmd EOF haphpipe ec_reads\\ --ncpu $ncpu\\ --fq1 ${outdir}/trimmed_1.fastq\\ --fq2 ${outdir}/trimmed_2.fastq\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi -- Part 3E - De novo assembly. Remember, you can find the output file names here . The ouput name for this stage is denovo_contigs.fna . Now, because we are doing denovo assembly after the error correction module, we need to change the input fastq reads for this module to be the fastq reads output from the previous step (error corrected reads). Therefore, the input fastq files are named corrected_1.fastq and corrected_2.fastq . Also remember that these files are now contained in the outdirectory specified by the input, so we have to list the path to the input fastq files. Finally, remember we want to specify that we do NOT want to do another round of error correction. Now this in the input for the base haphpipe command for this stage: haphpipe assemble_denovo\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --no_error_correction\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} The entire stage's bash script is here: stage= assemble_denovo echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/denovo_contigs.fna ]]; then echo [---$SN---] ($(date)) EXISTS: $stage denovo_contigs.fna else read -r -d '' cmd EOF haphpipe assemble_denovo\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --no_error_correction\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi -- Part 3F - Assemble scaffold. Remember, you can find the output file names here . The ouput name for this stage is scaffold_assembly.fa . There are other outputs, but we focus on this one for further use in the refinement and finalize modules. Now, because we are doing scaffold assembly after the denovo assembly module, we need to specify that the input file is the output contig file ( denovo_contigs.fna ). Again, remember that this file is now contained in the outdirectory specified by the input, so we have to list the path too. We also have to use our input reference fasta file and input sampleID from the script command. Now this in the input for the base haphpipe command for this stage: haphpipe assemble_scaffold\\ --contigs_fa ${outdir}/denovo_contigs.fna\\ --ref_fa ${refFA}\\ --seqname ${sampid}\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} The entire stage's bash script is here: stage= assemble_scaffold echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/scaffold_assembly.fa ]]; then echo [---$SN---] ($(date)) EXISTS: $stage scaffold_assembly.fa else read -r -d '' cmd EOF haphpipe assemble_scaffold\\ --contigs_fa ${outdir}/denovo_contigs.fna\\ --ref_fa ${refFA}\\ --seqname ${sampid}\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi -- Part 3G - Refine assembly. Remember, you can find the output file names here . The ouput name for this stage is refined.fna . Now, because we are doing refining the assembly after the scaffold assembly module, we need to change the input fastq reads for this module to be the fastq reads output from the error correction step (error corrected reads). Therefore, the input fastq files are named corrected_1.fastq and corrected_2.fastq . Also remember that these files are now contained in the outdirectory specified by the input, so we have to list the path to the input fastq files. Finally, we need to specify that the input reference file is the scaffold_assembly.fa file from the previous scaffold assembly step (above), which is also located in the outdirectory. Now this in the input for the base haphpipe command for this stage: hp_refine_assembly\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --ref_fa ${outdir}/scaffold_assembly.fa\\ --sample_id ${sampid}\\ --max_step 5\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} The entire stage's bash script is here: stage= refine_assembly echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e ${outdir}/refined.fna ]]; then echo [---$SN---] ($(date)) EXISTS: $stage refined.fna else read -r -d '' cmd EOF hp_refine_assembly\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --ref_fa ${outdir}/scaffold_assembly.fa\\ --sample_id ${sampid}\\ --max_step 5\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi -- Part 3H - Finalize assembly. Remember, you can find the output file names here . The ouput files for this stage are: final.fna , final.bam , final.vcf.gz . Now, because we are doing finalizing the assembly after the refinement module, we need to change the input fastq reads for this module to be the fastq reads output from the error correction step (error corrected reads). Therefore, the input fastq files are named corrected_1.fastq and corrected_2.fastq . Also remember that these files are now contained in the outdirectory specified by the input, so we have to list the path to the input fastq files. Finally, we need to specify that the input reference file is the refined.fna file from the previous refinement step (above), which is also located in the outdirectory. Now this in the input for the base haphpipe command for this stage: hp_finalize_assembly\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --sample_id ${sampid}\\ --ref_fa ${outdir}/refined.fna\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} The entire stage's bash script is here: stage= finalize_assembly echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e ${outdir}/final.fna -e ${outdir}/final.bam -e ${outdir}/final.vcf.gz ]]; then echo [---$SN---] ($(date)) EXISTS: $stage final.fna,final.bam,final.vcf.gz else read -r -d '' cmd EOF hp_finalize_assembly\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --sample_id ${sampid}\\ --ref_fa ${outdir}/refined.fna\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi -- Part 3I - Gather all the individual module scripts into the final pipeline script. For readable code, we will separate each module with ### like so: ############################################################################### # Step #: description here ############################################################################### insert code here .. Once we concatenate all our code, we end up with this: ############################################################################### # Step 1: Sample Reads ############################################################################### stage= sample_reads echo -e \\n[---$SN---] ($(date)) Stage: $stage # if the sampled files are present, skip this stage. Otherwise, call sample_reads if [[ -e $outdir/sample_1.fastq -e ${outdir}/sample_2.fastq ]]; then echo [---$SN---] ($(date)) EXISTS: $stage sample_1.fastq, sample_2.fastq else # this reads in the sample_reads command and saves it in the variable cmd read -r -d '' cmd EOF haphpipe sample_reads\\ --fq1 $raw1\\ --fq2 $raw2\\ --nreads 50000\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi ############################################################################### # Step 2: Trim Reads ############################################################################### stage= trim_reads echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/trimmed_1.fastq -e ${outdir}/trimmed_2.fastq ]]; then echo [---$SN---] ($(date)) EXISTS: $stage trimmed_1.fastq,trimmed_2.fastq else read -r -d '' cmd EOF haphpipe trim_reads\\ --ncpu $ncpu\\ --fq1 $raw1\\ --fq2 $raw2\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi ############################################################################### # Step 3: Error correction using Spades ############################################################################### stage= ec_reads echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/corrected_1.fastq -e $outdir/corrected_2.fastq ]]; then echo [---$SN---] ($(date)) EXISTS: $stage corrected_1.fastq,corrected_2.fastq else read -r -d '' cmd EOF haphpipe ec_reads\\ --ncpu $ncpu\\ --fq1 ${outdir}/trimmed_1.fastq\\ --fq2 ${outdir}/trimmed_2.fastq\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi ############################################################################### # Step 4: Denovo assembly ############################################################################### stage= assemble_denovo echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/denovo_contigs.fna ]]; then echo [---$SN---] ($(date)) EXISTS: $stage denovo_contigs.fna else read -r -d '' cmd EOF haphpipe assemble_denovo\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --no_error_correction\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi ############################################################################### # Step 5: Scaffold assembly ############################################################################### stage= assemble_scaffold echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/scaffold_assembly.fa ]]; then echo [---$SN---] ($(date)) EXISTS: $stage scaffold_assembly.fa else read -r -d '' cmd EOF haphpipe assemble_scaffold\\ --contigs_fa ${outdir}/denovo_contigs.fna\\ --ref_fa ${refFA}\\ --seqname ${sampid}\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi ############################################################################### # Step 5: Refine assembly ############################################################################### stage= refine_assembly echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e ${outdir}/refined.fna ]]; then echo [---$SN---] ($(date)) EXISTS: $stage refined.fna else read -r -d '' cmd EOF hp_refine_assembly\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --ref_fa ${outdir}/scaffold_assembly.fa\\ --sample_id ${sampid}\\ --max_step 5\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi ############################################################################### # Step 5: Finalize assembly ############################################################################### stage= finalize_assembly echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e ${outdir}/final.fna -e ${outdir}/final.bam -e ${outdir}/final.vcf.gz ]]; then echo [---$SN---] ($(date)) EXISTS: $stage final.fna,final.bam,final.vcf.gz else read -r -d '' cmd EOF hp_finalize_assembly\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --sample_id ${sampid}\\ --ref_fa ${outdir}/refined.fna\\ ${quiet} --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi Step 4 - Combine both the input code and bash scripts for each module into a single script. We named this script covid_genome_assembly.sh and the entire code is: #!/usr/bin/env bash ############################################################################### # This pipeline implements genome assembly using a denovo approach. Reads are # error-corrected and used to refine the scaffolded assembly, with up to 3 # refinement steps. This pipeline is used as an example for advanced usage # - making own pipeline in the User Guide. ############################################################################### SN='covid_genome_assembly' read -r -d '' USAGE EOF USAGE: $SN [read1] [read2] [reference_fasta] [samp_id] outdir ----- COVID19 Genome Assembly Pipeline ----- This pipeline implements genome assembly using a denovo approach. Reads are error-corrected and used to refine the scaffolded assembly, with up to 3 refinement steps. This pipeline is used as an example for advanced usage - making own pipeline in the User Guide. Input: read1: Fastq file for read 1. May be compressed (.gz) read2: Fastq file for read 2. May be compressed (.gz) reference_fasta: Reference sequence (fasta) samp_id: Sample ID outdir: Output directory (default is [sample_dir]/$SN) EOF #--- Read command line args and if arg is -h, provide the usage information [[ -n $1 ]] [[ $1 == '-h' ]] echo $USAGE exit 0 #--- Read command line args [[ -n $1 ]] raw1= $1 [[ -n $2 ]] raw2= $2 [[ -n $3 ]] refFA= $3 [[ -n $4 ]] sampid= $4 [[ -n $5 ]] outdir= $5 #--- Check that files are provided and exist [[ -z ${raw1+x} ]] echo FAILED: read1 is not set echo $USAGE exit 1 [[ ! -e $raw1 ]] echo [---$SN---] ($(date)) FAILED: file $raw1 does not exist exit 1 [[ -z ${raw2+x} ]] echo FAILED: read2 is not set echo $USAGE exit 1 [[ ! -e $raw2 ]] echo [---$SN---] ($(date)) FAILED: file $raw2 does not exist exit 1 [[ -z ${refFA+x} ]] echo FAILED: refFA is not set echo $USAGE exit 1 [[ ! -e $refFA ]] echo [---$SN---] ($(date)) FAILED: file $refFA does not exist exit 1 [[ -z ${sampid+x} ]] echo FAILED: sampid is not set echo $USAGE exit 1 #--- Set outdirectory [[ -z ${outdir+x} ]] outdir=$(dirname $raw1)/$SN mkdir -p $outdir #--- Determine CPUs to use # First examines NCPU environment variable, then nproc, finally sets to 1 [[ -n $NCPU ]] ncpu=$NCPU [[ -z $ncpu ]] ncpu=$(nproc 2 /dev/null) [[ -z $ncpu ]] ncpu=1 echo [---$SN---] ($(date)) read1: $raw1 echo [---$SN---] ($(date)) read2: $raw2 echo [---$SN---] ($(date)) reference_fasta: $refFA echo [---$SN---] ($(date)) samp_id: $sampid echo [---$SN---] ($(date)) outdir: $outdir echo [---$SN---] ($(date)) num CPU: $ncpu #--- Start the timer t1=$(date + %s ) ############################################################################### # Step 1: Sample Reads ############################################################################### stage= sample_reads echo -e \\n[---$SN---] ($(date)) Stage: $stage # if the sampled files are present, skip this stage. Otherwise, call sample_reads if [[ -e $outdir/sample_1.fastq -e ${outdir}/sample_2.fastq ]]; then echo [---$SN---] ($(date)) EXISTS: $stage sample_1.fastq, sample_2.fastq else # this reads in the sample_reads command and saves it in the variable cmd read -r -d '' cmd EOF haphpipe sample_reads\\ --fq1 $raw1\\ --fq2 $raw2\\ --nreads 50000\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi ############################################################################### # Step 2: Trim Reads ############################################################################### stage= trim_reads echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/trimmed_1.fastq -e ${outdir}/trimmed_2.fastq ]]; then echo [---$SN---] ($(date)) EXISTS: $stage trimmed_1.fastq,trimmed_2.fastq else read -r -d '' cmd EOF haphpipe trim_reads\\ --ncpu $ncpu\\ --fq1 $raw1\\ --fq2 $raw2\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi ############################################################################### # Step 3: Error correction using Spades ############################################################################### stage= ec_reads echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/corrected_1.fastq -e $outdir/corrected_2.fastq ]]; then echo [---$SN---] ($(date)) EXISTS: $stage corrected_1.fastq,corrected_2.fastq else read -r -d '' cmd EOF haphpipe ec_reads\\ --ncpu $ncpu\\ --fq1 ${outdir}/trimmed_1.fastq\\ --fq2 ${outdir}/trimmed_2.fastq\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi ############################################################################### # Step 4: Denovo assembly ############################################################################### stage= assemble_denovo echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/denovo_contigs.fna ]]; then echo [---$SN---] ($(date)) EXISTS: $stage denovo_contigs.fna else read -r -d '' cmd EOF haphpipe assemble_denovo\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --no_error_correction\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi ############################################################################### # Step 5: Scaffold assembly ############################################################################### stage= assemble_scaffold echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/scaffold_assembly.fa ]]; then echo [---$SN---] ($(date)) EXISTS: $stage scaffold_assembly.fa else read -r -d '' cmd EOF haphpipe assemble_scaffold\\ --contigs_fa ${outdir}/denovo_contigs.fna\\ --ref_fa ${refFA}\\ --seqname ${sampid}\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi ############################################################################### # Step 5: Refine assembly ############################################################################### stage= refine_assembly echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e ${outdir}/refined.fna ]]; then echo [---$SN---] ($(date)) EXISTS: $stage refined.fna else read -r -d '' cmd EOF hp_refine_assembly\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --ref_fa ${outdir}/scaffold_assembly.fa\\ --sample_id ${sampid}\\ --max_step 5\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi ############################################################################### # Step 5: Finalize assembly ############################################################################### stage= finalize_assembly echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e ${outdir}/final.fna -e ${outdir}/final.bam -e ${outdir}/final.vcf.gz ]]; then echo [---$SN---] ($(date)) EXISTS: $stage final.fna,final.bam,final.vcf.gz else read -r -d '' cmd EOF hp_finalize_assembly\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --sample_id ${sampid}\\ --ref_fa ${outdir}/refined.fna\\ ${quiet} --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi #### Put haphpipe module scripts here #---Complete job t2=$(date + %s ) diff=$(($t2-$t1)) echo [---$SN---] ($(date)) $(($diff / 60)) minutes and $(($diff % 60)) seconds elapsed. echo [---$SN---] ($(date)) $SN COMPLETE. Step 5 - Executing the script. Example to run: bash covid_genome_assembly.sh SRR11140750/SRR11140750_1.fastq SRR11140750/SRR11140750_2.fastq SARSCoV2.NC_045512.COVID19.fasta SRR11140750 Example to run in a loop over all the samples: for sra in SRR11140744 SRR11140746 SRR11140748 SRR11140750; do bash covid_genome_assembly.sh ${sra}/${sra}_1.fastq ${sra}/${sra}_2.fastq SARSCoV2.NC_045512.COVID19.fasta ${sra} covid_genome_assembly done Directories should look like such after running this script: . \u251c\u2500\u2500 SRR11140744 | \u251c\u2500\u2500 SRR11140744_1.fastq | \u251c\u2500\u2500 SRR11140744_2.fastq | \u2514\u2500\u2500 covid_genome_assembly | \u251c\u2500\u2500 corrected_1.fastq | \u251c\u2500\u2500 corrected_2.fastq | \u251c\u2500\u2500 corrected_U.fastq | \u251c\u2500\u2500 denovo_contigs.fna | \u251c\u2500\u2500 denovo_summary.txt | \u251c\u2500\u2500 haphpipe.out | \u251c\u2500\u2500 final.bam | \u251c\u2500\u2500 final.bam.bai | \u251c\u2500\u2500 final_bt2.out | \u251c\u2500\u2500 final.fna | \u251c\u2500\u2500 final.vcf.gz | \u251c\u2500\u2500 final.vcf.gz.tbi | \u251c\u2500\u2500 refined.01.fna | \u251c\u2500\u2500 refined_bt2.01.out | \u251c\u2500\u2500 refined.fna | \u251c\u2500\u2500 refined_bt2.out | \u251c\u2500\u2500 refined_summary.out | \u251c\u2500\u2500 sample_1.fastq | \u251c\u2500\u2500 sample_2.fastq | \u251c\u2500\u2500 scaffold_aligned.fa | \u251c\u2500\u2500 scaffold_assembly.fa | \u251c\u2500\u2500 scaffold_imputed.fa | \u251c\u2500\u2500 scaffold_padded.out | \u251c\u2500\u2500 trimmed_1.fastq | \u251c\u2500\u2500 trimmed_2.fastq | \u251c\u2500\u2500 trimmed_U.fastq | \u2514\u2500\u2500 trimmomatic_summary.out \u251c\u2500\u2500 SRR11140746 | \u251c\u2500\u2500 SRR11140746_1.fastq | \u251c\u2500\u2500 SRR11140746_2.fastq | \u2514\u2500\u2500 covid_genome_assembly .... Step 6 - Optional inclusions: PredictHaplo, multiple alignment, and building a phylogenetic tree. Here are some option modules to include within the pipeline. -- Adding PredictHaplo as a stage. If you desire PredictHaplo, you can either utilize the option --interval_txt or you can rerun the pipeline with this gtf file. It is easier for PredictHaplo to run on smaller regions than the entire genome like we implemented above. $ cat SARSCoV2.NC_045512.COVID19.gtf SARSCoV2.NC_045512.COVID19 NCBI_refseq amplicon 265 13482 . + 0 name orf1a ; SARSCoV2.NC_045512.COVID19 NCBI_refseq amplicon 21562 25383 . + 0 name surface ; SARSCoV2.NC_045512.COVID19 NCBI_refseq amplicon 26244 26471 . + 0 name envelope ; Remember, you can find the output file names here . The ouput name for this stage is PH0#.best*.fas . We are doing this stage after the finalize_assembly stage, doing refining the assembly after the scaffold assembly module. Therefore, the input fastq files are named corrected_1.fastq and corrected_2.fastq . Also remember that these files are now contained in the outdirectory specified by the input, so we have to list the path to the input fastq files. Finally, we need to specify that the input reference file is the final.fna file from the finalize assembly stage, which is also located in the outdirectory. We have to do loops for the PredictHaplo and parser stages because there are multiple haplotype regions. Now this in the input for the base haphpipe command for this stage: hp_predict_haplo --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --ref_fa ${outdir}/final.fna\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} The entire stage's bash script is here: stage= predict_haplo echo -e \\n[---$SN---] ($(date)) Stage: $stage for PH in ${outdir}/PH*; do if [[ -e ${PH} ]]; then for PHbest in ${outdir}/PH*/*best*.fas; do if [[ -e ${PHbest} ]]; then echo [---$SN---] ($(date)) EXISTS: $stage $PHbest fi done else read -r -d '' cmd EOF hp_predict_haplo --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --ref_fa ${outdir}/final.fna\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi done -- Following up PredictHaplo with ph_parser In order to use the output from PredictHaplo (see description here ), we need to run the ph_parser stage. hp_ph_parser --haplotypes_fa ${outdir}/${PH}/*best*.fas\\ --prefix ${sampID}_${PH}\\ --logfile ${outdir}/${PH}/haphpipe.out\\ --outdir ${outdir} Now we have to do a loop, because there are multiple regions (i.e., PH0# directories). The entire stage's bash script is here: stage= ph_parser echo -e \\n[---$SN---] ($(date)) Stage: $stage for PH in ${outdir}/PH*/ph_haplotypes.fna; do #if [[ -e ${PH} ]]; then # echo [---$SN---] ($(date)) EXISTS: $stage $PH #else read -r -d '' cmd EOF hp_ph_parser --haplotypes_fa $(dirname $PH)/*best*.fas\\ --prefix ${sampID}_$(dirname $(basename $PH))\\ --logfile ${outdir}/haphpipe.out\\ --outdir $(dirname $PH) EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) #fi done -- Adding multiple_align as a stage Remember, you can find the output file names here . The ouput name for this stage is alignment.fasta . We first need to make a text file that has a list of directories that contin final.fna . Because this is a genome assembly, we want to use the --alignall option to align the entire region and not use a GTF file. We also choose to output a phylip file using the option --phyipout . Now this in the input for the base haphpipe command for this stage: haphpipe multiple_align --ncpu $ncpu\\ --dir_list dir_list.txt\\ --phylipout\\ --alignall\\ --logfile haphpipe.out The entire stage's bash script is here: stage= multiple_align echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/multiple_align/alignment.fasta ]] [[ -e $outdir/multiple_align/alignment.phy ]]; then echo [---$SN---] ($(date)) EXISTS: $stage alignment.fasta,alignment.phy else read -r -d '' cmd EOF haphpipe multiple_align\\ --ncpu $ncpu\\ --dir_list dir_list.txt\\ --phylipout\\ --alignall\\ --logfile haphpipe.out EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi -- Adding model_test as a stage Remember, you can find the output file names here . The ouput name for this stage is modeltest_results.out . The input will be the alignment file from multiple_align . We'll also give the output an id of covid19_genome so the output file will be covid19_genome_modeltest_results.out . We will also chose the template of the output as raxml, since the next module build_tree uses RAxML. Now this in the input for the base haphpipe command for this stage: haphpipe model_test --seqs alignment.fasta\\ --run_id covid_genome\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir}\\ --template raxml\\ --ncpu ${ncpu} The entire stage's bash script is here: stage= model_test echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/covid19_genome_modeltest_results.out ]]; then echo [---$SN---] ($(date)) EXISTS: $stage covid19_genome_modeltest_results.out else read -r -d '' cmd EOF haphpipe model_test --seqs alignment.fasta\\ --run_id covid_genome\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir}\\ --template raxml\\ --ncpu ${ncpu} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi","title":"Making your own pipelines with Bash"},{"location":"demos/","text":"Demo Module (add)","title":"Demo"},{"location":"demos/#demo-module","text":"(add)","title":"Demo Module"},{"location":"expipes/","text":"The example pipelines are written in bash scripting language. The reference files used in both examples are included in the demo data. To run in haphpipe, execute one of the following lines: haphpipe_assemble_01 read1.fq.gz read2.fq.gz ../refs/HIV_B.K03455.HXB2.fasta ../refs/HIV_B.K03455.HXB2.gtf sampleID haphpipe_assemble_02 read1.fq.gz read2.fq.gz ../refs/HIV_B.K03455.HXB2.amplicons.fasta sampleID Pipeline 1 implements amplicon assembly using a de novo approach. Reads are error-corrected and used to refine the initial assembly, with up to 5 refinement steps. Pipeline 2 implements amplicon assembly using a reference-based mapping approach. Reads are error-corrected and used to refine the initial assembly, with up to 5 refinement steps. Pipeline 1: haphpipe_assemble_01 This pipeline implements de novo assembly. Reads are first trimmed ( trim_reads ) and used as input for denovo assembly ( assemble_denovo ). The de novo assembly stage automatically performs error correction on the trimmed reads. The assembled contigs are used as input for amplicon assembly ( assemble_amplicons ) along with reference FASTA and GTF files. The assembly is then iteratively refined up to five times ( refine_assembly ) by mapping corrected reads to the assembled FASTA file and lastly finalized ( finalize_assembly ), resulting in a FASTA file with final consensus sequences, final VCF, and aligned BAM file. To see the input information for Pipeline 1, use the -h option again like so: haphpipe_assemble_01 -h , and it will show the output: USAGE: haphpipe_assemble_01 [read1] [read2] [reference_fasta] [reference_gtf] [samp_id] outdir ----- HAPHPIPE assembly pipeline 01 ----- This pipeline implements amplicon assembly using a denovo approach. Reads are error-corrected and used to refine the initial assembly, with up to 5 refinement steps. Input: read1: Fastq file for read 1. May be compressed (.gz) read2: Fastq file for read 2. May be compressed (.gz) reference_fasta: Reference sequence (fasta) reference_gtf: Amplicon regions (GTF) samp_id: Sample ID outdir: Output directory (default is [sample_dir]/haphpipe_assemble_01) General command to execute pipeline 1: haphpipe_assemble_01 samp/read1.fq.gz samp/read2.fq.gz refs/ref.fasta refs/ref.gtf samp Example command to run with demo samples: haphpipe_assemble_01 SRR8525886/SRR8525886_1.fastq SRR8525886/SRR8525886_2.fastq refs/HIV_B.K03455.HXB2.fasta refs/HIV_B.K03455.HXB2.gtf SRR8525886 SRR8525886 Pipeline 2: haphpipe_assemble_02 This pipeline implements reference-based mapping assembly. Reads are first trimmed ( trim_reads ) and error-corrected ( ec_reads ). The corrected reads are used as input for reference-based mapping assembly ( refine_assembly ) for up to five iterations. Lastly, the assembly is finalized ( finalize_assembly ) by mapping reads onto the refined reference sequence. The final output is a FASTA file with final consensus sequences, final VCF, and aligned BAM file. To see the input information for Pipeline 2, use the -h option again like so: haphpipe_assemble_02 -h , and it will show the output: USAGE: haphpipe_assemble_02 [read1] [read2] [amplicons_fasta] [samp_id] outdir ----- HAPHPIPE assembly pipeline 02 ----- This pipeline implements amplicon assembly using a reference-based approach. Reads are error-corrected and aligned to provided amplicon reference with up to five refinement steps. Input: read1: Fastq file for read 1. May be compressed (.gz) read2: Fastq file for read 2. May be compressed (.gz) amplicons_fasta: Amplicon reference sequence (fasta) samp_id: Sample ID outdir: Output directory (default is sample_dir/haphpipe_assemble_02) General command to execute pipeline 2: haphpipe_assemble_02 samp/read1.fq.gz samp/read2.fq.gz refs/ref.fasta samp Example command to run with demo samples: haphpipe_assemble_02 SRR8525886/SRR8525886_1.fastq SRR8525886/SRR8525886_2.fastq refs/HIV_B.K03455.HXB2.amplicons fasta SRR8525886 SRR8525886","title":"Example Pipelines"},{"location":"expipes/#pipeline-1-haphpipe_assemble_01","text":"This pipeline implements de novo assembly. Reads are first trimmed ( trim_reads ) and used as input for denovo assembly ( assemble_denovo ). The de novo assembly stage automatically performs error correction on the trimmed reads. The assembled contigs are used as input for amplicon assembly ( assemble_amplicons ) along with reference FASTA and GTF files. The assembly is then iteratively refined up to five times ( refine_assembly ) by mapping corrected reads to the assembled FASTA file and lastly finalized ( finalize_assembly ), resulting in a FASTA file with final consensus sequences, final VCF, and aligned BAM file. To see the input information for Pipeline 1, use the -h option again like so: haphpipe_assemble_01 -h , and it will show the output: USAGE: haphpipe_assemble_01 [read1] [read2] [reference_fasta] [reference_gtf] [samp_id] outdir ----- HAPHPIPE assembly pipeline 01 ----- This pipeline implements amplicon assembly using a denovo approach. Reads are error-corrected and used to refine the initial assembly, with up to 5 refinement steps. Input: read1: Fastq file for read 1. May be compressed (.gz) read2: Fastq file for read 2. May be compressed (.gz) reference_fasta: Reference sequence (fasta) reference_gtf: Amplicon regions (GTF) samp_id: Sample ID outdir: Output directory (default is [sample_dir]/haphpipe_assemble_01) General command to execute pipeline 1: haphpipe_assemble_01 samp/read1.fq.gz samp/read2.fq.gz refs/ref.fasta refs/ref.gtf samp Example command to run with demo samples: haphpipe_assemble_01 SRR8525886/SRR8525886_1.fastq SRR8525886/SRR8525886_2.fastq refs/HIV_B.K03455.HXB2.fasta refs/HIV_B.K03455.HXB2.gtf SRR8525886 SRR8525886","title":"Pipeline 1: haphpipe_assemble_01"},{"location":"expipes/#pipeline-2-haphpipe_assemble_02","text":"This pipeline implements reference-based mapping assembly. Reads are first trimmed ( trim_reads ) and error-corrected ( ec_reads ). The corrected reads are used as input for reference-based mapping assembly ( refine_assembly ) for up to five iterations. Lastly, the assembly is finalized ( finalize_assembly ) by mapping reads onto the refined reference sequence. The final output is a FASTA file with final consensus sequences, final VCF, and aligned BAM file. To see the input information for Pipeline 2, use the -h option again like so: haphpipe_assemble_02 -h , and it will show the output: USAGE: haphpipe_assemble_02 [read1] [read2] [amplicons_fasta] [samp_id] outdir ----- HAPHPIPE assembly pipeline 02 ----- This pipeline implements amplicon assembly using a reference-based approach. Reads are error-corrected and aligned to provided amplicon reference with up to five refinement steps. Input: read1: Fastq file for read 1. May be compressed (.gz) read2: Fastq file for read 2. May be compressed (.gz) amplicons_fasta: Amplicon reference sequence (fasta) samp_id: Sample ID outdir: Output directory (default is sample_dir/haphpipe_assemble_02) General command to execute pipeline 2: haphpipe_assemble_02 samp/read1.fq.gz samp/read2.fq.gz refs/ref.fasta samp Example command to run with demo samples: haphpipe_assemble_02 SRR8525886/SRR8525886_1.fastq SRR8525886/SRR8525886_2.fastq refs/HIV_B.K03455.HXB2.amplicons fasta SRR8525886 SRR8525886","title":"Pipeline 2: haphpipe_assemble_02"},{"location":"faq/","text":"HAPHPIPE How does HAPHPIPE compare to existing viral NGS pipelines? Please refer to our papers (links here) for a thorough comparison. How was HAPHPIPE tested? Please refer to our validation study (link here). NGS What is the difference between next-generation sequencing and Sanger sequencing? The critical difference between Sanger sequencing and NGS is sequencing volume. While the Sanger method only sequences a single DNA fragment at a time, NGS is massively parallel, sequencing millions of fragments simultaneously per run. This high-throughput process translates into sequencing hundreds to thousands of genes at one time. NGS also offers greater discovery power to detect novel or rare variants with deep sequencing. I am new to using NGS data. Where should I start? See this link for a beginner's guide to NGS analysis. Using the Command Line I am new to using the command line. Where should I start? See this link for a beginner's guide to the command line. What is a directory structure? Why is this important? In computing, a directory structure is the way an operating system\u2019s file system and its files are displayed to the user. As researchers and software engineers, we should make sure that someone who is unfamiliar with our project is able to look at our computer files and understand in detail what we did and why. We all have the rough experience where after a few months, we may simply not remember what we were up to when we created a particular set of files, or we may be forgetful about what conclusions were drew. We will either have to then spend time reconstructing previous experiments or lose whatever insights gained from those experiments. So maintaining a well-structured directory is essential to both software development and bioinformatics. What is an interactive job and how do I run one? When using HAPHPIPE on an HPC cluster, if stages/pipelines are not being run in a job through SLURM or a similar package manager, HAPHPIPE commands should be run on an interactive node of your HPC. This guide gives instructions for using interactive nodes on GW's ColonialOne. Conda What is Conda? See the Conda documentation here . Bash I am new to using bash. Where should I start? See this link for a beginner's guide to bash scripting.","title":"FAQ"},{"location":"faq/#haphpipe","text":"How does HAPHPIPE compare to existing viral NGS pipelines? Please refer to our papers (links here) for a thorough comparison. How was HAPHPIPE tested? Please refer to our validation study (link here).","title":"HAPHPIPE"},{"location":"faq/#ngs","text":"What is the difference between next-generation sequencing and Sanger sequencing? The critical difference between Sanger sequencing and NGS is sequencing volume. While the Sanger method only sequences a single DNA fragment at a time, NGS is massively parallel, sequencing millions of fragments simultaneously per run. This high-throughput process translates into sequencing hundreds to thousands of genes at one time. NGS also offers greater discovery power to detect novel or rare variants with deep sequencing. I am new to using NGS data. Where should I start? See this link for a beginner's guide to NGS analysis.","title":"NGS"},{"location":"faq/#using-the-command-line","text":"I am new to using the command line. Where should I start? See this link for a beginner's guide to the command line. What is a directory structure? Why is this important? In computing, a directory structure is the way an operating system\u2019s file system and its files are displayed to the user. As researchers and software engineers, we should make sure that someone who is unfamiliar with our project is able to look at our computer files and understand in detail what we did and why. We all have the rough experience where after a few months, we may simply not remember what we were up to when we created a particular set of files, or we may be forgetful about what conclusions were drew. We will either have to then spend time reconstructing previous experiments or lose whatever insights gained from those experiments. So maintaining a well-structured directory is essential to both software development and bioinformatics. What is an interactive job and how do I run one? When using HAPHPIPE on an HPC cluster, if stages/pipelines are not being run in a job through SLURM or a similar package manager, HAPHPIPE commands should be run on an interactive node of your HPC. This guide gives instructions for using interactive nodes on GW's ColonialOne.","title":"Using the Command Line"},{"location":"faq/#conda","text":"What is Conda? See the Conda documentation here .","title":"Conda"},{"location":"faq/#bash","text":"I am new to using bash. Where should I start? See this link for a beginner's guide to bash scripting.","title":"Bash"},{"location":"help/","text":"Conda Bioconda Bioconda channels Bash Command line tutorial List of bash commands Bash scripting tutorial Additional scripting help: linuxconfig.org flaviocopesc.com ryanstutorials.net NGS overview: NYU resource Accessing on a PC using a virtual machine Guide to options VirtualBox","title":"Helpful Resources"},{"location":"help/#conda","text":"Bioconda Bioconda channels","title":"Conda"},{"location":"help/#bash","text":"Command line tutorial List of bash commands Bash scripting tutorial Additional scripting help: linuxconfig.org flaviocopesc.com ryanstutorials.net","title":"Bash"},{"location":"help/#ngs-overview","text":"NYU resource","title":"NGS overview:"},{"location":"help/#accessing-on-a-pc-using-a-virtual-machine","text":"Guide to options VirtualBox","title":"Accessing on a PC using a virtual machine"},{"location":"hp_annotate/","text":"hp_description includes stages to annotate and describe consensus sequence(s) and haplotypes. Use -h after any command for a list of options. pairwise_align Apply correct coordinate system to final sequence(s) to facilitate downstream analyses. Input is the final sequence file in FASTA format, a reference sequence in FASTA format, and a reference GFT file. Output is a JSON file to be used in extract_pairwise . Usage: haphpipe pairwise_align [SETTINGS] --amplicons_fa FASTA --ref_fa FASTA --ref_gtf GTF [--outdir] (or): hp_pairwise_align [SETTINGS] --amplicons_fa FASTA --ref_fa FASTA --ref_gtf GTF [--outdir] Output files: pairwise_aligned.json Input/Output Arguments: Option Description --amplicons_fa Fasta file with assembled amplicons. --ref_fa Reference fasta file. --ref_gtf GTF format file containing amplicon regions. Primary and alternate coding regions should be provided in the attribute field (for amino acid alignment). --outdir Output directory (default: False). Settings: Option Description --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe pairwise_align --amplicons_fa final.fna --ref_fa HIV_B.K03455.HXB2.fasta --ref_gtf HIV_B.K03455.HXB2.gtf extract_pairwise Extract sequence regions from the pairwise alignment produced in pairwise_align . Input is the JSON file from pairwise_align . Output is either an unaligned nucleotide FASTA file, an aligned nucleotide FASTA file, an amino acid FASTA file, an amplicon GTF file, or a tab-separated values (TSV) file (default: nucleotide FASTA with regions of interest from GTF file used in pairwise_align ). Usage: haphpipe extract_pairwise [OPTIONS] [SETTINGS] --align_json JSON [--outdir] (or): hp_extract_pairwise [OPTIONS] [SETTINGS] --align_json JSON [--outdir] Output files: stdout.fasta Input/Output Arguments: Option Description --align_json JSON file describing alignment (output of pairwise_align stage). --outfile Output file (default: stdout). Options: Option Description --outfmt Format for output: nuc_fa, aln_fa, amp_gtf, ost, or prot_fa (default: nuc_fa). --refreg Reference region. String format is ref:start-stop. For example, the region string to extract pol when aligned to HXB2 is HIV_B.K03455.HXB2:2085-5096. Settings: Option Description --debug Print commands but do not run (default: False). Example usage: haphpipe extract_pairwise --align_json pairwise_aligned.json --refreg HIV_B.K03455.HXB2:2085-5096 summary_stats Report summary statistics from an alignment and/or haplotype calling as TXT and TSV files. Input is a list of paths to directories (TXT format, one per line), each of which contain the following files: final_bt2.out , trimmomatic_summary.out , final.bam , final.fna , and final.vcf.gz . If applicable, also input a list of directories containing PredictHaplo summary files ( ph_summary.txt ). If amplicons were used in assembly, use the --amplicons option to report statistics per amplicon. Usage: haphpipe summary_stats [SETTINGS] --dir_list TXT [--ph_list TXT ] [--amplicons] [--outdir] (or): hp_summary_stats [SETTINGS] --dir_list TXT [--ph_list TXT ] [--amplicons] [--outdir] Output files: summary_stats.txt, summary_stats.tsv, PH_summary_stats.tsv Input/Output Arguments: Option Description --dir_list List of directories which include the required files, one on each line. --ph_list List of directories which include haplotype summary files, one on each line. --amplicons Amplicons used in assembly (default: False). Settings: Option Description --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Name for log file. --debug Print commands but do not run (default: False). Example usage: haphpipe summary_stats --dir_list demo_sra_list.txt --ph_list demo_sra_ph_list.txt --amplicons annotate_ from_ref Annotate consensus sequence from reference annotation. Input is JSON file from pairwise_align and reference GTF file. Usage: haphpipe annotate_from_ref [OPTIONS] [SETTINGS] --haplotypes_fa best.fas [--outdir] (or): hp_annotate_from_ref [OPTIONS] [SETTINGS] --haplotypes_fa best.fas [--outdir] Output files: Input/Output Arguments Settings: Example usage: (add)","title":"Description"},{"location":"hp_annotate/#pairwise_align","text":"Apply correct coordinate system to final sequence(s) to facilitate downstream analyses. Input is the final sequence file in FASTA format, a reference sequence in FASTA format, and a reference GFT file. Output is a JSON file to be used in extract_pairwise . Usage: haphpipe pairwise_align [SETTINGS] --amplicons_fa FASTA --ref_fa FASTA --ref_gtf GTF [--outdir] (or): hp_pairwise_align [SETTINGS] --amplicons_fa FASTA --ref_fa FASTA --ref_gtf GTF [--outdir] Output files: pairwise_aligned.json Input/Output Arguments: Option Description --amplicons_fa Fasta file with assembled amplicons. --ref_fa Reference fasta file. --ref_gtf GTF format file containing amplicon regions. Primary and alternate coding regions should be provided in the attribute field (for amino acid alignment). --outdir Output directory (default: False). Settings: Option Description --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe pairwise_align --amplicons_fa final.fna --ref_fa HIV_B.K03455.HXB2.fasta --ref_gtf HIV_B.K03455.HXB2.gtf","title":"pairwise_align"},{"location":"hp_annotate/#extract_pairwise","text":"Extract sequence regions from the pairwise alignment produced in pairwise_align . Input is the JSON file from pairwise_align . Output is either an unaligned nucleotide FASTA file, an aligned nucleotide FASTA file, an amino acid FASTA file, an amplicon GTF file, or a tab-separated values (TSV) file (default: nucleotide FASTA with regions of interest from GTF file used in pairwise_align ). Usage: haphpipe extract_pairwise [OPTIONS] [SETTINGS] --align_json JSON [--outdir] (or): hp_extract_pairwise [OPTIONS] [SETTINGS] --align_json JSON [--outdir] Output files: stdout.fasta Input/Output Arguments: Option Description --align_json JSON file describing alignment (output of pairwise_align stage). --outfile Output file (default: stdout). Options: Option Description --outfmt Format for output: nuc_fa, aln_fa, amp_gtf, ost, or prot_fa (default: nuc_fa). --refreg Reference region. String format is ref:start-stop. For example, the region string to extract pol when aligned to HXB2 is HIV_B.K03455.HXB2:2085-5096. Settings: Option Description --debug Print commands but do not run (default: False). Example usage: haphpipe extract_pairwise --align_json pairwise_aligned.json --refreg HIV_B.K03455.HXB2:2085-5096","title":"extract_pairwise"},{"location":"hp_annotate/#summary_stats","text":"Report summary statistics from an alignment and/or haplotype calling as TXT and TSV files. Input is a list of paths to directories (TXT format, one per line), each of which contain the following files: final_bt2.out , trimmomatic_summary.out , final.bam , final.fna , and final.vcf.gz . If applicable, also input a list of directories containing PredictHaplo summary files ( ph_summary.txt ). If amplicons were used in assembly, use the --amplicons option to report statistics per amplicon. Usage: haphpipe summary_stats [SETTINGS] --dir_list TXT [--ph_list TXT ] [--amplicons] [--outdir] (or): hp_summary_stats [SETTINGS] --dir_list TXT [--ph_list TXT ] [--amplicons] [--outdir] Output files: summary_stats.txt, summary_stats.tsv, PH_summary_stats.tsv Input/Output Arguments: Option Description --dir_list List of directories which include the required files, one on each line. --ph_list List of directories which include haplotype summary files, one on each line. --amplicons Amplicons used in assembly (default: False). Settings: Option Description --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Name for log file. --debug Print commands but do not run (default: False). Example usage: haphpipe summary_stats --dir_list demo_sra_list.txt --ph_list demo_sra_ph_list.txt --amplicons","title":"summary_stats"},{"location":"hp_annotate/#annotate_-from_ref","text":"Annotate consensus sequence from reference annotation. Input is JSON file from pairwise_align and reference GTF file. Usage: haphpipe annotate_from_ref [OPTIONS] [SETTINGS] --haplotypes_fa best.fas [--outdir] (or): hp_annotate_from_ref [OPTIONS] [SETTINGS] --haplotypes_fa best.fas [--outdir] Output files: Input/Output Arguments Settings: Example usage: (add)","title":"annotate_ from_ref"},{"location":"hp_assemble/","text":"Stages in hp_assemble are designed to construct consensus sequence(s). Input reads (in FASTQ format) are assembled using either denovo assembly or reference-based alignment. Resulting consensus can be further refined. Use -h after any command for a list of options. assemble_denovo Assemble reads via de novo assembly using SPAdes ( documentation ). Input is reads in FASTQ format. Output is contigs in FNA format. Usage: haphpipe assemble_denovo [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ [--outdir] (or): hp_assemble_denovo [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ [--outdir] Output files: denovo_contigs.fna denovo_summary.txt Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --outdir Output directory (default: current directory). Options: Option Description --no_error_correction Do not perform error correction (default: False) --subsample Use a subsample of reads for assembly --seed Seed for random number generator (ignored if not subsampling) Settings: Option Description --ncpu Number of CPU to use (default: 1). --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe assemble_denovo --fq1 corrected_1.fastq --fq2 corrected_2.fastq --outdir denovo_assembly --no_error_correction TRUE assemble_amplicons Assemble contigs from de novo assembly using both a reference sequence and amplicon regions with MUMMER 3+ ( documentation ). Input is contigs and reference sequence in FASTA format and amplicon regions in GTF format. Usage: haphpipe assemble_amplicons [OPTIONS] [SETTINGS] --contigs_fa FASTA --ref_fa FASTA --ref_gtf GTF [--outdir] (or): hp_assemble_amplicons [OPTIONS] [SETTINGS] --contigs_fa FASTA --ref_fa FASTA --ref_gtf GTF [--outdir] Output files: amplicon_assembly.fna Input/Output Arguments: Option Description --contigs_fa Fasta file with assembled contigs. --ref_fa Fasta file with reference genome to scaffold against. --ref_gtf GTF format file containing amplicon regions. --outdir Output directory (default: current directory). Scaffold Options: Option Description --sample_id Sample ID (default: sampleXX). --padding Bases to include outside reference annotation (default: 50). Settings: Option Description --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe assemble_amplicons --contigs_fa denovo_contigs.fa --ref_fa HIV_B.K03455.HXB2.fasta --ref_gtf HIV_B.K03455.HXB2.gtf assemble_scaffold Scaffold contigs against a reference sequence with MUMMER 3+ ( documentation ). Input is contigs in FASTA format and reference sequence in FASTA format. Output is scaffold assembly, alligned scaffold, imputed scaffold, and padded scaffold in FASTA format. Usage: haphpipe assemble_scaffold [OPTIONS] [SETTINGS] --contigs_fa FASTA --ref_fa FASTA [--outdir] (or): hp_assemble_scaffold [OPTIONS] [SETTINGS] --contigs_fa FASTA --ref_fa FASTA [--outdir] Output files: scaffold_aligned.fa scaffold_assembly.fa scaffold_imputed.fa scaffold_padded.out Input/Output Arguments: Option Description --contigs_fa Fasta file with assembled contigs. --ref_fa Fasta file with reference genome to scaffold against. --outdir Output directory (default: current directory). Options: Option Description --seqname Name to append to scaffold sequence (default: sample01). Settings: Option Description --keep_tmp Additional options (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe assemble_scaffold --contigs_fa denovo_contigs.fa --ref_fa HIV_B.K03455.HXB2.fasta align_reads Map reads to reference sequence (instead of running de novo assembly) using Bowtie2 ( documentation ) and Picard ( documentation ). Input is reads in FASTQ format and reference sequence in FASTA format. Usage: haphpipe align_reads [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ --ref_fa FASTA [--outdir] (or): hp_align_reads [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ --ref_fa FASTA [--outdir] Output files: aligned.bam aligned.bt2.out Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --ref_fa Reference fasta file. --outdir Output directory (default: current directory). Options: Option Description --bt2_preset {very-fast, fast, sensitive,very-sensitive,very-fast-local,fast-local,sensitive-local,very-sensitive-local} --sample_id Sample ID. Used as read group ID in BAM (default: sampleXX). --no_realign Do not realign indels (default: False). --remove_duplicates Remove duplicates from final alignment. Otherwise duplicates are marked but not removed (default: False). --encoding {Phred+33,Phred+64} Quality score encoding. Settings: Option Description --ncpu Number of CPUs to use (default: 1). --xmx Maximum heap size for Java VM, in GB (default: 32). --keep_tmp Additional options (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe align_reads --fq1 corrected_1.fastq --fq2 corrected _2.fastq --ref_fa HIV_B.K03455.HXB2.fasta call_variants Variant calling from alignment using GATK ( documentation ). Input is alignment file in BAM format and reference sequence in FASTA format (either reference from reference-based assembly or consensus final sequence from de novo assembly). Output is a Variant Call File (VCF) format file. Usage: haphpipe call_variants [OPTIONS] [SETTINGS] --aln_bam BAM --ref_fa FASTA [--outdir] (or): hp_call_variants [OPTIONS] [SETTINGS] --aln_bam BAM --ref_fa FASTA [--outdir] Output files: variants.vcf.gz Input/Output Arguments: Option Description --aln_bam Alignment file. --ref_fa Reference fasta file. --outdir Output directory (default: False). Options: Option Description --emit_all Output calls for all site (default: False). --min_base_qual Minimum base quality required to consider a base for calling (default: 15). Settings: Option Description --ncpu Number of CPUs to use (default: 1). --xmx Maximum heap size for Java VM, in GB (default: 32). --keep_tmp Additional options (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe call_variants --aln_bam alignment.bam --ref_fa HIV_B.K03455.HXB2.fasta vcf_to_consensus Generate a consensus sequence from a VCF file. Input is a VCF file. Output is the consensus sequence in FASTA format. Usage: haphpipe vcf_to_consensus [OPTIONS] [SETTINGS] --vcf FASTQ [--outdir] [--sampidx] (or): hp_vcf_to_consensus [OPTIONS] [SETTINGS] --vcf FASTQ [--outdir] [--sampidx] Output files: consensus.fna Input/Output Arguments: Option Description --vcf VCF file (created with all sites). --outdir Output directory (default: False). --sampidx Index for sample if multi-sample VCF (default: 0). Options: Option Description --min_DP Minimum depth to call site (default: 1). --major Allele fraction to make unambiguous call (default: 0.5). --minor Allele fraction to make ambiguous call (default: 0.2). Settings: Option Description --keep_tmp Additional options (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. Example usage: haphpipe vcf_to_consensus --vcf variants.vcf refine_assembly Map reads to a denovo assembly or reference alignment. Assembly or alignment is iteratively updated. Input is reads in FASTQ format and reference sequence (assembly or reference alignment) in FASTA format. Output is refined assembly in FASTA format. Usage: haphpipe refine_assembly [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ --ref_fa FASTA [--outdir] (or): hp_refine_assembly [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ --ref_fa FASTA [--outdir] Output files: refined.fna Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --ref_fa Reference fasta file. --outdir Output directory (default: False). Options: Option Description --max_step Maximum number of refinement steps (default: 1). --subsample Use a subsample of reads for refinement. --seed Seed for random number generator (ignored if not subsampling). --sample_id Sample ID. Used as read group ID in BAM (default: sampleXX). Settings: Option Description --ncpu Number of CPUs to use (default: 1). --xmx Maximum heap size for Java VM, in GB (default: 32). --keep_tmp Additional options (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe refine_assembly --fq_1 corrected_1.fastq --fq2 corrected_2.fastq --ref_fa HIV_B.K03455.HXB2.fasta finalize_assembly Finalize consensus, map reads to consensus, and call variants. Input is reads in FASTQ format and reference sequence in FASTA format. Output is finalized reference sequence, alignment, and variants (in FASTA, BAM, and VCF formats, respectively). Usage: haphpipe finalize_assembly [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ --ref_fa FASTA [--outdir] (or): hp_finalize_assembly [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ --ref_fa FASTA [--outdir] Output files: final.fna final.ban final.vcf.gz Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --ref_fa Consensus fasta file. --outdir Output directory (default: current directory). Options: Option Description --bt2_preset {very-fast,fast,sensitive,very-sensitive,very-fast-local,fast-local,sensitive-local,very-sensitive-local} Bowtie2 preset to use (default: very-sensitive). --sample_id Sample ID (default: sampleXX). Settings: Option Description --ncpu Number of CPU to use (default: 1). --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe finalize_assembly --fq_1 corrected_1.fastq --fq2 corrected_2.fastq --ref_fa refined.fna","title":"Assemble"},{"location":"hp_assemble/#assemble_denovo","text":"Assemble reads via de novo assembly using SPAdes ( documentation ). Input is reads in FASTQ format. Output is contigs in FNA format. Usage: haphpipe assemble_denovo [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ [--outdir] (or): hp_assemble_denovo [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ [--outdir] Output files: denovo_contigs.fna denovo_summary.txt Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --outdir Output directory (default: current directory). Options: Option Description --no_error_correction Do not perform error correction (default: False) --subsample Use a subsample of reads for assembly --seed Seed for random number generator (ignored if not subsampling) Settings: Option Description --ncpu Number of CPU to use (default: 1). --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe assemble_denovo --fq1 corrected_1.fastq --fq2 corrected_2.fastq --outdir denovo_assembly --no_error_correction TRUE","title":"assemble_denovo"},{"location":"hp_assemble/#assemble_amplicons","text":"Assemble contigs from de novo assembly using both a reference sequence and amplicon regions with MUMMER 3+ ( documentation ). Input is contigs and reference sequence in FASTA format and amplicon regions in GTF format. Usage: haphpipe assemble_amplicons [OPTIONS] [SETTINGS] --contigs_fa FASTA --ref_fa FASTA --ref_gtf GTF [--outdir] (or): hp_assemble_amplicons [OPTIONS] [SETTINGS] --contigs_fa FASTA --ref_fa FASTA --ref_gtf GTF [--outdir] Output files: amplicon_assembly.fna Input/Output Arguments: Option Description --contigs_fa Fasta file with assembled contigs. --ref_fa Fasta file with reference genome to scaffold against. --ref_gtf GTF format file containing amplicon regions. --outdir Output directory (default: current directory). Scaffold Options: Option Description --sample_id Sample ID (default: sampleXX). --padding Bases to include outside reference annotation (default: 50). Settings: Option Description --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe assemble_amplicons --contigs_fa denovo_contigs.fa --ref_fa HIV_B.K03455.HXB2.fasta --ref_gtf HIV_B.K03455.HXB2.gtf","title":"assemble_amplicons"},{"location":"hp_assemble/#assemble_scaffold","text":"Scaffold contigs against a reference sequence with MUMMER 3+ ( documentation ). Input is contigs in FASTA format and reference sequence in FASTA format. Output is scaffold assembly, alligned scaffold, imputed scaffold, and padded scaffold in FASTA format. Usage: haphpipe assemble_scaffold [OPTIONS] [SETTINGS] --contigs_fa FASTA --ref_fa FASTA [--outdir] (or): hp_assemble_scaffold [OPTIONS] [SETTINGS] --contigs_fa FASTA --ref_fa FASTA [--outdir] Output files: scaffold_aligned.fa scaffold_assembly.fa scaffold_imputed.fa scaffold_padded.out Input/Output Arguments: Option Description --contigs_fa Fasta file with assembled contigs. --ref_fa Fasta file with reference genome to scaffold against. --outdir Output directory (default: current directory). Options: Option Description --seqname Name to append to scaffold sequence (default: sample01). Settings: Option Description --keep_tmp Additional options (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe assemble_scaffold --contigs_fa denovo_contigs.fa --ref_fa HIV_B.K03455.HXB2.fasta","title":"assemble_scaffold"},{"location":"hp_assemble/#align_reads","text":"Map reads to reference sequence (instead of running de novo assembly) using Bowtie2 ( documentation ) and Picard ( documentation ). Input is reads in FASTQ format and reference sequence in FASTA format. Usage: haphpipe align_reads [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ --ref_fa FASTA [--outdir] (or): hp_align_reads [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ --ref_fa FASTA [--outdir] Output files: aligned.bam aligned.bt2.out Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --ref_fa Reference fasta file. --outdir Output directory (default: current directory). Options: Option Description --bt2_preset {very-fast, fast, sensitive,very-sensitive,very-fast-local,fast-local,sensitive-local,very-sensitive-local} --sample_id Sample ID. Used as read group ID in BAM (default: sampleXX). --no_realign Do not realign indels (default: False). --remove_duplicates Remove duplicates from final alignment. Otherwise duplicates are marked but not removed (default: False). --encoding {Phred+33,Phred+64} Quality score encoding. Settings: Option Description --ncpu Number of CPUs to use (default: 1). --xmx Maximum heap size for Java VM, in GB (default: 32). --keep_tmp Additional options (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe align_reads --fq1 corrected_1.fastq --fq2 corrected _2.fastq --ref_fa HIV_B.K03455.HXB2.fasta","title":"align_reads"},{"location":"hp_assemble/#call_variants","text":"Variant calling from alignment using GATK ( documentation ). Input is alignment file in BAM format and reference sequence in FASTA format (either reference from reference-based assembly or consensus final sequence from de novo assembly). Output is a Variant Call File (VCF) format file. Usage: haphpipe call_variants [OPTIONS] [SETTINGS] --aln_bam BAM --ref_fa FASTA [--outdir] (or): hp_call_variants [OPTIONS] [SETTINGS] --aln_bam BAM --ref_fa FASTA [--outdir] Output files: variants.vcf.gz Input/Output Arguments: Option Description --aln_bam Alignment file. --ref_fa Reference fasta file. --outdir Output directory (default: False). Options: Option Description --emit_all Output calls for all site (default: False). --min_base_qual Minimum base quality required to consider a base for calling (default: 15). Settings: Option Description --ncpu Number of CPUs to use (default: 1). --xmx Maximum heap size for Java VM, in GB (default: 32). --keep_tmp Additional options (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe call_variants --aln_bam alignment.bam --ref_fa HIV_B.K03455.HXB2.fasta","title":"call_variants"},{"location":"hp_assemble/#vcf_to_consensus","text":"Generate a consensus sequence from a VCF file. Input is a VCF file. Output is the consensus sequence in FASTA format. Usage: haphpipe vcf_to_consensus [OPTIONS] [SETTINGS] --vcf FASTQ [--outdir] [--sampidx] (or): hp_vcf_to_consensus [OPTIONS] [SETTINGS] --vcf FASTQ [--outdir] [--sampidx] Output files: consensus.fna Input/Output Arguments: Option Description --vcf VCF file (created with all sites). --outdir Output directory (default: False). --sampidx Index for sample if multi-sample VCF (default: 0). Options: Option Description --min_DP Minimum depth to call site (default: 1). --major Allele fraction to make unambiguous call (default: 0.5). --minor Allele fraction to make ambiguous call (default: 0.2). Settings: Option Description --keep_tmp Additional options (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. Example usage: haphpipe vcf_to_consensus --vcf variants.vcf","title":"vcf_to_consensus"},{"location":"hp_assemble/#refine_assembly","text":"Map reads to a denovo assembly or reference alignment. Assembly or alignment is iteratively updated. Input is reads in FASTQ format and reference sequence (assembly or reference alignment) in FASTA format. Output is refined assembly in FASTA format. Usage: haphpipe refine_assembly [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ --ref_fa FASTA [--outdir] (or): hp_refine_assembly [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ --ref_fa FASTA [--outdir] Output files: refined.fna Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --ref_fa Reference fasta file. --outdir Output directory (default: False). Options: Option Description --max_step Maximum number of refinement steps (default: 1). --subsample Use a subsample of reads for refinement. --seed Seed for random number generator (ignored if not subsampling). --sample_id Sample ID. Used as read group ID in BAM (default: sampleXX). Settings: Option Description --ncpu Number of CPUs to use (default: 1). --xmx Maximum heap size for Java VM, in GB (default: 32). --keep_tmp Additional options (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe refine_assembly --fq_1 corrected_1.fastq --fq2 corrected_2.fastq --ref_fa HIV_B.K03455.HXB2.fasta","title":"refine_assembly"},{"location":"hp_assemble/#finalize_assembly","text":"Finalize consensus, map reads to consensus, and call variants. Input is reads in FASTQ format and reference sequence in FASTA format. Output is finalized reference sequence, alignment, and variants (in FASTA, BAM, and VCF formats, respectively). Usage: haphpipe finalize_assembly [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ --ref_fa FASTA [--outdir] (or): hp_finalize_assembly [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ --ref_fa FASTA [--outdir] Output files: final.fna final.ban final.vcf.gz Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --ref_fa Consensus fasta file. --outdir Output directory (default: current directory). Options: Option Description --bt2_preset {very-fast,fast,sensitive,very-sensitive,very-fast-local,fast-local,sensitive-local,very-sensitive-local} Bowtie2 preset to use (default: very-sensitive). --sample_id Sample ID (default: sampleXX). Settings: Option Description --ncpu Number of CPU to use (default: 1). --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe finalize_assembly --fq_1 corrected_1.fastq --fq2 corrected_2.fastq --ref_fa refined.fna","title":"finalize_assembly"},{"location":"hp_haplotype/","text":"hp_haplotype includes haplotype assembly stages. HAPHPIPE implements PredictHaplo ( paper ), although other haplotype reconstruction programs can be utilized outside of HAPHPIPE using the final output of HAPHPIPE, typically with the final consensus sequence (FASTA) file, reads (raw, trimmed, and/or corrected), and/or final alignment (BAM) file as input. Use -h after any command for a list of options. predict_haplo Haplotype identification with PredictHaplo. Input is reads in FASTQ format and and reference sequence in FASTA format. Output is the longest global haplotype file and corresponding HTML file. Note: PredictHaplo must be installed separately before running this stage. Usage: haphpipe predict_haplo [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --ref_fa FASTA --interval_txt [TXT] [--outdir] (or): hp_predict_haplo [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --ref_fa FASTA --interval_txt [TXT] [--outdir] Output files: best.fa Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --ref_fa Reference sequence used to align reads (Fasta). --interval_txt File with intervals to perform haplotype reconstruction. --outdir Output directory (default: current directory). Options: Option Description --min_readlength Minimum read length passed to PredictHaplo (default: 36). Settings: Option Description --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe predict_haplo corrected_1.fastq --fq2 corrected_2.fastq --ref_fa final.fna ph_parser Returns PredictHaplo output as a correctly formatted FASTA file. Input is the output file from predict_haplo (longest global .fas file). Output is a correctly formatted FASTA file. Usage: haphpipe ph_parser [OPTIONS] [SETTINGS] --haplotypes_fa best.fas [--outdir] (or): hp_ph_parser [OPTIONS] [SETTINGS] --haplotypes_fa best.fas [--outdir] Output files: ph_summary.txt ph_haplotypes.fna Input/Output Arguments: Option Description --haplotypes_fa Haplotype file created by PredictHaplo. --outdir Output directory (default: current directory). Options: Option Description --prefix Prefix to add to sequence names. --keep_gaps Do not remove gaps from alignment (default: False). Settings: Option Description --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. Example usage: haphpipe ph_parser PH01.best_1_864.fas Example: By default, PredictHaplo outputs their own unique version of a fasta file. It includes the frequency, some information regarding their unqiue overlapping scores, and their unique confidence scores. This file is always named PH#.best_#_#.fas , where the first number is the reconstructed haplotype number and the next numbers are the start and end of the longest haplotype reconstructed by PredictHaplo. $ cat PH01.best_1_1208.fas reconstructed_0 ;Freq:0.190638 ;Overlap quality scores (5,10):1,1 ;Confidence scores ;~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~n7~~~y~~~~~t~~~i~jjk~~zz ;~~~~{~~~~~N{~{~Sx~~~~~~z~K~F~~~~~~~y~~~~~~~|~~F~wx|~~{~~|~~s |]~~~~~~ ;~m~kj|{|v~{|_`~~~z~~~~~~~jy{y~~~~~a~__~~|~~~~~{wXZ~}~~~qm~xV~~~~~~~}~ ;Q~}~y||~~~}}~~~z~~~~~~{}A~}b|~~u~~|}}|~}}z~}bx~~n||~~||{~}~~d}~bz~~~} ;|~}}~~~}~~~{|}}g{~~~}~r~}}~~~~u~~{kx{~}~}~|~}~}{}~}~}||~~~~[~}~}}~~~~ ;~~}~~~U||U}~|}}~~}}~~~~}u~b|}~w~~~~~{}|wv~}Dxzp{|}~@~~P~}~}~V~~z}~|ry ;q~}|}~}~~}t~o~}~f}~~}{~~~}~{~~~~~~~}~~}~~|~~~M~~}~}x~}c~}^v~~~yzA~}~} ;y}~z}~~~~~~~~~{z~~}~~~}~{}~~~~~}~~~~~~~|~~v~~}~~|y~]|{~||~~~~~~~~||~~ ;Y||~~|Q~|~~~|~~~~~|~~~z~~z~{{{y~~~~~~~~~~w{{~wz|~Z|~z|~~}p|~~|}}~~x}} ;z}~}}|a|}}}}{}|~~}}~}}~}~{~|~}}~}{}|}|}}}~|}}}}{}}}|}}|}|}}}|}}}}~}}} ;||}}}}{}}~}~}}}}}}}}}}}~}}}}}}}}}}}}}}}}}y}~}}}}}}}}}}~|}}}}|}}}}|}}} ;}}}}}}}}|}}}}}}}}~}}}}}}|}}}}}}}}|}}}}}|}}}}|~|}}}}{}}}}}}}}}}}}}}}}} ;}}=}}}{}}}}}}}}}}}}}}}}}}}}}}}}~||}}|~}}}}}{}}}}}|}}|}}}}}}}}}}}}}|}} ;|}~}}}}|}}}}}}}}|}|~J}}}}}}~}}}}}}}}}}}}}}}}}}}}|}}`~}}}}|}}}}}}}}}}~ ;|}}}}}}}}}}}~}}}|}}}}}}}}}}|}}}}}}}}}}}t}}{}`}}}}}}|}~}~}|~}}}|k}}}}} ;|}|~}}|}}~}}~}|}}z}}}}}}}}}}}}}}~}}~}}}~~}|}}}~}}}}}}}}~}~}}||~~}~z}} ;~~}~}||~|~}|{}||~|z~~||}~|~}}|}~~}|}}~}}|}z~~~~{~}{}~y~~~~~{|}}~~|y~~ ;~|~~||~~~~~~~|n~~{~~~~~~~~~~~~~~~ ;EndOfComments ACCAAATGAAAGATTGTACTGAGAGACAGGCTAATTTTTTAGGGAATATCTGGCCTTCCCGCAAGGGAAG GCCAGGGAACTTTCCTCAGAGCAGGCTAGAGCCAACAGCCCCACCAGAAGAGAGCTTCAGGCTTGGGGAA GCAACAACAAAGCCCTCTCAGAAACAGGAGCCAATAGACAAGGAAATGTATCCTTTAACTTCCCTCAGAT CACTCTTTGGCAACGACCCCTTGTCACAATAAAGATAGGGGAGCAACTAAAGGAAGCCCTGTTAGATACA GGAGCAGATGATACAGTATTAGAAGAAATGAATTTGCCAGGAAGATGGAAACCAAGAATGATAGGGGGAA TCGGGGGTTTTATCAAAGTAAGACAGTATGATCAGATAGCCATAGACATCTGTGGCCATAAAGCTATAGG TACAGTATTAGTAGGACCTACACCTGTCAACATAATTGGAAGAAATCTGTTGACTCAGCTTGGTTGCACT TTAAATTTTCCCATTAGTCCTATTGAAACTGTACCAGTAAAATTGAAGCCAGGAATGGATGGCCCAAAGG TTAAACAATGGCCATTGACAGAAGAAAAAATAAAAGCATTAGTAGAAATTTGTACAGAAATGGAAAAAGA AGGGAAAATTTCAAAAATTGGGCCTGAAAATCCATACAATACTCCAGTATTTGCCATAAGGAAAAAAGAC AGTACTAAATGGAGAAAATTAGTAGATTTCAGAGAACTTAATAAGAGAACTCAGGACTTCTGGGAAGTCC AATTAGGAATACCACATCCCTCAGGGTTAAAAAAGAAAAAATCAGTAACAGTACTGGATGCGGGTGATGC ATATTTTTCAGTTCCCTTAGATGAAGACTTCAGAAAGTATACTGCATTTACCATACCTAGTGTAAACAAT GAGACACTAGGGATTAGGTATCAGTACAATGTGCTTCCACAAGGATGGAAAGGATCACCAGCAATATTCC AAGCTAGCATGACAAAAATCTTAGAGCCTTTTAGAAAACAAAATCCAGACATAGTTATCTATCAATACAT GGATGATCTGTATGTAGGATCTGACCTAGAAATAGGGCAGCATAGAACAAAAATAGAGGAACTGAGAGAA CATCTGTTGAGGTGGGGGTTTTGCACACCAGACAAGAAACATCAGAAGGAACCTCCATTCCTTTGGATGG GTTATGAACTCCATCCTT reconstructed_1 ;Freq:0.294104 ;Overlap quality scores (5,10):1,1 ;Confidence scores ;~~~~~~~~z~~~~~~~~~|~y~~~~u}|~~~~~}}~~~~~~~ya~|~~}}~~~y~~~j~YXH~~s~ ;~~~~z~~|~~b|}^}xZ}}~}z~t~r~{}~}x}}yb}~~~}~}u~~}~gu|}}{~|}}~ozsw}|}}h~ ;|l}v|^][t}zz}vw}}s}}}}~v}|~|~~}}}~~}}}}}~}}}}}}zo\\}}}~}vn}iv}}}}}}}}} ;v}}~|u}}}}}}}}}|}}}}}}|}^}}[}~}r}}}{|}}|t|}}{z}}pz}}{}}}}}}}v~}y|~}}} ;}}}}~}}~}}}tz}}`}}}~}}q}}}}}~}y}}|r||}}}}~}}~}}{}}}}}}}}}}}y~}}}}t}}} ;}}}}}}{|}|}}{r}}}}tv~}}}t}m|~}h}}}}}v}}qt}|y|{|u~}}|}~_}~|~~|}~|}~}|e ;Y}~}}~}}~}v}t}}}j}}~}}}~}}~|}}}}|}}|}}}~}}}}};}}~}}x}}|}}yu}}}wy`}|~~ ;{}}x}}}}}~}}}~{v}}}}}{}~}}~}~}}|}}}}}}}}}}u}}y~~{g}L}}}}}|~}{~|}|}|}} ;l}}}}{_}}}}~z~}|}~~|~~x|~}~z~~|~}~~~}~~~}y}~~|k}}Pf}w~~~~T}}}F~~}~z}{ ;{~|}|zZ~s~|}r~}}~~|}}}}}~|}}}}n}}}~}}}}~}}}}}}}}}}}|~}}}}}}~~}}~~~~~} ;N}}}}}|~}}~}}}}{}}}~}}|~~|}|}}}~|}~}}}}}}D}~}}}}}}}}}}}}}}}}|}}z}|}}} ;}k}}}}}}}}}}}}}}}}}}}|}}}}}|}}}}}|}}}}}|}}}}|~{~}}}G}~}}}}}~}}}}}}}}} ;}}h}}~|}|}}}}}}}}}}}}}}}}}}}}}}}}{}}|}}}}|~|}}}}}}}M}}}}}}}~}}}}}}}}} ;}}~|}}}}}~~~}}}~}}|}g}}}}}}}}~|}}}}}}}}}}}}}}}}}|}~t~}|}}}}}}}||~}}}} ;|}}}}~}}}}}}~}}}gi}}~}}}}}}}}}}}}|}}~}}g}~}}v}}}}}|{}}}}}}~}}~|e}}}}} ;}}|}}~}~}~}}~}|~}|}~|||}}}}|~}}~}}}}}}}}|}}~}}~}|~}|y~}~}}}}}|}}~~}~} ;}}~}~}|~}}}~}}}}~}{}}~}}~}}}||~~}||~}x|}{~|}|~~|~}}}|{~}}~}{}}||~}}}~ ;|~}~}}}~|}}~~|,~~~~~~~~~~~~~~m~~~ ;EndOfComments ACCAAATGAAAGATTGTACTGAGAGACAGGCTAATTTTTTAGGGAAGATCTGGCCTTCCCGCGGCGGAAG GCCAGGGAATTTTCTTCAGAGCAGACCAGAGCCAACAGCCCCACCAGAAGAGAGCTTCAGGTTTGGGGAG GAGACAACAACTCCCTCTCAGAAGCAGGAGCCGAAGGACAAGGAAACATATCCTTTAGCTTCCCTCAAAT CACTCTTTGGCAACGACCCCTTGTTACAATAAAGATAGGGGGGCAGCTAAAGGAAGCTCTATTAGATACA GGAGCAGATGACACAGTATTAGAAGAAATGGATTTGCCAGGAAGATGGAAACCAAAAATGATAGGGGGAA TTGGAGGTTTTATCAAAGTAAAACAGTATGATCAGATACCCATAGAAATTTGTGGACATAAAGTGATAGG TACAGTATTAGTAGGACCTACACCTGTCAACATAATTGGAAGAAATCTGTTGACTCAGCTTGGTTGCACT TTAAATTTTCCCATTAGTCCTATTGAAACTGTACCAGTAAAATTGAAGCCAGGAATGGATGGCCCAAAGG TTAAACAATGGCCATTGACAGAAGAAAAAATAAAAGCATTAATAGAAATCTGTGCAGAAATGGAAAAGGA AGGGAAAATTTCAAAAATTGGGCCTGAAAATCCATACAATACTCCAGTATTTGCCATAAGAAAAAAAGAC AGTACTAAATGGAGAAAATTAGTAGATTTCAAAGAACTTAATAAGAGAACTCAGGACTTCTGGGAAGTCC AATTAGGAATACCACATCCCTCAGGGTTAAAAAAGAAAAAGTCAGTAACAGTACTGGATGTGGGTGATGC ATATTTTTCAGTTCCCTTAGATGAAGACTTCAGAAAGTACACTGCATTTACCATACCTAGTGTAAACAAT GAGACACCAGGGATTAGGTATCAGTACAATGTGCTTCCACAAGGATGGAAAGGATCACCAGCAATATTCC AATGTAGCATGACAAAAATCTTAGAACCTTTTAGAAAACAAAATCCAGATATAGTTATCTATCAATACAT GGATGATCTGTATGTAGGATCTGACCTAGAAATAGGGCAGCATAGAACAAAAATAGAGGAACTGAGAGAA CATCTGTTGAGGTGGGGGTTTTGCACACCAGACAAGAAACATCAGAAGGAACCTCCATTCCTTTGGATGG GTTATGAACTCCATCCTT reconstructed_2 ;Freq:0.515259 ;Overlap quality scores (5,10):4,4 ;Confidence scores ;~~~~~~~w~~zz~~~~z~~w~|wy~|~~{|~~|~~~~~~~~}~{~hD~~~}|~|}}t~~_;~tue}}S} ;~}~~p}}l}~P}}ZlvL~z}}h~g~A~x}}}s}}{S~}~}}}}l}~|}SOq}}q}w}}~i\\pz~w~~z~ ;}7~|vzUu4|smtgm}~W}}~~}r}n`f`}{PZLSI:Vs_V_}}}}_ db}}}}}CQ}YJ}t}}}~}}} ;T}}~|}}}}}}}~}}k}}}}}}q}L}}]}}}L}}}tt|~}{q}}{M}lAIu}}}s{}}}~y~}st}|}} ;z}}}}|}}}}}{q|}Z}}}}}}P~}|}|}}E}}cLbi}vu}}|}}|}g}}}}}}|}~}~{}~}|}{}}} ;}~}|}}}{{|~}pp}~~~{|}}~}[yEx}}\\}}}{}N}uoK}}pauSy}}}v~~J}}q}}v~}c}~}\\i ;]|}}~}|}~}r}Y}}}B}}~||~}}}}{}}}|}|~|}~~~~}}~{w}{}~|R|}s|}k{~}}}ra}q}} ;o}}d}~}}}}}~}}|X~}}~}e}}}}}}}|}y|}}}}~}|~}c~~b~~nL~a}}}}}~}}m}}~{}}}} ;g}}}}uLm~l}}`~|n}}~}}|d~~y~u}|tz~~~~|}}~~t}}}tx}|Tu~v}}~}^|~~w~|~}m{} ;u}}}|w;}{}{}r}||}}z|}|~|}z}||}y}}}~}}}|}}~}}}|}y}}}}}}}~}||}}}}}}~}}| ;v{}|}}{}}}|~|}}|}}}}}}}}}}|}}}}}|}~}}}}}}p}}||}}}}}~}}}{}}}}}}}z}}}}} ;}|}}~}}}|}}}}}}}}}}}}}}}}}}}}}}|}||}}|}|}}}}|}|}}}}u}}}}}}}}}}}}}}}}} ;}}g}}}{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}y}}}}}|}m}}}}}}}}}}}}}}}}} ;}}}}}}}}}}}}}}}}}}|}i}}}}}}}}}}|}}}}}}}}}}}}}}}}}}}H}}}}}}}}}|}{}}}}} ;|}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}y}}|}H}}}}}}}}}}}}}}}}}}u}}}}} ;}}}}}}}}}}}}}}}}}z}}}}}}}}}}}}}}}}|}}}|}}}}}}}}}|}}}}}}}}}}}}|}}}}|}} ;}}}}}}}}}}}}||}|}}|}}}}}}|}}|}}}~}|}|}}}|}{}}}}|}}}}|{|}}}}x||}}}|{}} ;}}}}}}}~}}~}~|n}~}}}}}~}}~~|~~~y~, ;EndOfComments ACCAAATGAAAGATTGTACTGAGAGACAGGCTAATTTTTTAGGGAAAATCTGGCCTTCCCACAAGGGAAG GCCAGGGAATTTTCTTCAGAGCAGACCAGAGCCAACAGCCCCACCAGAAGAGAGCTTCAGGTTTGGGGAA GAGACAACAACTCCCTCTCAGAAGCAGGAGCCGATAGACAAGGAAATGTATCCTTTAGCTTCCCTCAAAT CACTCTTTGGCAACGACCCCTCGTCACAATAAAGATAGGGGGGCAACTAAAGGAAGCTCTATTAGATACA GGAGCAGATGATACAGTATTAGAAGAAATGAATTTGCCAGGAAGATGGAAACCAAAAATGATAGGGGGAA TTGGAGGTTTTATCAAAGTAAAACAGTATGATCAGATACCCATAGAAATCTGTGGACATAAAGCTATAGG TACAGTATTAGTAGGACCTACACCTGTCAACATAATTGGAAGAAATCTGTTGACTCAGATTGGTTGCACT TTAAATTTTCCCATTAGTCCTATTGAAACTGTACCAGTAAAATTAAAGCCAGGAATGGATGGCCCAAAAG TTAAACAATGGCCATTGACAGAAGAAAAAATAAAAGCATTAGTAGAAATTTGTACAGAAATGGAAAAGGA AGGGAAAATTTCAAAAATTGGGCCTGAAAATCCATACAATACTCCAGTATTTGCCATAAGGAAAAAAGAC AGTACTAAATGGAGAAAATTAGTAGATTTCAGAGAACTTAATAAGAGAACTCAAGACTTCCGGGAAGTCC AATTAGGAATACCACATCCCTCAGGGTTAAAAAAGAAAAAATCAGTAACAGTACTGGATGTGGGTGATGC ATATTTTTCAGTTCCCTTAGATGAAGACTTCAGAAAGTATACTGCATTTACCATACCTAGTGTAAACAAT GAGACACCAGGGATTAGGTATCAGTACAATGTGCTTCCACAAGGATGGAAAGGATCACCAGCAATATTCC AAGCTAGCATGACAAAAATCTTAGAGCCTTTTAGAAAACAAAATCCAGACATAGTTATCTATCAATACAT GGATGATCTGTATGTAGGATCTGACCTAGAAATAGGGCAGCATAGAACAAAAATAGAGGAACTGAGAGAA CATCTGTTGAGGTGGGGGTTTTGCACACCAGACAAGAAACATCAGAAGGAACCTCCATTCCTTTGGATGG GTTATGAACTCCATCCCC ph_parser takes this output and creates a proper fasta file with each resontructed haplotype and a text file that has the hpalotype diversity estimate. $ cat ph_summary.txt PH_num_hap 3 PH_hap_diversity 0.611668153059 PH_seq_len 1208 $ cat ph_haplotypes.fna reconstructed_0 Freq=0.190638 ACCAAATGAAAGATTGTACTGAGAGACAGGCTAATTTTTTAGGGAATATCTGGCCTTCCCGCAAGGGAAG GCCAGGGAACTTTCCTCAGAGCAGGCTAGAGCCAACAGCCCCACCAGAAGAGAGCTTCAGGCTTGGGGAA GCAACAACAAAGCCCTCTCAGAAACAGGAGCCAATAGACAAGGAAATGTATCCTTTAACTTCCCTCAGAT CACTCTTTGGCAACGACCCCTTGTCACAATAAAGATAGGGGAGCAACTAAAGGAAGCCCTGTTAGATACA GGAGCAGATGATACAGTATTAGAAGAAATGAATTTGCCAGGAAGATGGAAACCAAGAATGATAGGGGGAA TCGGGGGTTTTATCAAAGTAAGACAGTATGATCAGATAGCCATAGACATCTGTGGCCATAAAGCTATAGG TACAGTATTAGTAGGACCTACACCTGTCAACATAATTGGAAGAAATCTGTTGACTCAGCTTGGTTGCACT TTAAATTTTCCCATTAGTCCTATTGAAACTGTACCAGTAAAATTGAAGCCAGGAATGGATGGCCCAAAGG TTAAACAATGGCCATTGACAGAAGAAAAAATAAAAGCATTAGTAGAAATTTGTACAGAAATGGAAAAAGA AGGGAAAATTTCAAAAATTGGGCCTGAAAATCCATACAATACTCCAGTATTTGCCATAAGGAAAAAAGAC AGTACTAAATGGAGAAAATTAGTAGATTTCAGAGAACTTAATAAGAGAACTCAGGACTTCTGGGAAGTCC AATTAGGAATACCACATCCCTCAGGGTTAAAAAAGAAAAAATCAGTAACAGTACTGGATGCGGGTGATGC ATATTTTTCAGTTCCCTTAGATGAAGACTTCAGAAAGTATACTGCATTTACCATACCTAGTGTAAACAAT GAGACACTAGGGATTAGGTATCAGTACAATGTGCTTCCACAAGGATGGAAAGGATCACCAGCAATATTCC AAGCTAGCATGACAAAAATCTTAGAGCCTTTTAGAAAACAAAATCCAGACATAGTTATCTATCAATACAT GGATGATCTGTATGTAGGATCTGACCTAGAAATAGGGCAGCATAGAACAAAAATAGAGGAACTGAGAGAA CATCTGTTGAGGTGGGGGTTTTGCACACCAGACAAGAAACATCAGAAGGAACCTCCATTCCTTTGGATGG GTTATGAACTCCATCCTT reconstructed_1 Freq=0.294104 ACCAAATGAAAGATTGTACTGAGAGACAGGCTAATTTTTTAGGGAAGATCTGGCCTTCCCGCGGCGGAAG GCCAGGGAATTTTCTTCAGAGCAGACCAGAGCCAACAGCCCCACCAGAAGAGAGCTTCAGGTTTGGGGAG GAGACAACAACTCCCTCTCAGAAGCAGGAGCCGAAGGACAAGGAAACATATCCTTTAGCTTCCCTCAAAT CACTCTTTGGCAACGACCCCTTGTTACAATAAAGATAGGGGGGCAGCTAAAGGAAGCTCTATTAGATACA GGAGCAGATGACACAGTATTAGAAGAAATGGATTTGCCAGGAAGATGGAAACCAAAAATGATAGGGGGAA TTGGAGGTTTTATCAAAGTAAAACAGTATGATCAGATACCCATAGAAATTTGTGGACATAAAGTGATAGG TACAGTATTAGTAGGACCTACACCTGTCAACATAATTGGAAGAAATCTGTTGACTCAGCTTGGTTGCACT TTAAATTTTCCCATTAGTCCTATTGAAACTGTACCAGTAAAATTGAAGCCAGGAATGGATGGCCCAAAGG TTAAACAATGGCCATTGACAGAAGAAAAAATAAAAGCATTAATAGAAATCTGTGCAGAAATGGAAAAGGA AGGGAAAATTTCAAAAATTGGGCCTGAAAATCCATACAATACTCCAGTATTTGCCATAAGAAAAAAAGAC AGTACTAAATGGAGAAAATTAGTAGATTTCAAAGAACTTAATAAGAGAACTCAGGACTTCTGGGAAGTCC AATTAGGAATACCACATCCCTCAGGGTTAAAAAAGAAAAAGTCAGTAACAGTACTGGATGTGGGTGATGC ATATTTTTCAGTTCCCTTAGATGAAGACTTCAGAAAGTACACTGCATTTACCATACCTAGTGTAAACAAT GAGACACCAGGGATTAGGTATCAGTACAATGTGCTTCCACAAGGATGGAAAGGATCACCAGCAATATTCC AATGTAGCATGACAAAAATCTTAGAACCTTTTAGAAAACAAAATCCAGATATAGTTATCTATCAATACAT GGATGATCTGTATGTAGGATCTGACCTAGAAATAGGGCAGCATAGAACAAAAATAGAGGAACTGAGAGAA CATCTGTTGAGGTGGGGGTTTTGCACACCAGACAAGAAACATCAGAAGGAACCTCCATTCCTTTGGATGG GTTATGAACTCCATCCTT reconstructed_2 Freq=0.515259 ACCAAATGAAAGATTGTACTGAGAGACAGGCTAATTTTTTAGGGAAAATCTGGCCTTCCCACAAGGGAAG GCCAGGGAATTTTCTTCAGAGCAGACCAGAGCCAACAGCCCCACCAGAAGAGAGCTTCAGGTTTGGGGAA GAGACAACAACTCCCTCTCAGAAGCAGGAGCCGATAGACAAGGAAATGTATCCTTTAGCTTCCCTCAAAT CACTCTTTGGCAACGACCCCTCGTCACAATAAAGATAGGGGGGCAACTAAAGGAAGCTCTATTAGATACA GGAGCAGATGATACAGTATTAGAAGAAATGAATTTGCCAGGAAGATGGAAACCAAAAATGATAGGGGGAA TTGGAGGTTTTATCAAAGTAAAACAGTATGATCAGATACCCATAGAAATCTGTGGACATAAAGCTATAGG TACAGTATTAGTAGGACCTACACCTGTCAACATAATTGGAAGAAATCTGTTGACTCAGATTGGTTGCACT TTAAATTTTCCCATTAGTCCTATTGAAACTGTACCAGTAAAATTAAAGCCAGGAATGGATGGCCCAAAAG TTAAACAATGGCCATTGACAGAAGAAAAAATAAAAGCATTAGTAGAAATTTGTACAGAAATGGAAAAGGA AGGGAAAATTTCAAAAATTGGGCCTGAAAATCCATACAATACTCCAGTATTTGCCATAAGGAAAAAAGAC AGTACTAAATGGAGAAAATTAGTAGATTTCAGAGAACTTAATAAGAGAACTCAAGACTTCCGGGAAGTCC AATTAGGAATACCACATCCCTCAGGGTTAAAAAAGAAAAAATCAGTAACAGTACTGGATGTGGGTGATGC ATATTTTTCAGTTCCCTTAGATGAAGACTTCAGAAAGTATACTGCATTTACCATACCTAGTGTAAACAAT GAGACACCAGGGATTAGGTATCAGTACAATGTGCTTCCACAAGGATGGAAAGGATCACCAGCAATATTCC AAGCTAGCATGACAAAAATCTTAGAGCCTTTTAGAAAACAAAATCCAGACATAGTTATCTATCAATACAT GGATGATCTGTATGTAGGATCTGACCTAGAAATAGGGCAGCATAGAACAAAAATAGAGGAACTGAGAGAA CATCTGTTGAGGTGGGGGTTTTGCACACCAGACAAGAAACATCAGAAGGAACCTCCATTCCTTTGGATGG GTTATGAACTCCATCCCC","title":"Haplotype"},{"location":"hp_haplotype/#predict_haplo","text":"Haplotype identification with PredictHaplo. Input is reads in FASTQ format and and reference sequence in FASTA format. Output is the longest global haplotype file and corresponding HTML file. Note: PredictHaplo must be installed separately before running this stage. Usage: haphpipe predict_haplo [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --ref_fa FASTA --interval_txt [TXT] [--outdir] (or): hp_predict_haplo [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --ref_fa FASTA --interval_txt [TXT] [--outdir] Output files: best.fa Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --ref_fa Reference sequence used to align reads (Fasta). --interval_txt File with intervals to perform haplotype reconstruction. --outdir Output directory (default: current directory). Options: Option Description --min_readlength Minimum read length passed to PredictHaplo (default: 36). Settings: Option Description --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe predict_haplo corrected_1.fastq --fq2 corrected_2.fastq --ref_fa final.fna","title":"predict_haplo"},{"location":"hp_haplotype/#ph_parser","text":"Returns PredictHaplo output as a correctly formatted FASTA file. Input is the output file from predict_haplo (longest global .fas file). Output is a correctly formatted FASTA file. Usage: haphpipe ph_parser [OPTIONS] [SETTINGS] --haplotypes_fa best.fas [--outdir] (or): hp_ph_parser [OPTIONS] [SETTINGS] --haplotypes_fa best.fas [--outdir] Output files: ph_summary.txt ph_haplotypes.fna Input/Output Arguments: Option Description --haplotypes_fa Haplotype file created by PredictHaplo. --outdir Output directory (default: current directory). Options: Option Description --prefix Prefix to add to sequence names. --keep_gaps Do not remove gaps from alignment (default: False). Settings: Option Description --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. Example usage: haphpipe ph_parser PH01.best_1_864.fas Example: By default, PredictHaplo outputs their own unique version of a fasta file. It includes the frequency, some information regarding their unqiue overlapping scores, and their unique confidence scores. This file is always named PH#.best_#_#.fas , where the first number is the reconstructed haplotype number and the next numbers are the start and end of the longest haplotype reconstructed by PredictHaplo. $ cat PH01.best_1_1208.fas reconstructed_0 ;Freq:0.190638 ;Overlap quality scores (5,10):1,1 ;Confidence scores ;~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~n7~~~y~~~~~t~~~i~jjk~~zz ;~~~~{~~~~~N{~{~Sx~~~~~~z~K~F~~~~~~~y~~~~~~~|~~F~wx|~~{~~|~~s |]~~~~~~ ;~m~kj|{|v~{|_`~~~z~~~~~~~jy{y~~~~~a~__~~|~~~~~{wXZ~}~~~qm~xV~~~~~~~}~ ;Q~}~y||~~~}}~~~z~~~~~~{}A~}b|~~u~~|}}|~}}z~}bx~~n||~~||{~}~~d}~bz~~~} ;|~}}~~~}~~~{|}}g{~~~}~r~}}~~~~u~~{kx{~}~}~|~}~}{}~}~}||~~~~[~}~}}~~~~ ;~~}~~~U||U}~|}}~~}}~~~~}u~b|}~w~~~~~{}|wv~}Dxzp{|}~@~~P~}~}~V~~z}~|ry ;q~}|}~}~~}t~o~}~f}~~}{~~~}~{~~~~~~~}~~}~~|~~~M~~}~}x~}c~}^v~~~yzA~}~} ;y}~z}~~~~~~~~~{z~~}~~~}~{}~~~~~}~~~~~~~|~~v~~}~~|y~]|{~||~~~~~~~~||~~ ;Y||~~|Q~|~~~|~~~~~|~~~z~~z~{{{y~~~~~~~~~~w{{~wz|~Z|~z|~~}p|~~|}}~~x}} ;z}~}}|a|}}}}{}|~~}}~}}~}~{~|~}}~}{}|}|}}}~|}}}}{}}}|}}|}|}}}|}}}}~}}} ;||}}}}{}}~}~}}}}}}}}}}}~}}}}}}}}}}}}}}}}}y}~}}}}}}}}}}~|}}}}|}}}}|}}} ;}}}}}}}}|}}}}}}}}~}}}}}}|}}}}}}}}|}}}}}|}}}}|~|}}}}{}}}}}}}}}}}}}}}}} ;}}=}}}{}}}}}}}}}}}}}}}}}}}}}}}}~||}}|~}}}}}{}}}}}|}}|}}}}}}}}}}}}}|}} ;|}~}}}}|}}}}}}}}|}|~J}}}}}}~}}}}}}}}}}}}}}}}}}}}|}}`~}}}}|}}}}}}}}}}~ ;|}}}}}}}}}}}~}}}|}}}}}}}}}}|}}}}}}}}}}}t}}{}`}}}}}}|}~}~}|~}}}|k}}}}} ;|}|~}}|}}~}}~}|}}z}}}}}}}}}}}}}}~}}~}}}~~}|}}}~}}}}}}}}~}~}}||~~}~z}} ;~~}~}||~|~}|{}||~|z~~||}~|~}}|}~~}|}}~}}|}z~~~~{~}{}~y~~~~~{|}}~~|y~~ ;~|~~||~~~~~~~|n~~{~~~~~~~~~~~~~~~ ;EndOfComments ACCAAATGAAAGATTGTACTGAGAGACAGGCTAATTTTTTAGGGAATATCTGGCCTTCCCGCAAGGGAAG GCCAGGGAACTTTCCTCAGAGCAGGCTAGAGCCAACAGCCCCACCAGAAGAGAGCTTCAGGCTTGGGGAA GCAACAACAAAGCCCTCTCAGAAACAGGAGCCAATAGACAAGGAAATGTATCCTTTAACTTCCCTCAGAT CACTCTTTGGCAACGACCCCTTGTCACAATAAAGATAGGGGAGCAACTAAAGGAAGCCCTGTTAGATACA GGAGCAGATGATACAGTATTAGAAGAAATGAATTTGCCAGGAAGATGGAAACCAAGAATGATAGGGGGAA TCGGGGGTTTTATCAAAGTAAGACAGTATGATCAGATAGCCATAGACATCTGTGGCCATAAAGCTATAGG TACAGTATTAGTAGGACCTACACCTGTCAACATAATTGGAAGAAATCTGTTGACTCAGCTTGGTTGCACT TTAAATTTTCCCATTAGTCCTATTGAAACTGTACCAGTAAAATTGAAGCCAGGAATGGATGGCCCAAAGG TTAAACAATGGCCATTGACAGAAGAAAAAATAAAAGCATTAGTAGAAATTTGTACAGAAATGGAAAAAGA AGGGAAAATTTCAAAAATTGGGCCTGAAAATCCATACAATACTCCAGTATTTGCCATAAGGAAAAAAGAC AGTACTAAATGGAGAAAATTAGTAGATTTCAGAGAACTTAATAAGAGAACTCAGGACTTCTGGGAAGTCC AATTAGGAATACCACATCCCTCAGGGTTAAAAAAGAAAAAATCAGTAACAGTACTGGATGCGGGTGATGC ATATTTTTCAGTTCCCTTAGATGAAGACTTCAGAAAGTATACTGCATTTACCATACCTAGTGTAAACAAT GAGACACTAGGGATTAGGTATCAGTACAATGTGCTTCCACAAGGATGGAAAGGATCACCAGCAATATTCC AAGCTAGCATGACAAAAATCTTAGAGCCTTTTAGAAAACAAAATCCAGACATAGTTATCTATCAATACAT GGATGATCTGTATGTAGGATCTGACCTAGAAATAGGGCAGCATAGAACAAAAATAGAGGAACTGAGAGAA CATCTGTTGAGGTGGGGGTTTTGCACACCAGACAAGAAACATCAGAAGGAACCTCCATTCCTTTGGATGG GTTATGAACTCCATCCTT reconstructed_1 ;Freq:0.294104 ;Overlap quality scores (5,10):1,1 ;Confidence scores ;~~~~~~~~z~~~~~~~~~|~y~~~~u}|~~~~~}}~~~~~~~ya~|~~}}~~~y~~~j~YXH~~s~ ;~~~~z~~|~~b|}^}xZ}}~}z~t~r~{}~}x}}yb}~~~}~}u~~}~gu|}}{~|}}~ozsw}|}}h~ ;|l}v|^][t}zz}vw}}s}}}}~v}|~|~~}}}~~}}}}}~}}}}}}zo\\}}}~}vn}iv}}}}}}}}} ;v}}~|u}}}}}}}}}|}}}}}}|}^}}[}~}r}}}{|}}|t|}}{z}}pz}}{}}}}}}}v~}y|~}}} ;}}}}~}}~}}}tz}}`}}}~}}q}}}}}~}y}}|r||}}}}~}}~}}{}}}}}}}}}}}y~}}}}t}}} ;}}}}}}{|}|}}{r}}}}tv~}}}t}m|~}h}}}}}v}}qt}|y|{|u~}}|}~_}~|~~|}~|}~}|e ;Y}~}}~}}~}v}t}}}j}}~}}}~}}~|}}}}|}}|}}}~}}}}};}}~}}x}}|}}yu}}}wy`}|~~ ;{}}x}}}}}~}}}~{v}}}}}{}~}}~}~}}|}}}}}}}}}}u}}y~~{g}L}}}}}|~}{~|}|}|}} ;l}}}}{_}}}}~z~}|}~~|~~x|~}~z~~|~}~~~}~~~}y}~~|k}}Pf}w~~~~T}}}F~~}~z}{ ;{~|}|zZ~s~|}r~}}~~|}}}}}~|}}}}n}}}~}}}}~}}}}}}}}}}}|~}}}}}}~~}}~~~~~} ;N}}}}}|~}}~}}}}{}}}~}}|~~|}|}}}~|}~}}}}}}D}~}}}}}}}}}}}}}}}}|}}z}|}}} ;}k}}}}}}}}}}}}}}}}}}}|}}}}}|}}}}}|}}}}}|}}}}|~{~}}}G}~}}}}}~}}}}}}}}} ;}}h}}~|}|}}}}}}}}}}}}}}}}}}}}}}}}{}}|}}}}|~|}}}}}}}M}}}}}}}~}}}}}}}}} ;}}~|}}}}}~~~}}}~}}|}g}}}}}}}}~|}}}}}}}}}}}}}}}}}|}~t~}|}}}}}}}||~}}}} ;|}}}}~}}}}}}~}}}gi}}~}}}}}}}}}}}}|}}~}}g}~}}v}}}}}|{}}}}}}~}}~|e}}}}} ;}}|}}~}~}~}}~}|~}|}~|||}}}}|~}}~}}}}}}}}|}}~}}~}|~}|y~}~}}}}}|}}~~}~} ;}}~}~}|~}}}~}}}}~}{}}~}}~}}}||~~}||~}x|}{~|}|~~|~}}}|{~}}~}{}}||~}}}~ ;|~}~}}}~|}}~~|,~~~~~~~~~~~~~~m~~~ ;EndOfComments ACCAAATGAAAGATTGTACTGAGAGACAGGCTAATTTTTTAGGGAAGATCTGGCCTTCCCGCGGCGGAAG GCCAGGGAATTTTCTTCAGAGCAGACCAGAGCCAACAGCCCCACCAGAAGAGAGCTTCAGGTTTGGGGAG GAGACAACAACTCCCTCTCAGAAGCAGGAGCCGAAGGACAAGGAAACATATCCTTTAGCTTCCCTCAAAT CACTCTTTGGCAACGACCCCTTGTTACAATAAAGATAGGGGGGCAGCTAAAGGAAGCTCTATTAGATACA GGAGCAGATGACACAGTATTAGAAGAAATGGATTTGCCAGGAAGATGGAAACCAAAAATGATAGGGGGAA TTGGAGGTTTTATCAAAGTAAAACAGTATGATCAGATACCCATAGAAATTTGTGGACATAAAGTGATAGG TACAGTATTAGTAGGACCTACACCTGTCAACATAATTGGAAGAAATCTGTTGACTCAGCTTGGTTGCACT TTAAATTTTCCCATTAGTCCTATTGAAACTGTACCAGTAAAATTGAAGCCAGGAATGGATGGCCCAAAGG TTAAACAATGGCCATTGACAGAAGAAAAAATAAAAGCATTAATAGAAATCTGTGCAGAAATGGAAAAGGA AGGGAAAATTTCAAAAATTGGGCCTGAAAATCCATACAATACTCCAGTATTTGCCATAAGAAAAAAAGAC AGTACTAAATGGAGAAAATTAGTAGATTTCAAAGAACTTAATAAGAGAACTCAGGACTTCTGGGAAGTCC AATTAGGAATACCACATCCCTCAGGGTTAAAAAAGAAAAAGTCAGTAACAGTACTGGATGTGGGTGATGC ATATTTTTCAGTTCCCTTAGATGAAGACTTCAGAAAGTACACTGCATTTACCATACCTAGTGTAAACAAT GAGACACCAGGGATTAGGTATCAGTACAATGTGCTTCCACAAGGATGGAAAGGATCACCAGCAATATTCC AATGTAGCATGACAAAAATCTTAGAACCTTTTAGAAAACAAAATCCAGATATAGTTATCTATCAATACAT GGATGATCTGTATGTAGGATCTGACCTAGAAATAGGGCAGCATAGAACAAAAATAGAGGAACTGAGAGAA CATCTGTTGAGGTGGGGGTTTTGCACACCAGACAAGAAACATCAGAAGGAACCTCCATTCCTTTGGATGG GTTATGAACTCCATCCTT reconstructed_2 ;Freq:0.515259 ;Overlap quality scores (5,10):4,4 ;Confidence scores ;~~~~~~~w~~zz~~~~z~~w~|wy~|~~{|~~|~~~~~~~~}~{~hD~~~}|~|}}t~~_;~tue}}S} ;~}~~p}}l}~P}}ZlvL~z}}h~g~A~x}}}s}}{S~}~}}}}l}~|}SOq}}q}w}}~i\\pz~w~~z~ ;}7~|vzUu4|smtgm}~W}}~~}r}n`f`}{PZLSI:Vs_V_}}}}_ db}}}}}CQ}YJ}t}}}~}}} ;T}}~|}}}}}}}~}}k}}}}}}q}L}}]}}}L}}}tt|~}{q}}{M}lAIu}}}s{}}}~y~}st}|}} ;z}}}}|}}}}}{q|}Z}}}}}}P~}|}|}}E}}cLbi}vu}}|}}|}g}}}}}}|}~}~{}~}|}{}}} ;}~}|}}}{{|~}pp}~~~{|}}~}[yEx}}\\}}}{}N}uoK}}pauSy}}}v~~J}}q}}v~}c}~}\\i ;]|}}~}|}~}r}Y}}}B}}~||~}}}}{}}}|}|~|}~~~~}}~{w}{}~|R|}s|}k{~}}}ra}q}} ;o}}d}~}}}}}~}}|X~}}~}e}}}}}}}|}y|}}}}~}|~}c~~b~~nL~a}}}}}~}}m}}~{}}}} ;g}}}}uLm~l}}`~|n}}~}}|d~~y~u}|tz~~~~|}}~~t}}}tx}|Tu~v}}~}^|~~w~|~}m{} ;u}}}|w;}{}{}r}||}}z|}|~|}z}||}y}}}~}}}|}}~}}}|}y}}}}}}}~}||}}}}}}~}}| ;v{}|}}{}}}|~|}}|}}}}}}}}}}|}}}}}|}~}}}}}}p}}||}}}}}~}}}{}}}}}}}z}}}}} ;}|}}~}}}|}}}}}}}}}}}}}}}}}}}}}}|}||}}|}|}}}}|}|}}}}u}}}}}}}}}}}}}}}}} ;}}g}}}{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}y}}}}}|}m}}}}}}}}}}}}}}}}} ;}}}}}}}}}}}}}}}}}}|}i}}}}}}}}}}|}}}}}}}}}}}}}}}}}}}H}}}}}}}}}|}{}}}}} ;|}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}y}}|}H}}}}}}}}}}}}}}}}}}u}}}}} ;}}}}}}}}}}}}}}}}}z}}}}}}}}}}}}}}}}|}}}|}}}}}}}}}|}}}}}}}}}}}}|}}}}|}} ;}}}}}}}}}}}}||}|}}|}}}}}}|}}|}}}~}|}|}}}|}{}}}}|}}}}|{|}}}}x||}}}|{}} ;}}}}}}}~}}~}~|n}~}}}}}~}}~~|~~~y~, ;EndOfComments ACCAAATGAAAGATTGTACTGAGAGACAGGCTAATTTTTTAGGGAAAATCTGGCCTTCCCACAAGGGAAG GCCAGGGAATTTTCTTCAGAGCAGACCAGAGCCAACAGCCCCACCAGAAGAGAGCTTCAGGTTTGGGGAA GAGACAACAACTCCCTCTCAGAAGCAGGAGCCGATAGACAAGGAAATGTATCCTTTAGCTTCCCTCAAAT CACTCTTTGGCAACGACCCCTCGTCACAATAAAGATAGGGGGGCAACTAAAGGAAGCTCTATTAGATACA GGAGCAGATGATACAGTATTAGAAGAAATGAATTTGCCAGGAAGATGGAAACCAAAAATGATAGGGGGAA TTGGAGGTTTTATCAAAGTAAAACAGTATGATCAGATACCCATAGAAATCTGTGGACATAAAGCTATAGG TACAGTATTAGTAGGACCTACACCTGTCAACATAATTGGAAGAAATCTGTTGACTCAGATTGGTTGCACT TTAAATTTTCCCATTAGTCCTATTGAAACTGTACCAGTAAAATTAAAGCCAGGAATGGATGGCCCAAAAG TTAAACAATGGCCATTGACAGAAGAAAAAATAAAAGCATTAGTAGAAATTTGTACAGAAATGGAAAAGGA AGGGAAAATTTCAAAAATTGGGCCTGAAAATCCATACAATACTCCAGTATTTGCCATAAGGAAAAAAGAC AGTACTAAATGGAGAAAATTAGTAGATTTCAGAGAACTTAATAAGAGAACTCAAGACTTCCGGGAAGTCC AATTAGGAATACCACATCCCTCAGGGTTAAAAAAGAAAAAATCAGTAACAGTACTGGATGTGGGTGATGC ATATTTTTCAGTTCCCTTAGATGAAGACTTCAGAAAGTATACTGCATTTACCATACCTAGTGTAAACAAT GAGACACCAGGGATTAGGTATCAGTACAATGTGCTTCCACAAGGATGGAAAGGATCACCAGCAATATTCC AAGCTAGCATGACAAAAATCTTAGAGCCTTTTAGAAAACAAAATCCAGACATAGTTATCTATCAATACAT GGATGATCTGTATGTAGGATCTGACCTAGAAATAGGGCAGCATAGAACAAAAATAGAGGAACTGAGAGAA CATCTGTTGAGGTGGGGGTTTTGCACACCAGACAAGAAACATCAGAAGGAACCTCCATTCCTTTGGATGG GTTATGAACTCCATCCCC ph_parser takes this output and creates a proper fasta file with each resontructed haplotype and a text file that has the hpalotype diversity estimate. $ cat ph_summary.txt PH_num_hap 3 PH_hap_diversity 0.611668153059 PH_seq_len 1208 $ cat ph_haplotypes.fna reconstructed_0 Freq=0.190638 ACCAAATGAAAGATTGTACTGAGAGACAGGCTAATTTTTTAGGGAATATCTGGCCTTCCCGCAAGGGAAG GCCAGGGAACTTTCCTCAGAGCAGGCTAGAGCCAACAGCCCCACCAGAAGAGAGCTTCAGGCTTGGGGAA GCAACAACAAAGCCCTCTCAGAAACAGGAGCCAATAGACAAGGAAATGTATCCTTTAACTTCCCTCAGAT CACTCTTTGGCAACGACCCCTTGTCACAATAAAGATAGGGGAGCAACTAAAGGAAGCCCTGTTAGATACA GGAGCAGATGATACAGTATTAGAAGAAATGAATTTGCCAGGAAGATGGAAACCAAGAATGATAGGGGGAA TCGGGGGTTTTATCAAAGTAAGACAGTATGATCAGATAGCCATAGACATCTGTGGCCATAAAGCTATAGG TACAGTATTAGTAGGACCTACACCTGTCAACATAATTGGAAGAAATCTGTTGACTCAGCTTGGTTGCACT TTAAATTTTCCCATTAGTCCTATTGAAACTGTACCAGTAAAATTGAAGCCAGGAATGGATGGCCCAAAGG TTAAACAATGGCCATTGACAGAAGAAAAAATAAAAGCATTAGTAGAAATTTGTACAGAAATGGAAAAAGA AGGGAAAATTTCAAAAATTGGGCCTGAAAATCCATACAATACTCCAGTATTTGCCATAAGGAAAAAAGAC AGTACTAAATGGAGAAAATTAGTAGATTTCAGAGAACTTAATAAGAGAACTCAGGACTTCTGGGAAGTCC AATTAGGAATACCACATCCCTCAGGGTTAAAAAAGAAAAAATCAGTAACAGTACTGGATGCGGGTGATGC ATATTTTTCAGTTCCCTTAGATGAAGACTTCAGAAAGTATACTGCATTTACCATACCTAGTGTAAACAAT GAGACACTAGGGATTAGGTATCAGTACAATGTGCTTCCACAAGGATGGAAAGGATCACCAGCAATATTCC AAGCTAGCATGACAAAAATCTTAGAGCCTTTTAGAAAACAAAATCCAGACATAGTTATCTATCAATACAT GGATGATCTGTATGTAGGATCTGACCTAGAAATAGGGCAGCATAGAACAAAAATAGAGGAACTGAGAGAA CATCTGTTGAGGTGGGGGTTTTGCACACCAGACAAGAAACATCAGAAGGAACCTCCATTCCTTTGGATGG GTTATGAACTCCATCCTT reconstructed_1 Freq=0.294104 ACCAAATGAAAGATTGTACTGAGAGACAGGCTAATTTTTTAGGGAAGATCTGGCCTTCCCGCGGCGGAAG GCCAGGGAATTTTCTTCAGAGCAGACCAGAGCCAACAGCCCCACCAGAAGAGAGCTTCAGGTTTGGGGAG GAGACAACAACTCCCTCTCAGAAGCAGGAGCCGAAGGACAAGGAAACATATCCTTTAGCTTCCCTCAAAT CACTCTTTGGCAACGACCCCTTGTTACAATAAAGATAGGGGGGCAGCTAAAGGAAGCTCTATTAGATACA GGAGCAGATGACACAGTATTAGAAGAAATGGATTTGCCAGGAAGATGGAAACCAAAAATGATAGGGGGAA TTGGAGGTTTTATCAAAGTAAAACAGTATGATCAGATACCCATAGAAATTTGTGGACATAAAGTGATAGG TACAGTATTAGTAGGACCTACACCTGTCAACATAATTGGAAGAAATCTGTTGACTCAGCTTGGTTGCACT TTAAATTTTCCCATTAGTCCTATTGAAACTGTACCAGTAAAATTGAAGCCAGGAATGGATGGCCCAAAGG TTAAACAATGGCCATTGACAGAAGAAAAAATAAAAGCATTAATAGAAATCTGTGCAGAAATGGAAAAGGA AGGGAAAATTTCAAAAATTGGGCCTGAAAATCCATACAATACTCCAGTATTTGCCATAAGAAAAAAAGAC AGTACTAAATGGAGAAAATTAGTAGATTTCAAAGAACTTAATAAGAGAACTCAGGACTTCTGGGAAGTCC AATTAGGAATACCACATCCCTCAGGGTTAAAAAAGAAAAAGTCAGTAACAGTACTGGATGTGGGTGATGC ATATTTTTCAGTTCCCTTAGATGAAGACTTCAGAAAGTACACTGCATTTACCATACCTAGTGTAAACAAT GAGACACCAGGGATTAGGTATCAGTACAATGTGCTTCCACAAGGATGGAAAGGATCACCAGCAATATTCC AATGTAGCATGACAAAAATCTTAGAACCTTTTAGAAAACAAAATCCAGATATAGTTATCTATCAATACAT GGATGATCTGTATGTAGGATCTGACCTAGAAATAGGGCAGCATAGAACAAAAATAGAGGAACTGAGAGAA CATCTGTTGAGGTGGGGGTTTTGCACACCAGACAAGAAACATCAGAAGGAACCTCCATTCCTTTGGATGG GTTATGAACTCCATCCTT reconstructed_2 Freq=0.515259 ACCAAATGAAAGATTGTACTGAGAGACAGGCTAATTTTTTAGGGAAAATCTGGCCTTCCCACAAGGGAAG GCCAGGGAATTTTCTTCAGAGCAGACCAGAGCCAACAGCCCCACCAGAAGAGAGCTTCAGGTTTGGGGAA GAGACAACAACTCCCTCTCAGAAGCAGGAGCCGATAGACAAGGAAATGTATCCTTTAGCTTCCCTCAAAT CACTCTTTGGCAACGACCCCTCGTCACAATAAAGATAGGGGGGCAACTAAAGGAAGCTCTATTAGATACA GGAGCAGATGATACAGTATTAGAAGAAATGAATTTGCCAGGAAGATGGAAACCAAAAATGATAGGGGGAA TTGGAGGTTTTATCAAAGTAAAACAGTATGATCAGATACCCATAGAAATCTGTGGACATAAAGCTATAGG TACAGTATTAGTAGGACCTACACCTGTCAACATAATTGGAAGAAATCTGTTGACTCAGATTGGTTGCACT TTAAATTTTCCCATTAGTCCTATTGAAACTGTACCAGTAAAATTAAAGCCAGGAATGGATGGCCCAAAAG TTAAACAATGGCCATTGACAGAAGAAAAAATAAAAGCATTAGTAGAAATTTGTACAGAAATGGAAAAGGA AGGGAAAATTTCAAAAATTGGGCCTGAAAATCCATACAATACTCCAGTATTTGCCATAAGGAAAAAAGAC AGTACTAAATGGAGAAAATTAGTAGATTTCAGAGAACTTAATAAGAGAACTCAAGACTTCCGGGAAGTCC AATTAGGAATACCACATCCCTCAGGGTTAAAAAAGAAAAAATCAGTAACAGTACTGGATGTGGGTGATGC ATATTTTTCAGTTCCCTTAGATGAAGACTTCAGAAAGTATACTGCATTTACCATACCTAGTGTAAACAAT GAGACACCAGGGATTAGGTATCAGTACAATGTGCTTCCACAAGGATGGAAAGGATCACCAGCAATATTCC AAGCTAGCATGACAAAAATCTTAGAGCCTTTTAGAAAACAAAATCCAGACATAGTTATCTATCAATACAT GGATGATCTGTATGTAGGATCTGACCTAGAAATAGGGCAGCATAGAACAAAAATAGAGGAACTGAGAGAA CATCTGTTGAGGTGGGGGTTTTGCACACCAGACAAGAAACATCAGAAGGAACCTCCATTCCTTTGGATGG GTTATGAACTCCATCCCC","title":"ph_parser"},{"location":"hp_reads/","text":"hp_reads involves cleaning up the raw read sequences, as well as other processing steps. Modules to manipulate reads. Use -h after any command for a list of options. sample_reads Subsample reads using seqtk ( documentation ). Input is reads in FASTQ format. Output is sampled reads in FASTQ format. You do not have to have all read options (i.e., read1, read2 AND unpaired reads). You can have a combination of any of those. Usage: haphpipe sample_reads [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ [--outdir] (or): hp_sample_reads [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ [--outdir] Output files: sample_1.fastq sample_2.fastq Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --outdir Output directory (default: current directory). Options: Option Description --nreads Number of reads to sample. If greater than the number of reads in file, all reads will be sampled. --frac Fraction of reads to sample, between 0 and 1. Each read has [frac] --seed Seed for random number generator. Settings: Option Description --quiet Do not write output to console (silence stdout and stderr), default is False. --logfile Append console output to this file. --debug Print commands but do not run, default is False. Example usage: This pulls 1000 reads from these paired end files with a starting seed of 1234. haphpipe sample_reads --fq1 read_1.fastq --fq2 read_2.fastq --nreads 1000 --seed 1234 -- trim_reads Trim reads using Trimmomatic ( documentation ). Input is reads in FASTQ format. Output is trimmed reads in FASTQ format. Usage: haphpipe trim_reads [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ [--outdir] (or): hp_trim_reads [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ [--outdir] Output files: trimmed_1.fastq trimmed_2.fastq trimmed_U.fastq trimmomatic_summary.out Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --outdir Output directory (default: current directory). Options: Option Description --adapter_file Adapter file. --trimmers Trim commands for trimmomatic (default: ['LEADING:3', 'TRAILING:3', 'SLIDINGWINDOW:4:15', 'MINLEN:36']). --encoding Quality score encoding. Settings: Option Description --ncpu Number of CPU to use (default: 1). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: This trims paired end read files 1 and 2. haphpipe trim_reads --fq1 read_1.fastq --fq2 read_2.fastq -- join_reads Join reads using FLASH ( paper ). Input is reads in FASTQ format. Output is joined reads in FASTQ format. Usage: haphpipe join_reads [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ [--outdir] (or): hp_join_reads [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ [--outdir] Output files: joined.fastq notjoined_1.fastq notjoined_2.fastq Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --outdir Output directory (default: current directory). Settings: Option Description --min_overlap The minimum required overlap length between two reads to provide a confident overlap (default: 10). --max_overlap Maximum overlap length expected in approximately 90% of read pairs, longer overlaps are penalized. --allow_outies Also try combining read pairs in the \"outie\" orientation (default: False). --encoding Quality score encoding. Settings: Option Description --ncpu Number of CPU to use (default: 1). --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe join_reads --fq1 trimmed_1.fastq --fq2 trimmed_2.fastq ec_reads Error correction using SPAdes ( documentation ). Input is reads in FASTQ format. Output is error-corrected reads in FASTQ format. Remember that HAPHPIPE is intended for Illumina reads, therefore the error correction is based on Illumina sequencing errors. Usage: haphpipe ec_reads [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ [--outdir] (or): hp_ec_reads [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ [--outdir] Output files: corrected_1.fastq corrected_2.fastq corrected_U.fastq Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --outdir Output directory (default: current directory). Settings: Option Description --ncpu Number of CPU to use (default: 1). --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run, default is False. Example usage: haphpipe ec_reads --fq1 trimmed_1.fastq --fq2 trimmed_2.fastq","title":"Reads"},{"location":"hp_reads/#sample_reads","text":"Subsample reads using seqtk ( documentation ). Input is reads in FASTQ format. Output is sampled reads in FASTQ format. You do not have to have all read options (i.e., read1, read2 AND unpaired reads). You can have a combination of any of those. Usage: haphpipe sample_reads [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ [--outdir] (or): hp_sample_reads [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ [--outdir] Output files: sample_1.fastq sample_2.fastq Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --outdir Output directory (default: current directory). Options: Option Description --nreads Number of reads to sample. If greater than the number of reads in file, all reads will be sampled. --frac Fraction of reads to sample, between 0 and 1. Each read has [frac] --seed Seed for random number generator. Settings: Option Description --quiet Do not write output to console (silence stdout and stderr), default is False. --logfile Append console output to this file. --debug Print commands but do not run, default is False. Example usage: This pulls 1000 reads from these paired end files with a starting seed of 1234. haphpipe sample_reads --fq1 read_1.fastq --fq2 read_2.fastq --nreads 1000 --seed 1234 --","title":"sample_reads"},{"location":"hp_reads/#trim_reads","text":"Trim reads using Trimmomatic ( documentation ). Input is reads in FASTQ format. Output is trimmed reads in FASTQ format. Usage: haphpipe trim_reads [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ [--outdir] (or): hp_trim_reads [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ [--outdir] Output files: trimmed_1.fastq trimmed_2.fastq trimmed_U.fastq trimmomatic_summary.out Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --outdir Output directory (default: current directory). Options: Option Description --adapter_file Adapter file. --trimmers Trim commands for trimmomatic (default: ['LEADING:3', 'TRAILING:3', 'SLIDINGWINDOW:4:15', 'MINLEN:36']). --encoding Quality score encoding. Settings: Option Description --ncpu Number of CPU to use (default: 1). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: This trims paired end read files 1 and 2. haphpipe trim_reads --fq1 read_1.fastq --fq2 read_2.fastq --","title":"trim_reads"},{"location":"hp_reads/#join_reads","text":"Join reads using FLASH ( paper ). Input is reads in FASTQ format. Output is joined reads in FASTQ format. Usage: haphpipe join_reads [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ [--outdir] (or): hp_join_reads [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ [--outdir] Output files: joined.fastq notjoined_1.fastq notjoined_2.fastq Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --outdir Output directory (default: current directory). Settings: Option Description --min_overlap The minimum required overlap length between two reads to provide a confident overlap (default: 10). --max_overlap Maximum overlap length expected in approximately 90% of read pairs, longer overlaps are penalized. --allow_outies Also try combining read pairs in the \"outie\" orientation (default: False). --encoding Quality score encoding. Settings: Option Description --ncpu Number of CPU to use (default: 1). --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe join_reads --fq1 trimmed_1.fastq --fq2 trimmed_2.fastq","title":"join_reads"},{"location":"hp_reads/#ec_reads","text":"Error correction using SPAdes ( documentation ). Input is reads in FASTQ format. Output is error-corrected reads in FASTQ format. Remember that HAPHPIPE is intended for Illumina reads, therefore the error correction is based on Illumina sequencing errors. Usage: haphpipe ec_reads [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ [--outdir] (or): hp_ec_reads [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ [--outdir] Output files: corrected_1.fastq corrected_2.fastq corrected_U.fastq Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --outdir Output directory (default: current directory). Settings: Option Description --ncpu Number of CPU to use (default: 1). --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run, default is False. Example usage: haphpipe ec_reads --fq1 trimmed_1.fastq --fq2 trimmed_2.fastq","title":"ec_reads"},{"location":"inout/","text":"Module Input Input Format Output Output File Names sample_reads FASTQ file(s) .fastq.gz or .fastq or .fq Subampled FASTQ file(s) sample_1.fastq sample_2.fastq trim_reads FASTQ file(s) .fastq.gz or .fastq or .fq Trimmed FASTQ file(s) and summary from Trimmomatic text file trimmed_1.fastq trimmed_2.fastq trimmed_U.fastq trimmomatic_summary.out join_reads FASTQ file(s) .fastq.gz or .fastq or .fq FASTQ files for joined reads and unjoined reads joined.fastq notjoined_1.fastq notjoined_2.fastq ec_reads FASTQ file(s) .fastq.gz or .fastq or .fq Error corrected FASTQ file(s) corrected_1.fastq corrected_2.fastq corrected_U.fastq assemble_denovo FASTQ file(s) .fastq.gz or .fastq or .fq De novo assembled contigs FASTA file and de novo summary text file denovo_contigs.fna denovo_summary.txt assemble_amplicons FASTA files and GTF file .fna or .fasta or .fa and .gtf FASTA file with assembled amplicons amplicon_assembly.fna assemble_scaffold FASTA files .fna or .fasta or .fa FASTA files with scaffolded and aligned sequences and FASTA file with assembled amplicons scaffold_aligned.fa scaffold_assembly.fa scaffold_imputed.fa scaffold_padded.out align_reads FASTQ files and FASTA file .fastq.gz or .fastq or .fq and .fna or .fasta or .fa Aligned BAM file and Bowtie2 alignment output summary text file aligned.bam aligned.bt2.out call_variants BAM and FASTA file .bam and .fna or .fasta or .fa VCF file with variants variants.vcf.gz vcf_to_consensus VCF file .vcf FASTA file with consensus sequence consensus.fna refine_assembly FASTQ files and FASTA file .fastq.gz or .fastq or .fq and .fna or .fasta or .fa FASTA file with refined consensus sequence refined.fna finalize_assembly FASTQ files and FASTA file .fastq.gz or .fastq or .fq and .fna or .fasta or .fa FASTA file with final refined consensus sequence and final BAM file with reads aligned to final FASTA file and VCF file with variants relative to final.fna final.fna and final.bam and final.vcf.gz predict_haplo FASTQ files and FASTA file .fastq.gz or .fastq or .fq and .fna or .fasta or .fa PredictHaplo's fasta-like output file best.fa ph_parser PredictHaplo's FAS output file .fas Summary text file with haplotype diversity statistics and FASTA file with haplotype sequences ph_summary.txt and ph_haplotypes.fna pairwise_align FASTA files and GTF file .fna or .fasta or .fa and .gtf JSON file pairwise_aligned.json extract_pairwise JSON file from pairwise_align output .json FASTA output with region extracted to standard out stdout.fasta annotate_from_ref JSON file from pairwise_align output and GTF file .json and .gtf (add) (add) summary_stats Direcoty lists for assembly and haplotype files (TXT) .txt Summary statistics (TXT and TSV) summary_stats.txt summary_stats.tsv PH_summary_stats.tsv multiple_align FASTA files and/or directory list (TXT) and GTF file .fna or .fasta or .fa and .txt and .gtf FASTA alignment and sequence files separated by amplicons in GTF alignment_regionX.fasta all_sequences_regionX.fasta model_test FASTA or PHYLIP alignment file .fna or .fasta or .fa or .phy Text file containing best-fit models modeltest_results.out build_tree FASTA or PHYLIP alignment file .fna or .fasta or .fa RAxML output files RaxML_info.build_tree.tre and RAxML tree files","title":"File Input/Output"},{"location":"install/","text":"HAPHPIPE Installation Instructions HAPHIPE depends on more than a dozen different programs, each of which may itself depend on other programs and libraries. Installing everything separately would be a nightmare, so you should use the package manager \"Bioconda\" to install everything. Bioconda is a popular package manager in the bioinformatics world. See the Helpful Resources section for more information and resources for Bioconda. Here, we will describe where to get Bioconda and how to install it, then how to use Bioconda to install the necessary programs for HAPHPIPE. We will also detail the acquisition and installation of one program, GATK, that is not handled by Bioconda, and finally the acquisition and installation of HAPHPIPE itself. Here, we describe the procedure for installing HAPHPIPE using the package manager Bioconda (Gr\u00fcning et al. 2018) on the command line. If you are unfamiliar with Bioconda, please see for installation and channel setup. HAPHPIPE is available here and is written in Python 3.7.2 coding language. The installation process begins with the creation of a conda environment that installs the necessary dependencies required by HAPHPIPE. Once the conda environment has been created, it can be activated for use with the command conda activate haphpipe. Due to license restrictions, Bioconda cannot distribute and install GATK (McKenna et al. 2010; Van der Auwera et al. 2013; Poplin et al. 2018) directly. To fully install GATK, you must download a licensed copy of GATK (version 3.8-0) from the Broad Institute (link) . You can then install HAPHPIPE using the single command pip install git+git://github.com/gwcbi/haphpipe.git , which pulls the repository from Github. 1. Install conda Download the conda package: wget https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-x86_64.sh sh Miniconda3-latest-MacOSX-x86_64.sh 2. Set up conda channels These need to be put in as a command in the order they come, as this sets the priority for where packages are pulled from. Therefore, in this order, conda-forge is top priority and explored first when downloading a program. conda config --add channels R conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge 3. Create a conda environment with the following dependencies conda create -n haphpipe \\ python=3.7 \\ future \\ pyyaml \\ biopython \\ seqtk \\ bowtie2 \\ bwa \\ flash \\ freebayes \\ mummer \\ picard \\ trimmomatic \\ samtools=1.9 \\ gatk=3.8 \\ spades \\ blast \\ sierrapy Note: on some HPC systems (including GW's Pegasus), certain dependencies must be installed as a 'User Install'. If this is the case (which will be apparent if you receive availability errors for any of the above packages after installing them through conda) use the following command to install the package: pip install --user package name 4. Activate the environment conda activate haphpipe 5. Install GATK Due to license restrictions, bioconda cannot distribute and install GATK directly. To fully install GATK, you must download a licensed copy of GATK (version 3.8-0) from the Broad Institute: https://software.broadinstitute.org/gatk/download/archive . Register the package using gatk3-register: gatk3-register /path/to/GenomeAnalysisTK-3.8-0-ge9d806836.tar.bz2 This will copy GATK into your conda environment. Alternatively, GATK may be installed directly on the command line with the following commands: mkdir -p /path/to/gatk_dir wget -O - 'https://software.broadinstitute.org/gatk/download/auth?package=GATK-archive version=3.6-0-g89b7209' | tar -xjvC /path/to/gatk_dir gatk3-register /path/to/gatk_dir/GenomeAnalysisTK.jar NOTE: HAPHPIPE was developed and tested using GATK 3.8. 6. Install HAPHPIPE pip install git+git://github.com/gwcbi/haphpipe.git Upon completion of the installation, you can test it to ensure the repository has been installed completely and correctly by running haphpipe -h (in quick start). Once HAPHPIPE is installed and performing correctly, there is no need to install it again; simply activate the conda environment when needed by executing conda activate haphpipe . If a new version is released in the future, HAPHPIPE can be updated by running the command line pip install --upgrade git+git://github.com/gwcbi/haphpipe.git . At any point, the -h option that follows any HAPHPIPE stage will output a help message that provides a description of the stage and the desired input(s) and output(s). PredictHaplo Installation Instructions Users are required to download PredictHaplo on their own computing system prior to running any of the haplotype stages ( hp_predict_haplo and hp_ph_parser ). Here is how the GW CBI team installed PredictHaplo onto our HPC, which has a slurm scheduling system and uses Lua module files. We cannot help with the installation of this software, but have provided the code that we used here to install PredictHaplo onto our system. Please see their website for contact information if needed. This module loads predicthaplo onto GWU's HPC - Colonial One. See https://bmda.dmi.unibas.ch/software.html cd /path/to/modules/predicthaplo # use gcc 4.9.4, add blas and lapack to library path module load gcc/4.9.4 module load blas/gcc/64 module load lapack/gcc/64 # download source cd archive wget https://bmda.dmi.unibas.ch/software/PredictHaplo-Paired-0.4.tgz cd .. # unzip source and change directory name tar xfvz archive/PredictHaplo-Paired-0.4.tgz mv PredictHaplo-Paired-0.4 0.4 cd 0.4 # install scythestat tar xfvz scythestat-1.0.3.tar.gz cd scythestat-1.0.3 ./configure --prefix=/path/to/modules/predicthaplo/0.4/NEWSCYTHE make install cd .. # compile predicthaplo make If a segfault error occurs during the hp_predict_haplo stage, this is not a characteristic of HAPHPIPE but rather that of PredictHaplo. Sometimes, we have luck if we just rerun the code again or move to an interactive CPU node. We are unsure what causes this error, and we only see it between the local and global reconstruction phases in PredictHaplo. Quick-Start 1. Activate haphpipe Make sure you have conda running. For students at GW using Colonial One, you need to load the miniconda3 module like such prior to activating the haphpipe conda environemnt: module load miniconda3 . conda activate haphpipe 2. Test that it is loaded correctly haphpipe -h should produce: Program: haphpipe (haplotype and phylodynamics pipeline) Version: 0.8.1 Commands: -- Reads sample_reads subsample reads using seqtk trim_reads trim reads using Trimmomatic join_reads join reads using FLASh ec_reads error correct reads using SPAdes -- Assemble assemble_denovo assemble reads denovo assemble_amplicons assemble contigs to amplicon regions assemble_scaffold assemble contigs to genome align_reads align reads to reference call_variants call variants vcf_to_consensus create consensus sequence from VCF refine_assembly iterative refinement: align - variants - consensus finalize_assembly finalize consensus sequence -- Haplotype predict_haplo assemble haplotypes with PredictHaplo ph_parser parse output from PredictHaplo. -- Annotate pairwise_align align consensus to an annotated reference extract_pairwise extract sequence regions from pairwise alignment annotate_from_ref annotate consensus from reference annotation -- Miscellaneous demo setup demo directory and test data Directory Structure Below is the recommended directory structure for using HAPHPIPE: We recommend creating a separate directory for each sample, as well as a directory for reference files. Windows Users HAPHPIPE is only available for Mac OSX or Linux platforms. We suggest the following options for running HAPHPIPE on a Windows machine: Run HAPHPIPE on your institution's HPC cluster, if available. Utilize the Windows Subsystem for Linux . Run Linux in a virtual machine via VirtualBox Reference Files Several modules in HAPHPIPE use reference files: either a FASTA file containing a reference sequence or a GTF file denoting genome regions for amplicon assembly. With HIV data, we use the file HIV_B.K03455.HXB2.fasta as a reference whole-genome file, HIV_B.K03455.HXB2.amplicons.fasta for amplicon assembly, and HIV_B.K03455.HXB2.gtf as a GTF file. All three files are downloaded in the demo module . The contents of each file are below. `HIV_B.K03455.HXB2.fasta` HIV_B.K03455.HXB2 TGGAAGGGCTAATTCACTCCCAACGAAGACAAGATATCCTTGATCTGTGGATCTACCACACACAAGGCTACTTCCCTGATTAGCAGAACTACACACCAGG GCCAGGGATCAGATATCCACTGACCTTTGGATGGTGCTACAAGCTAGTACCAGTTGAGCCAGAGAAGTTAGAAGAAGCCAACAAAGGAGAGAACACCAGC TTGTTACACCCTGTGAGCCTGCATGGAATGGATGACCCGGAGAGAGAAGTGTTAGAGTGGAGGTTTGACAGCCGCCTAGCATTTCATCACATGGCCCGAG AGCTGCATCCGGAGTACTTCAAGAACTGCTGACATCGAGCTTGCTACAAGGGACTTTCCGCTGGGGACTTTCCAGGGAGGCGTGGCCTGGGCGGGACTGG GGAGTGGCGAGCCCTCAGATCCTGCATATAAGCAGCTGCTTTTTGCCTGTACTGGGTCTCTCTGGTTAGACCAGATCTGAGCCTGGGAGCTCTCTGGCTA ACTAGGGAACCCACTGCTTAAGCCTCAATAAAGCTTGCCTTGAGTGCTTCAAGTAGTGTGTGCCCGTCTGTTGTGTGACTCTGGTAACTAGAGATCCCTC AGACCCTTTTAGTCAGTGTGGAAAATCTCTAGCAGTGGCGCCCGAACAGGGACCTGAAAGCGAAAGGGAAACCAGAGGAGCTCTCTCGACGCAGGACTCG GCTTGCTGAAGCGCGCACGGCAAGAGGCGAGGGGCGGCGACTGGTGAGTACGCCAAAAATTTTGACTAGCGGAGGCTAGAAGGAGAGAGATGGGTGCGAG AGCGTCAGTATTAAGCGGGGGAGAATTAGATCGATGGGAAAAAATTCGGTTAAGGCCAGGGGGAAAGAAAAAATATAAATTAAAACATATAGTATGGGCA AGCAGGGAGCTAGAACGATTCGCAGTTAATCCTGGCCTGTTAGAAACATCAGAAGGCTGTAGACAAATACTGGGACAGCTACAACCATCCCTTCAGACAG GATCAGAAGAACTTAGATCATTATATAATACAGTAGCAACCCTCTATTGTGTGCATCAAAGGATAGAGATAAAAGACACCAAGGAAGCTTTAGACAAGAT AGAGGAAGAGCAAAACAAAAGTAAGAAAAAAGCACAGCAAGCAGCAGCTGACACAGGACACAGCAATCAGGTCAGCCAAAATTACCCTATAGTGCAGAAC ATCCAGGGGCAAATGGTACATCAGGCCATATCACCTAGAACTTTAAATGCATGGGTAAAAGTAGTAGAAGAGAAGGCTTTCAGCCCAGAAGTGATACCCA TGTTTTCAGCATTATCAGAAGGAGCCACCCCACAAGATTTAAACACCATGCTAAACACAGTGGGGGGACATCAAGCAGCCATGCAAATGTTAAAAGAGAC CATCAATGAGGAAGCTGCAGAATGGGATAGAGTGCATCCAGTGCATGCAGGGCCTATTGCACCAGGCCAGATGAGAGAACCAAGGGGAAGTGACATAGCA GGAACTACTAGTACCCTTCAGGAACAAATAGGATGGATGACAAATAATCCACCTATCCCAGTAGGAGAAATTTATAAAAGATGGATAATCCTGGGATTAA ATAAAATAGTAAGAATGTATAGCCCTACCAGCATTCTGGACATAAGACAAGGACCAAAGGAACCCTTTAGAGACTATGTAGACCGGTTCTATAAAACTCT AAGAGCCGAGCAAGCTTCACAGGAGGTAAAAAATTGGATGACAGAAACCTTGTTGGTCCAAAATGCGAACCCAGATTGTAAGACTATTTTAAAAGCATTG GGACCAGCGGCTACACTAGAAGAAATGATGACAGCATGTCAGGGAGTAGGAGGACCCGGCCATAAGGCAAGAGTTTTGGCTGAAGCAATGAGCCAAGTAA CAAATTCAGCTACCATAATGATGCAGAGAGGCAATTTTAGGAACCAAAGAAAGATTGTTAAGTGTTTCAATTGTGGCAAAGAAGGGCACACAGCCAGAAA TTGCAGGGCCCCTAGGAAAAAGGGCTGTTGGAAATGTGGAAAGGAAGGACACCAAATGAAAGATTGTACTGAGAGACAGGCTAATTTTTTAGGGAAGATC TGGCCTTCCTACAAGGGAAGGCCAGGGAATTTTCTTCAGAGCAGACCAGAGCCAACAGCCCCACCAGAAGAGAGCTTCAGGTCTGGGGTAGAGACAACAA CTCCCCCTCAGAAGCAGGAGCCGATAGACAAGGAACTGTATCCTTTAACTTCCCTCAGGTCACTCTTTGGCAACGACCCCTCGTCACAATAAAGATAGGG GGGCAACTAAAGGAAGCTCTATTAGATACAGGAGCAGATGATACAGTATTAGAAGAAATGAGTTTGCCAGGAAGATGGAAACCAAAAATGATAGGGGGAA TTGGAGGTTTTATCAAAGTAAGACAGTATGATCAGATACTCATAGAAATCTGTGGACATAAAGCTATAGGTACAGTATTAGTAGGACCTACACCTGTCAA CATAATTGGAAGAAATCTGTTGACTCAGATTGGTTGCACTTTAAATTTTCCCATTAGCCCTATTGAGACTGTACCAGTAAAATTAAAGCCAGGAATGGAT GGCCCAAAAGTTAAACAATGGCCATTGACAGAAGAAAAAATAAAAGCATTAGTAGAAATTTGTACAGAGATGGAAAAGGAAGGGAAAATTTCAAAAATTG GGCCTGAAAATCCATACAATACTCCAGTATTTGCCATAAAGAAAAAAGACAGTACTAAATGGAGAAAATTAGTAGATTTCAGAGAACTTAATAAGAGAAC TCAAGACTTCTGGGAAGTTCAATTAGGAATACCACATCCCGCAGGGTTAAAAAAGAAAAAATCAGTAACAGTACTGGATGTGGGTGATGCATATTTTTCA GTTCCCTTAGATGAAGACTTCAGGAAGTATACTGCATTTACCATACCTAGTATAAACAATGAGACACCAGGGATTAGATATCAGTACAATGTGCTTCCAC AGGGATGGAAAGGATCACCAGCAATATTCCAAAGTAGCATGACAAAAATCTTAGAGCCTTTTAGAAAACAAAATCCAGACATAGTTATCTATCAATACAT GGATGATTTGTATGTAGGATCTGACTTAGAAATAGGGCAGCATAGAACAAAAATAGAGGAGCTGAGACAACATCTGTTGAGGTGGGGACTTACCACACCA GACAAAAAACATCAGAAAGAACCTCCATTCCTTTGGATGGGTTATGAACTCCATCCTGATAAATGGACAGTACAGCCTATAGTGCTGCCAGAAAAAGACA GCTGGACTGTCAATGACATACAGAAGTTAGTGGGGAAATTGAATTGGGCAAGTCAGATTTACCCAGGGATTAAAGTAAGGCAATTATGTAAACTCCTTAG AGGAACCAAAGCACTAACAGAAGTAATACCACTAACAGAAGAAGCAGAGCTAGAACTGGCAGAAAACAGAGAGATTCTAAAAGAACCAGTACATGGAGTG TATTATGACCCATCAAAAGACTTAATAGCAGAAATACAGAAGCAGGGGCAAGGCCAATGGACATATCAAATTTATCAAGAGCCATTTAAAAATCTGAAAA CAGGAAAATATGCAAGAATGAGGGGTGCCCACACTAATGATGTAAAACAATTAACAGAGGCAGTGCAAAAAATAACCACAGAAAGCATAGTAATATGGGG AAAGACTCCTAAATTTAAACTGCCCATACAAAAGGAAACATGGGAAACATGGTGGACAGAGTATTGGCAAGCCACCTGGATTCCTGAGTGGGAGTTTGTT AATACCCCTCCCTTAGTGAAATTATGGTACCAGTTAGAGAAAGAACCCATAGTAGGAGCAGAAACCTTCTATGTAGATGGGGCAGCTAACAGGGAGACTA AATTAGGAAAAGCAGGATATGTTACTAATAGAGGAAGACAAAAAGTTGTCACCCTAACTGACACAACAAATCAGAAGACTGAGTTACAAGCAATTTATCT AGCTTTGCAGGATTCGGGATTAGAAGTAAACATAGTAACAGACTCACAATATGCATTAGGAATCATTCAAGCACAACCAGATCAAAGTGAATCAGAGTTA GTCAATCAAATAATAGAGCAGTTAATAAAAAAGGAAAAGGTCTATCTGGCATGGGTACCAGCACACAAAGGAATTGGAGGAAATGAACAAGTAGATAAAT TAGTCAGTGCTGGAATCAGGAAAGTACTATTTTTAGATGGAATAGATAAGGCCCAAGATGAACATGAGAAATATCACAGTAATTGGAGAGCAATGGCTAG TGATTTTAACCTGCCACCTGTAGTAGCAAAAGAAATAGTAGCCAGCTGTGATAAATGTCAGCTAAAAGGAGAAGCCATGCATGGACAAGTAGACTGTAGT CCAGGAATATGGCAACTAGATTGTACACATTTAGAAGGAAAAGTTATCCTGGTAGCAGTTCATGTAGCCAGTGGATATATAGAAGCAGAAGTTATTCCAG CAGAAACAGGGCAGGAAACAGCATATTTTCTTTTAAAATTAGCAGGAAGATGGCCAGTAAAAACAATACATACTGACAATGGCAGCAATTTCACCGGTGC TACGGTTAGGGCCGCCTGTTGGTGGGCGGGAATCAAGCAGGAATTTGGAATTCCCTACAATCCCCAAAGTCAAGGAGTAGTAGAATCTATGAATAAAGAA TTAAAGAAAATTATAGGACAGGTAAGAGATCAGGCTGAACATCTTAAGACAGCAGTACAAATGGCAGTATTCATCCACAATTTTAAAAGAAAAGGGGGGA TTGGGGGGTACAGTGCAGGGGAAAGAATAGTAGACATAATAGCAACAGACATACAAACTAAAGAATTACAAAAACAAATTACAAAAATTCAAAATTTTCG GGTTTATTACAGGGACAGCAGAAATCCACTTTGGAAAGGACCAGCAAAGCTCCTCTGGAAAGGTGAAGGGGCAGTAGTAATACAAGATAATAGTGACATA AAAGTAGTGCCAAGAAGAAAAGCAAAGATCATTAGGGATTATGGAAAACAGATGGCAGGTGATGATTGTGTGGCAAGTAGACAGGATGAGGATTAGAACA TGGAAAAGTTTAGTAAAACACCATATGTATGTTTCAGGGAAAGCTAGGGGATGGTTTTATAGACATCACTATGAAAGCCCTCATCCAAGAATAAGTTCAG AAGTACACATCCCACTAGGGGATGCTAGATTGGTAATAACAACATATTGGGGTCTGCATACAGGAGAAAGAGACTGGCATTTGGGTCAGGGAGTCTCCAT AGAATGGAGGAAAAAGAGATATAGCACACAAGTAGACCCTGAACTAGCAGACCAACTAATTCATCTGTATTACTTTGACTGTTTTTCAGACTCTGCTATA AGAAAGGCCTTATTAGGACACATAGTTAGCCCTAGGTGTGAATATCAAGCAGGACATAACAAGGTAGGATCTCTACAATACTTGGCACTAGCAGCATTAA TAACACCAAAAAAGATAAAGCCACCTTTGCCTAGTGTTACGAAACTGACAGAGGATAGATGGAACAAGCCCCAGAAGACCAAGGGCCACAGAGGGAGCCA CACAATGAATGGACACTAGAGCTTTTAGAGGAGCTTAAGAATGAAGCTGTTAGACATTTTCCTAGGATTTGGCTCCATGGCTTAGGGCAACATATCTATG AAACTTATGGGGATACTTGGGCAGGAGTGGAAGCCATAATAAGAATTCTGCAACAACTGCTGTTTATCCATTTTCAGAATTGGGTGTCGACATAGCAGAA TAGGCGTTACTCGACAGAGGAGAGCAAGAAATGGAGCCAGTAGATCCTAGACTAGAGCCCTGGAAGCATCCAGGAAGTCAGCCTAAAACTGCTTGTACCA ATTGCTATTGTAAAAAGTGTTGCTTTCATTGCCAAGTTTGTTTCATAACAAAAGCCTTAGGCATCTCCTATGGCAGGAAGAAGCGGAGACAGCGACGAAG AGCTCATCAGAACAGTCAGACTCATCAAGCTTCTCTATCAAAGCAGTAAGTAGTACATGTAACGCAACCTATACCAATAGTAGCAATAGTAGCATTAGTA GTAGCAATAATAATAGCAATAGTTGTGTGGTCCATAGTAATCATAGAATATAGGAAAATATTAAGACAAAGAAAAATAGACAGGTTAATTGATAGACTAA TAGAAAGAGCAGAAGACAGTGGCAATGAGAGTGAAGGAGAAATATCAGCACTTGTGGAGATGGGGGTGGAGATGGGGCACCATGCTCCTTGGGATGTTGA TGATCTGTAGTGCTACAGAAAAATTGTGGGTCACAGTCTATTATGGGGTACCTGTGTGGAAGGAAGCAACCACCACTCTATTTTGTGCATCAGATGCTAA AGCATATGATACAGAGGTACATAATGTTTGGGCCACACATGCCTGTGTACCCACAGACCCCAACCCACAAGAAGTAGTATTGGTAAATGTGACAGAAAAT TTTAACATGTGGAAAAATGACATGGTAGAACAGATGCATGAGGATATAATCAGTTTATGGGATCAAAGCCTAAAGCCATGTGTAAAATTAACCCCACTCT GTGTTAGTTTAAAGTGCACTGATTTGAAGAATGATACTAATACCAATAGTAGTAGCGGGAGAATGATAATGGAGAAAGGAGAGATAAAAAACTGCTCTTT CAATATCAGCACAAGCATAAGAGGTAAGGTGCAGAAAGAATATGCATTTTTTTATAAACTTGATATAATACCAATAGATAATGATACTACCAGCTATAAG TTGACAAGTTGTAACACCTCAGTCATTACACAGGCCTGTCCAAAGGTATCCTTTGAGCCAATTCCCATACATTATTGTGCCCCGGCTGGTTTTGCGATTC TAAAATGTAATAATAAGACGTTCAATGGAACAGGACCATGTACAAATGTCAGCACAGTACAATGTACACATGGAATTAGGCCAGTAGTATCAACTCAACT GCTGTTAAATGGCAGTCTAGCAGAAGAAGAGGTAGTAATTAGATCTGTCAATTTCACGGACAATGCTAAAACCATAATAGTACAGCTGAACACATCTGTA GAAATTAATTGTACAAGACCCAACAACAATACAAGAAAAAGAATCCGTATCCAGAGAGGACCAGGGAGAGCATTTGTTACAATAGGAAAAATAGGAAATA TGAGACAAGCACATTGTAACATTAGTAGAGCAAAATGGAATAACACTTTAAAACAGATAGCTAGCAAATTAAGAGAACAATTTGGAAATAATAAAACAAT AATCTTTAAGCAATCCTCAGGAGGGGACCCAGAAATTGTAACGCACAGTTTTAATTGTGGAGGGGAATTTTTCTACTGTAATTCAACACAACTGTTTAAT AGTACTTGGTTTAATAGTACTTGGAGTACTGAAGGGTCAAATAACACTGAAGGAAGTGACACAATCACCCTCCCATGCAGAATAAAACAAATTATAAACA TGTGGCAGAAAGTAGGAAAAGCAATGTATGCCCCTCCCATCAGTGGACAAATTAGATGTTCATCAAATATTACAGGGCTGCTATTAACAAGAGATGGTGG TAATAGCAACAATGAGTCCGAGATCTTCAGACCTGGAGGAGGAGATATGAGGGACAATTGGAGAAGTGAATTATATAAATATAAAGTAGTAAAAATTGAA CCATTAGGAGTAGCACCCACCAAGGCAAAGAGAAGAGTGGTGCAGAGAGAAAAAAGAGCAGTGGGAATAGGAGCTTTGTTCCTTGGGTTCTTGGGAGCAG CAGGAAGCACTATGGGCGCAGCCTCAATGACGCTGACGGTACAGGCCAGACAATTATTGTCTGGTATAGTGCAGCAGCAGAACAATTTGCTGAGGGCTAT TGAGGCGCAACAGCATCTGTTGCAACTCACAGTCTGGGGCATCAAGCAGCTCCAGGCAAGAATCCTGGCTGTGGAAAGATACCTAAAGGATCAACAGCTC CTGGGGATTTGGGGTTGCTCTGGAAAACTCATTTGCACCACTGCTGTGCCTTGGAATGCTAGTTGGAGTAATAAATCTCTGGAACAGATTTGGAATCACA CGACCTGGATGGAGTGGGACAGAGAAATTAACAATTACACAAGCTTAATACACTCCTTAATTGAAGAATCGCAAAACCAGCAAGAAAAGAATGAACAAGA ATTATTGGAATTAGATAAATGGGCAAGTTTGTGGAATTGGTTTAACATAACAAATTGGCTGTGGTATATAAAATTATTCATAATGATAGTAGGAGGCTTG GTAGGTTTAAGAATAGTTTTTGCTGTACTTTCTATAGTGAATAGAGTTAGGCAGGGATATTCACCATTATCGTTTCAGACCCACCTCCCAACCCCGAGGG GACCCGACAGGCCCGAAGGAATAGAAGAAGAAGGTGGAGAGAGAGACAGAGACAGATCCATTCGATTAGTGAACGGATCCTTGGCACTTATCTGGGACGA TCTGCGGAGCCTGTGCCTCTTCAGCTACCACCGCTTGAGAGACTTACTCTTGATTGTAACGAGGATTGTGGAACTTCTGGGACGCAGGGGGTGGGAAGCC CTCAAATATTGGTGGAATCTCCTACAGTATTGGAGTCAGGAACTAAAGAATAGTGCTGTTAGCTTGCTCAATGCCACAGCCATAGCAGTAGCTGAGGGGA CAGATAGGGTTATAGAAGTAGTACAAGGAGCTTGTAGAGCTATTCGCCACATACCTAGAAGAATAAGACAGGGCTTGGAAAGGATTTTGCTATAAGATGG GTGGCAAGTGGTCAAAAAGTAGTGTGATTGGATGGCCTACTGTAAGGGAAAGAATGAGACGAGCTGAGCCAGCAGCAGATAGGGTGGGAGCAGCATCTCG AGACCTGGAAAAACATGGAGCAATCACAAGTAGCAATACAGCAGCTACCAATGCTGCTTGTGCCTGGCTAGAAGCACAAGAGGAGGAGGAGGTGGGTTTT CCAGTCACACCTCAGGTACCTTTAAGACCAATGACTTACAAGGCAGCTGTAGATCTTAGCCACTTTTTAAAAGAAAAGGGGGGACTGGAAGGGCTAATTC ACTCCCAAAGAAGACAAGATATCCTTGATCTGTGGATCTACCACACACAAGGCTACTTCCCTGATTAGCAGAACTACACACCAGGGCCAGGGGTCAGATA TCCACTGACCTTTGGATGGTGCTACAAGCTAGTACCAGTTGAGCCAGATAAGATAGAAGAGGCCAATAAAGGAGAGAACACCAGCTTGTTACACCCTGTG AGCCTGCATGGGATGGATGACCCGGAGAGAGAAGTGTTAGAGTGGAGGTTTGACAGCCGCCTAGCATTTCATCACGTGGCCCGAGAGCTGCATCCGGAGT ACTTCAAGAACTGCTGACATCGAGCTTGCTACAAGGGACTTTCCGCTGGGGACTTTCCAGGGAGGCGTGGCCTGGGCGGGACTGGGGAGTGGCGAGCCCT CAGATCCTGCATATAAGCAGCTGCTTTTTGCCTGTACTGGGTCTCTCTGGTTAGACCAGATCTGAGCCTGGGAGCTCTCTGGCTAACTAGGGAACCCACT GCTTAAGCCTCAATAAAGCTTGCCTTGAGTGCTTCAAGTAGTGTGTGCCCGTCTGTTGTGTGACTCTGGTAACTAGAGATCCCTCAGACCCTTTTAGTCA GTGTGGAAAATCTCTAGCA `HIV_B.K03455.HXB2.amplicons.fasta` ref|HIV_B.K03455.HXB2|reg|PRRT| CCCTCAGGTCACTCTTTGGCAACGACCCCTCGTCACAATAAAGATAGGGGGGCAACTAAAGGAAGCTCTATTAGATACAG GAGCAGATGATACAGTATTAGAAGAAATGAGTTTGCCAGGAAGATGGAAACCAAAAATGATAGGGGGAATTGGAGGTTTT ATCAAAGTAAGACAGTATGATCAGATACTCATAGAAATCTGTGGACATAAAGCTATAGGTACAGTATTAGTAGGACCTAC ACCTGTCAACATAATTGGAAGAAATCTGTTGACTCAGATTGGTTGCACTTTAAATTTTCCCATTAGCCCTATTGAGACTG TACCAGTAAAATTAAAGCCAGGAATGGATGGCCCAAAAGTTAAACAATGGCCATTGACAGAAGAAAAAATAAAAGCATTA GTAGAAATTTGTACAGAGATGGAAAAGGAAGGGAAAATTTCAAAAATTGGGCCTGAAAATCCATACAATACTCCAGTATT TGCCATAAAGAAAAAAGACAGTACTAAATGGAGAAAATTAGTAGATTTCAGAGAACTTAATAAGAGAACTCAAGACTTCT GGGAAGTTCAATTAGGAATACCACATCCCGCAGGGTTAAAAAAGAAAAAATCAGTAACAGTACTGGATGTGGGTGATGCA TATTTTTCAGTTCCCTTAGATGAAGACTTCAGGAAGTATACTGCATTTACCATACCTAGTATAAACAATGAGACACCAGG GATTAGATATCAGTACAATGTGCTTCCACAGGGATGGAAAGGATCACCAGCAATATTCCAAAGTAGCATGACAAAAATCT TAGAGCCTTTTAGAAAACAAAATCCAGACATAGTTATCTATCAATACATGGATGATTTGTATGTAGGATCTGACTTAGAA ATAGGGCAGCATAGAACAAAAATAGAGGAGCTGAGACAACATCTGTTGAGGTGGGGACTTACCACACCAGACAAAAAACA TCAGAAAGAACCTCCATTCCTTTGGATGGGTTATGAACTCCATCCTGATAAATGGACAGTACAGCCTATAGTGCTGCCAG AAAAAGACAGCTGGACTGTCAATGACATACAGAAGTTAGTGGGGAAATTGAATTGGGCAAGTCAGATTTACCCAGGGATT AAAGTAAGGCAATTATGTAAACTCCTTAGAGGAACCAAAGCACTAACAGAAGTAATACCACTAACAGAAGAAGCAGAGCT AGAACTGGCAGAAAACAGAGAGATTCTAAAAGAACCAGTACATGGAGTGTATTATGACCCATCAAAAGACTTAATAGCAG AAATACAGAAGCAGGGGCAAGGCCAATGGACATATCAAATTTATCAAGAGCCATTTAAAAATCTGAAAACAGGAAAATAT GCAAGAATGAGGGGTGCCCACACTAATGATGTAAAACAATTAACAGAGGCAGTGCAAAAAATAACCACAGAAAGCATAGT AATATGGGGAAAGACTCCTAAATTTAAACTGCCCATACAAAAGGAAACATGGGAAACATGGTGGACAGAGTATTGGCAAG CCACCTGGATTCCTGAGTGGGAGTTTGTTAATACCCCTCCCTTAGTGAAATTATGGTACCAGTTAGAGAAAGAACCCATA GTAGGAGCAGAAACCTTC ref|HIV_B.K03455.HXB2|reg|INT| TTTTTAGATGGAATAGATAAGGCCCAAGATGAACATGAGAAATATCACAGTAATTGGAGAGCAATGGCTAGTGATTTTAA CCTGCCACCTGTAGTAGCAAAAGAAATAGTAGCCAGCTGTGATAAATGTCAGCTAAAAGGAGAAGCCATGCATGGACAAG TAGACTGTAGTCCAGGAATATGGCAACTAGATTGTACACATTTAGAAGGAAAAGTTATCCTGGTAGCAGTTCATGTAGCC AGTGGATATATAGAAGCAGAAGTTATTCCAGCAGAAACAGGGCAGGAAACAGCATATTTTCTTTTAAAATTAGCAGGAAG ATGGCCAGTAAAAACAATACATACTGACAATGGCAGCAATTTCACCGGTGCTACGGTTAGGGCCGCCTGTTGGTGGGCGG GAATCAAGCAGGAATTTGGAATTCCCTACAATCCCCAAAGTCAAGGAGTAGTAGAATCTATGAATAAAGAATTAAAGAAA ATTATAGGACAGGTAAGAGATCAGGCTGAACATCTTAAGACAGCAGTACAAATGGCAGTATTCATCCACAATTTTAAAAG AAAAGGGGGGATTGGGGGGTACAGTGCAGGGGAAAGAATAGTAGACATAATAGCAACAGACATACAAACTAAAGAATTAC AAAAACAAATTACAAAAATTCAAAATTTTCGGGTTTATTACAGGGACAGCAGAAATCCACTTTGGAAAGGACCAGCAAAG CTCCTCTGGAAAGGTGAAGGGGCAGTAGTAATACAAGATAATAGTGACATAAAAGTAGTGCCAAGAAGAAAAGCAAAGAT CATTAGGGATTATGGAAAACAGATGGCAGGTGATGATTGTGTGGCAAGTAGACAGGATGAGGAT ref|HIV_B.K03455.HXB2|reg|gp120| CAGTAGATCCTAGACTAGAGCCCTGGAAGCATCCAGGAAGTCAGCCTAAAACTGCTTGTACCAATTGCTATTGTAAAAAG TGTTGCTTTCATTGCCAAGTTTGTTTCATAACAAAAGCCTTAGGCATCTCCTATGGCAGGAAGAAGCGGAGACAGCGACG AAGAGCTCATCAGAACAGTCAGACTCATCAAGCTTCTCTATCAAAGCAGTAAGTAGTACATGTAACGCAACCTATACCAA TAGTAGCAATAGTAGCATTAGTAGTAGCAATAATAATAGCAATAGTTGTGTGGTCCATAGTAATCATAGAATATAGGAAA ATATTAAGACAAAGAAAAATAGACAGGTTAATTGATAGACTAATAGAAAGAGCAGAAGACAGTGGCAATGAGAGTGAAGG AGAAATATCAGCACTTGTGGAGATGGGGGTGGAGATGGGGCACCATGCTCCTTGGGATGTTGATGATCTGTAGTGCTACA GAAAAATTGTGGGTCACAGTCTATTATGGGGTACCTGTGTGGAAGGAAGCAACCACCACTCTATTTTGTGCATCAGATGC TAAAGCATATGATACAGAGGTACATAATGTTTGGGCCACACATGCCTGTGTACCCACAGACCCCAACCCACAAGAAGTAG TATTGGTAAATGTGACAGAAAATTTTAACATGTGGAAAAATGACATGGTAGAACAGATGCATGAGGATATAATCAGTTTA TGGGATCAAAGCCTAAAGCCATGTGTAAAATTAACCCCACTCTGTGTTAGTTTAAAGTGCACTGATTTGAAGAATGATAC TAATACCAATAGTAGTAGCGGGAGAATGATAATGGAGAAAGGAGAGATAAAAAACTGCTCTTTCAATATCAGCACAAGCA TAAGAGGTAAGGTGCAGAAAGAATATGCATTTTTTTATAAACTTGATATAATACCAATAGATAATGATACTACCAGCTAT AAGTTGACAAGTTGTAACACCTCAGTCATTACACAGGCCTGTCCAAAGGTATCCTTTGAGCCAATTCCCATACATTATTG TGCCCCGGCTGGTTTTGCGATTCTAAAATGTAATAATAAGACGTTCAATGGAACAGGACCATGTACAAATGTCAGCACAG TACAATGTACACATGGAATTAGGCCAGTAGTATCAACTCAACTGCTGTTAAATGGCAGTCTAGCAGAAGAAGAGGTAGTA ATTAGATCTGTCAATTTCACGGACAATGCTAAAACCATAATAGTACAGCTGAACACATCTGTAGAAATTAATTGTACAAG ACCCAACAACAATACAAGAAAAAGAATCCGTATCCAGAGAGGACCAGGGAGAGCATTTGTTACAATAGGAAAAATAGGAA ATATGAGACAAGCACATTGTAACATTAGTAGAGCAAAATGGAATAACACTTTAAAACAGATAGCTAGCAAATTAAGAGAA CAATTTGGAAATAATAAAACAATAATCTTTAAGCAATCCTCAGGAGGGGACCCAGAAATTGTAACGCACAGTTTTAATTG TGGAGGGGAATTTTTCTACTGTAATTCAACACAACTGTTTAATAGTACTTGGTTTAATAGTACTTGGAGTACTGAAGGGT CAAATAACACTGAAGGAAGTGACACAATCACCCTCCCATGCAGAATAAAACAAATTATAAACATGTGGCAGAAAGTAGGA AAAGCAATGTATGCCCCTCCCATCAGTGGACAAATTAGATGTTCATCAAATATTACAGGGCTGCTATTAACAAGAGATGG TGGTAATAGCAACAATGAGTCCGAGATCTTCAGACCTGGAGGAGGAGATATGAGGGACAATTGGAGAAGTGAATTATATA AATATAAAGTAGTAAAAATTGAACCATTAGGAGTAGCACCCACCAAGGCAAAGAGAAGAGTGGTGCAGAGAGAAAAAAGA `HIV_B.K03455.HXB2.gtf` HIV_B.K03455.HXB2 LANL amplicon 2252 3869 . + 2 name PRRT ; primary_cds 2252-2549 ; alt_cds 2550-3869 ; HIV_B.K03455.HXB2 LANL amplicon 4230 5093 . + 0 name INT ; primary_cds 2085-5096 ; alt_cds 5098-5619 ; HIV_B.K03455.HXB2 LANL amplicon 6225 7757 . + 1 name gp120 ; primary_cds 6225-8795 ; alt_cds 5831-6223 ;","title":"Installation"},{"location":"install/#haphpipe-installation-instructions","text":"HAPHIPE depends on more than a dozen different programs, each of which may itself depend on other programs and libraries. Installing everything separately would be a nightmare, so you should use the package manager \"Bioconda\" to install everything. Bioconda is a popular package manager in the bioinformatics world. See the Helpful Resources section for more information and resources for Bioconda. Here, we will describe where to get Bioconda and how to install it, then how to use Bioconda to install the necessary programs for HAPHPIPE. We will also detail the acquisition and installation of one program, GATK, that is not handled by Bioconda, and finally the acquisition and installation of HAPHPIPE itself. Here, we describe the procedure for installing HAPHPIPE using the package manager Bioconda (Gr\u00fcning et al. 2018) on the command line. If you are unfamiliar with Bioconda, please see for installation and channel setup. HAPHPIPE is available here and is written in Python 3.7.2 coding language. The installation process begins with the creation of a conda environment that installs the necessary dependencies required by HAPHPIPE. Once the conda environment has been created, it can be activated for use with the command conda activate haphpipe. Due to license restrictions, Bioconda cannot distribute and install GATK (McKenna et al. 2010; Van der Auwera et al. 2013; Poplin et al. 2018) directly. To fully install GATK, you must download a licensed copy of GATK (version 3.8-0) from the Broad Institute (link) . You can then install HAPHPIPE using the single command pip install git+git://github.com/gwcbi/haphpipe.git , which pulls the repository from Github. 1. Install conda Download the conda package: wget https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-x86_64.sh sh Miniconda3-latest-MacOSX-x86_64.sh 2. Set up conda channels These need to be put in as a command in the order they come, as this sets the priority for where packages are pulled from. Therefore, in this order, conda-forge is top priority and explored first when downloading a program. conda config --add channels R conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge 3. Create a conda environment with the following dependencies conda create -n haphpipe \\ python=3.7 \\ future \\ pyyaml \\ biopython \\ seqtk \\ bowtie2 \\ bwa \\ flash \\ freebayes \\ mummer \\ picard \\ trimmomatic \\ samtools=1.9 \\ gatk=3.8 \\ spades \\ blast \\ sierrapy Note: on some HPC systems (including GW's Pegasus), certain dependencies must be installed as a 'User Install'. If this is the case (which will be apparent if you receive availability errors for any of the above packages after installing them through conda) use the following command to install the package: pip install --user package name 4. Activate the environment conda activate haphpipe 5. Install GATK Due to license restrictions, bioconda cannot distribute and install GATK directly. To fully install GATK, you must download a licensed copy of GATK (version 3.8-0) from the Broad Institute: https://software.broadinstitute.org/gatk/download/archive . Register the package using gatk3-register: gatk3-register /path/to/GenomeAnalysisTK-3.8-0-ge9d806836.tar.bz2 This will copy GATK into your conda environment. Alternatively, GATK may be installed directly on the command line with the following commands: mkdir -p /path/to/gatk_dir wget -O - 'https://software.broadinstitute.org/gatk/download/auth?package=GATK-archive version=3.6-0-g89b7209' | tar -xjvC /path/to/gatk_dir gatk3-register /path/to/gatk_dir/GenomeAnalysisTK.jar NOTE: HAPHPIPE was developed and tested using GATK 3.8. 6. Install HAPHPIPE pip install git+git://github.com/gwcbi/haphpipe.git Upon completion of the installation, you can test it to ensure the repository has been installed completely and correctly by running haphpipe -h (in quick start). Once HAPHPIPE is installed and performing correctly, there is no need to install it again; simply activate the conda environment when needed by executing conda activate haphpipe . If a new version is released in the future, HAPHPIPE can be updated by running the command line pip install --upgrade git+git://github.com/gwcbi/haphpipe.git . At any point, the -h option that follows any HAPHPIPE stage will output a help message that provides a description of the stage and the desired input(s) and output(s).","title":"HAPHPIPE Installation Instructions"},{"location":"install/#predicthaplo-installation-instructions","text":"Users are required to download PredictHaplo on their own computing system prior to running any of the haplotype stages ( hp_predict_haplo and hp_ph_parser ). Here is how the GW CBI team installed PredictHaplo onto our HPC, which has a slurm scheduling system and uses Lua module files. We cannot help with the installation of this software, but have provided the code that we used here to install PredictHaplo onto our system. Please see their website for contact information if needed. This module loads predicthaplo onto GWU's HPC - Colonial One. See https://bmda.dmi.unibas.ch/software.html cd /path/to/modules/predicthaplo # use gcc 4.9.4, add blas and lapack to library path module load gcc/4.9.4 module load blas/gcc/64 module load lapack/gcc/64 # download source cd archive wget https://bmda.dmi.unibas.ch/software/PredictHaplo-Paired-0.4.tgz cd .. # unzip source and change directory name tar xfvz archive/PredictHaplo-Paired-0.4.tgz mv PredictHaplo-Paired-0.4 0.4 cd 0.4 # install scythestat tar xfvz scythestat-1.0.3.tar.gz cd scythestat-1.0.3 ./configure --prefix=/path/to/modules/predicthaplo/0.4/NEWSCYTHE make install cd .. # compile predicthaplo make If a segfault error occurs during the hp_predict_haplo stage, this is not a characteristic of HAPHPIPE but rather that of PredictHaplo. Sometimes, we have luck if we just rerun the code again or move to an interactive CPU node. We are unsure what causes this error, and we only see it between the local and global reconstruction phases in PredictHaplo.","title":"PredictHaplo Installation Instructions"},{"location":"install/#quick-start","text":"1. Activate haphpipe Make sure you have conda running. For students at GW using Colonial One, you need to load the miniconda3 module like such prior to activating the haphpipe conda environemnt: module load miniconda3 . conda activate haphpipe 2. Test that it is loaded correctly haphpipe -h should produce: Program: haphpipe (haplotype and phylodynamics pipeline) Version: 0.8.1 Commands: -- Reads sample_reads subsample reads using seqtk trim_reads trim reads using Trimmomatic join_reads join reads using FLASh ec_reads error correct reads using SPAdes -- Assemble assemble_denovo assemble reads denovo assemble_amplicons assemble contigs to amplicon regions assemble_scaffold assemble contigs to genome align_reads align reads to reference call_variants call variants vcf_to_consensus create consensus sequence from VCF refine_assembly iterative refinement: align - variants - consensus finalize_assembly finalize consensus sequence -- Haplotype predict_haplo assemble haplotypes with PredictHaplo ph_parser parse output from PredictHaplo. -- Annotate pairwise_align align consensus to an annotated reference extract_pairwise extract sequence regions from pairwise alignment annotate_from_ref annotate consensus from reference annotation -- Miscellaneous demo setup demo directory and test data","title":"Quick-Start"},{"location":"install/#directory-structure","text":"Below is the recommended directory structure for using HAPHPIPE: We recommend creating a separate directory for each sample, as well as a directory for reference files.","title":"Directory Structure"},{"location":"install/#windows-users","text":"HAPHPIPE is only available for Mac OSX or Linux platforms. We suggest the following options for running HAPHPIPE on a Windows machine: Run HAPHPIPE on your institution's HPC cluster, if available. Utilize the Windows Subsystem for Linux . Run Linux in a virtual machine via VirtualBox","title":"Windows Users"},{"location":"install/#reference-files","text":"Several modules in HAPHPIPE use reference files: either a FASTA file containing a reference sequence or a GTF file denoting genome regions for amplicon assembly. With HIV data, we use the file HIV_B.K03455.HXB2.fasta as a reference whole-genome file, HIV_B.K03455.HXB2.amplicons.fasta for amplicon assembly, and HIV_B.K03455.HXB2.gtf as a GTF file. All three files are downloaded in the demo module . The contents of each file are below. `HIV_B.K03455.HXB2.fasta` HIV_B.K03455.HXB2 TGGAAGGGCTAATTCACTCCCAACGAAGACAAGATATCCTTGATCTGTGGATCTACCACACACAAGGCTACTTCCCTGATTAGCAGAACTACACACCAGG GCCAGGGATCAGATATCCACTGACCTTTGGATGGTGCTACAAGCTAGTACCAGTTGAGCCAGAGAAGTTAGAAGAAGCCAACAAAGGAGAGAACACCAGC TTGTTACACCCTGTGAGCCTGCATGGAATGGATGACCCGGAGAGAGAAGTGTTAGAGTGGAGGTTTGACAGCCGCCTAGCATTTCATCACATGGCCCGAG AGCTGCATCCGGAGTACTTCAAGAACTGCTGACATCGAGCTTGCTACAAGGGACTTTCCGCTGGGGACTTTCCAGGGAGGCGTGGCCTGGGCGGGACTGG GGAGTGGCGAGCCCTCAGATCCTGCATATAAGCAGCTGCTTTTTGCCTGTACTGGGTCTCTCTGGTTAGACCAGATCTGAGCCTGGGAGCTCTCTGGCTA ACTAGGGAACCCACTGCTTAAGCCTCAATAAAGCTTGCCTTGAGTGCTTCAAGTAGTGTGTGCCCGTCTGTTGTGTGACTCTGGTAACTAGAGATCCCTC AGACCCTTTTAGTCAGTGTGGAAAATCTCTAGCAGTGGCGCCCGAACAGGGACCTGAAAGCGAAAGGGAAACCAGAGGAGCTCTCTCGACGCAGGACTCG GCTTGCTGAAGCGCGCACGGCAAGAGGCGAGGGGCGGCGACTGGTGAGTACGCCAAAAATTTTGACTAGCGGAGGCTAGAAGGAGAGAGATGGGTGCGAG AGCGTCAGTATTAAGCGGGGGAGAATTAGATCGATGGGAAAAAATTCGGTTAAGGCCAGGGGGAAAGAAAAAATATAAATTAAAACATATAGTATGGGCA AGCAGGGAGCTAGAACGATTCGCAGTTAATCCTGGCCTGTTAGAAACATCAGAAGGCTGTAGACAAATACTGGGACAGCTACAACCATCCCTTCAGACAG GATCAGAAGAACTTAGATCATTATATAATACAGTAGCAACCCTCTATTGTGTGCATCAAAGGATAGAGATAAAAGACACCAAGGAAGCTTTAGACAAGAT AGAGGAAGAGCAAAACAAAAGTAAGAAAAAAGCACAGCAAGCAGCAGCTGACACAGGACACAGCAATCAGGTCAGCCAAAATTACCCTATAGTGCAGAAC ATCCAGGGGCAAATGGTACATCAGGCCATATCACCTAGAACTTTAAATGCATGGGTAAAAGTAGTAGAAGAGAAGGCTTTCAGCCCAGAAGTGATACCCA TGTTTTCAGCATTATCAGAAGGAGCCACCCCACAAGATTTAAACACCATGCTAAACACAGTGGGGGGACATCAAGCAGCCATGCAAATGTTAAAAGAGAC CATCAATGAGGAAGCTGCAGAATGGGATAGAGTGCATCCAGTGCATGCAGGGCCTATTGCACCAGGCCAGATGAGAGAACCAAGGGGAAGTGACATAGCA GGAACTACTAGTACCCTTCAGGAACAAATAGGATGGATGACAAATAATCCACCTATCCCAGTAGGAGAAATTTATAAAAGATGGATAATCCTGGGATTAA ATAAAATAGTAAGAATGTATAGCCCTACCAGCATTCTGGACATAAGACAAGGACCAAAGGAACCCTTTAGAGACTATGTAGACCGGTTCTATAAAACTCT AAGAGCCGAGCAAGCTTCACAGGAGGTAAAAAATTGGATGACAGAAACCTTGTTGGTCCAAAATGCGAACCCAGATTGTAAGACTATTTTAAAAGCATTG GGACCAGCGGCTACACTAGAAGAAATGATGACAGCATGTCAGGGAGTAGGAGGACCCGGCCATAAGGCAAGAGTTTTGGCTGAAGCAATGAGCCAAGTAA CAAATTCAGCTACCATAATGATGCAGAGAGGCAATTTTAGGAACCAAAGAAAGATTGTTAAGTGTTTCAATTGTGGCAAAGAAGGGCACACAGCCAGAAA TTGCAGGGCCCCTAGGAAAAAGGGCTGTTGGAAATGTGGAAAGGAAGGACACCAAATGAAAGATTGTACTGAGAGACAGGCTAATTTTTTAGGGAAGATC TGGCCTTCCTACAAGGGAAGGCCAGGGAATTTTCTTCAGAGCAGACCAGAGCCAACAGCCCCACCAGAAGAGAGCTTCAGGTCTGGGGTAGAGACAACAA CTCCCCCTCAGAAGCAGGAGCCGATAGACAAGGAACTGTATCCTTTAACTTCCCTCAGGTCACTCTTTGGCAACGACCCCTCGTCACAATAAAGATAGGG GGGCAACTAAAGGAAGCTCTATTAGATACAGGAGCAGATGATACAGTATTAGAAGAAATGAGTTTGCCAGGAAGATGGAAACCAAAAATGATAGGGGGAA TTGGAGGTTTTATCAAAGTAAGACAGTATGATCAGATACTCATAGAAATCTGTGGACATAAAGCTATAGGTACAGTATTAGTAGGACCTACACCTGTCAA CATAATTGGAAGAAATCTGTTGACTCAGATTGGTTGCACTTTAAATTTTCCCATTAGCCCTATTGAGACTGTACCAGTAAAATTAAAGCCAGGAATGGAT GGCCCAAAAGTTAAACAATGGCCATTGACAGAAGAAAAAATAAAAGCATTAGTAGAAATTTGTACAGAGATGGAAAAGGAAGGGAAAATTTCAAAAATTG GGCCTGAAAATCCATACAATACTCCAGTATTTGCCATAAAGAAAAAAGACAGTACTAAATGGAGAAAATTAGTAGATTTCAGAGAACTTAATAAGAGAAC TCAAGACTTCTGGGAAGTTCAATTAGGAATACCACATCCCGCAGGGTTAAAAAAGAAAAAATCAGTAACAGTACTGGATGTGGGTGATGCATATTTTTCA GTTCCCTTAGATGAAGACTTCAGGAAGTATACTGCATTTACCATACCTAGTATAAACAATGAGACACCAGGGATTAGATATCAGTACAATGTGCTTCCAC AGGGATGGAAAGGATCACCAGCAATATTCCAAAGTAGCATGACAAAAATCTTAGAGCCTTTTAGAAAACAAAATCCAGACATAGTTATCTATCAATACAT GGATGATTTGTATGTAGGATCTGACTTAGAAATAGGGCAGCATAGAACAAAAATAGAGGAGCTGAGACAACATCTGTTGAGGTGGGGACTTACCACACCA GACAAAAAACATCAGAAAGAACCTCCATTCCTTTGGATGGGTTATGAACTCCATCCTGATAAATGGACAGTACAGCCTATAGTGCTGCCAGAAAAAGACA GCTGGACTGTCAATGACATACAGAAGTTAGTGGGGAAATTGAATTGGGCAAGTCAGATTTACCCAGGGATTAAAGTAAGGCAATTATGTAAACTCCTTAG AGGAACCAAAGCACTAACAGAAGTAATACCACTAACAGAAGAAGCAGAGCTAGAACTGGCAGAAAACAGAGAGATTCTAAAAGAACCAGTACATGGAGTG TATTATGACCCATCAAAAGACTTAATAGCAGAAATACAGAAGCAGGGGCAAGGCCAATGGACATATCAAATTTATCAAGAGCCATTTAAAAATCTGAAAA CAGGAAAATATGCAAGAATGAGGGGTGCCCACACTAATGATGTAAAACAATTAACAGAGGCAGTGCAAAAAATAACCACAGAAAGCATAGTAATATGGGG AAAGACTCCTAAATTTAAACTGCCCATACAAAAGGAAACATGGGAAACATGGTGGACAGAGTATTGGCAAGCCACCTGGATTCCTGAGTGGGAGTTTGTT AATACCCCTCCCTTAGTGAAATTATGGTACCAGTTAGAGAAAGAACCCATAGTAGGAGCAGAAACCTTCTATGTAGATGGGGCAGCTAACAGGGAGACTA AATTAGGAAAAGCAGGATATGTTACTAATAGAGGAAGACAAAAAGTTGTCACCCTAACTGACACAACAAATCAGAAGACTGAGTTACAAGCAATTTATCT AGCTTTGCAGGATTCGGGATTAGAAGTAAACATAGTAACAGACTCACAATATGCATTAGGAATCATTCAAGCACAACCAGATCAAAGTGAATCAGAGTTA GTCAATCAAATAATAGAGCAGTTAATAAAAAAGGAAAAGGTCTATCTGGCATGGGTACCAGCACACAAAGGAATTGGAGGAAATGAACAAGTAGATAAAT TAGTCAGTGCTGGAATCAGGAAAGTACTATTTTTAGATGGAATAGATAAGGCCCAAGATGAACATGAGAAATATCACAGTAATTGGAGAGCAATGGCTAG TGATTTTAACCTGCCACCTGTAGTAGCAAAAGAAATAGTAGCCAGCTGTGATAAATGTCAGCTAAAAGGAGAAGCCATGCATGGACAAGTAGACTGTAGT CCAGGAATATGGCAACTAGATTGTACACATTTAGAAGGAAAAGTTATCCTGGTAGCAGTTCATGTAGCCAGTGGATATATAGAAGCAGAAGTTATTCCAG CAGAAACAGGGCAGGAAACAGCATATTTTCTTTTAAAATTAGCAGGAAGATGGCCAGTAAAAACAATACATACTGACAATGGCAGCAATTTCACCGGTGC TACGGTTAGGGCCGCCTGTTGGTGGGCGGGAATCAAGCAGGAATTTGGAATTCCCTACAATCCCCAAAGTCAAGGAGTAGTAGAATCTATGAATAAAGAA TTAAAGAAAATTATAGGACAGGTAAGAGATCAGGCTGAACATCTTAAGACAGCAGTACAAATGGCAGTATTCATCCACAATTTTAAAAGAAAAGGGGGGA TTGGGGGGTACAGTGCAGGGGAAAGAATAGTAGACATAATAGCAACAGACATACAAACTAAAGAATTACAAAAACAAATTACAAAAATTCAAAATTTTCG GGTTTATTACAGGGACAGCAGAAATCCACTTTGGAAAGGACCAGCAAAGCTCCTCTGGAAAGGTGAAGGGGCAGTAGTAATACAAGATAATAGTGACATA AAAGTAGTGCCAAGAAGAAAAGCAAAGATCATTAGGGATTATGGAAAACAGATGGCAGGTGATGATTGTGTGGCAAGTAGACAGGATGAGGATTAGAACA TGGAAAAGTTTAGTAAAACACCATATGTATGTTTCAGGGAAAGCTAGGGGATGGTTTTATAGACATCACTATGAAAGCCCTCATCCAAGAATAAGTTCAG AAGTACACATCCCACTAGGGGATGCTAGATTGGTAATAACAACATATTGGGGTCTGCATACAGGAGAAAGAGACTGGCATTTGGGTCAGGGAGTCTCCAT AGAATGGAGGAAAAAGAGATATAGCACACAAGTAGACCCTGAACTAGCAGACCAACTAATTCATCTGTATTACTTTGACTGTTTTTCAGACTCTGCTATA AGAAAGGCCTTATTAGGACACATAGTTAGCCCTAGGTGTGAATATCAAGCAGGACATAACAAGGTAGGATCTCTACAATACTTGGCACTAGCAGCATTAA TAACACCAAAAAAGATAAAGCCACCTTTGCCTAGTGTTACGAAACTGACAGAGGATAGATGGAACAAGCCCCAGAAGACCAAGGGCCACAGAGGGAGCCA CACAATGAATGGACACTAGAGCTTTTAGAGGAGCTTAAGAATGAAGCTGTTAGACATTTTCCTAGGATTTGGCTCCATGGCTTAGGGCAACATATCTATG AAACTTATGGGGATACTTGGGCAGGAGTGGAAGCCATAATAAGAATTCTGCAACAACTGCTGTTTATCCATTTTCAGAATTGGGTGTCGACATAGCAGAA TAGGCGTTACTCGACAGAGGAGAGCAAGAAATGGAGCCAGTAGATCCTAGACTAGAGCCCTGGAAGCATCCAGGAAGTCAGCCTAAAACTGCTTGTACCA ATTGCTATTGTAAAAAGTGTTGCTTTCATTGCCAAGTTTGTTTCATAACAAAAGCCTTAGGCATCTCCTATGGCAGGAAGAAGCGGAGACAGCGACGAAG AGCTCATCAGAACAGTCAGACTCATCAAGCTTCTCTATCAAAGCAGTAAGTAGTACATGTAACGCAACCTATACCAATAGTAGCAATAGTAGCATTAGTA GTAGCAATAATAATAGCAATAGTTGTGTGGTCCATAGTAATCATAGAATATAGGAAAATATTAAGACAAAGAAAAATAGACAGGTTAATTGATAGACTAA TAGAAAGAGCAGAAGACAGTGGCAATGAGAGTGAAGGAGAAATATCAGCACTTGTGGAGATGGGGGTGGAGATGGGGCACCATGCTCCTTGGGATGTTGA TGATCTGTAGTGCTACAGAAAAATTGTGGGTCACAGTCTATTATGGGGTACCTGTGTGGAAGGAAGCAACCACCACTCTATTTTGTGCATCAGATGCTAA AGCATATGATACAGAGGTACATAATGTTTGGGCCACACATGCCTGTGTACCCACAGACCCCAACCCACAAGAAGTAGTATTGGTAAATGTGACAGAAAAT TTTAACATGTGGAAAAATGACATGGTAGAACAGATGCATGAGGATATAATCAGTTTATGGGATCAAAGCCTAAAGCCATGTGTAAAATTAACCCCACTCT GTGTTAGTTTAAAGTGCACTGATTTGAAGAATGATACTAATACCAATAGTAGTAGCGGGAGAATGATAATGGAGAAAGGAGAGATAAAAAACTGCTCTTT CAATATCAGCACAAGCATAAGAGGTAAGGTGCAGAAAGAATATGCATTTTTTTATAAACTTGATATAATACCAATAGATAATGATACTACCAGCTATAAG TTGACAAGTTGTAACACCTCAGTCATTACACAGGCCTGTCCAAAGGTATCCTTTGAGCCAATTCCCATACATTATTGTGCCCCGGCTGGTTTTGCGATTC TAAAATGTAATAATAAGACGTTCAATGGAACAGGACCATGTACAAATGTCAGCACAGTACAATGTACACATGGAATTAGGCCAGTAGTATCAACTCAACT GCTGTTAAATGGCAGTCTAGCAGAAGAAGAGGTAGTAATTAGATCTGTCAATTTCACGGACAATGCTAAAACCATAATAGTACAGCTGAACACATCTGTA GAAATTAATTGTACAAGACCCAACAACAATACAAGAAAAAGAATCCGTATCCAGAGAGGACCAGGGAGAGCATTTGTTACAATAGGAAAAATAGGAAATA TGAGACAAGCACATTGTAACATTAGTAGAGCAAAATGGAATAACACTTTAAAACAGATAGCTAGCAAATTAAGAGAACAATTTGGAAATAATAAAACAAT AATCTTTAAGCAATCCTCAGGAGGGGACCCAGAAATTGTAACGCACAGTTTTAATTGTGGAGGGGAATTTTTCTACTGTAATTCAACACAACTGTTTAAT AGTACTTGGTTTAATAGTACTTGGAGTACTGAAGGGTCAAATAACACTGAAGGAAGTGACACAATCACCCTCCCATGCAGAATAAAACAAATTATAAACA TGTGGCAGAAAGTAGGAAAAGCAATGTATGCCCCTCCCATCAGTGGACAAATTAGATGTTCATCAAATATTACAGGGCTGCTATTAACAAGAGATGGTGG TAATAGCAACAATGAGTCCGAGATCTTCAGACCTGGAGGAGGAGATATGAGGGACAATTGGAGAAGTGAATTATATAAATATAAAGTAGTAAAAATTGAA CCATTAGGAGTAGCACCCACCAAGGCAAAGAGAAGAGTGGTGCAGAGAGAAAAAAGAGCAGTGGGAATAGGAGCTTTGTTCCTTGGGTTCTTGGGAGCAG CAGGAAGCACTATGGGCGCAGCCTCAATGACGCTGACGGTACAGGCCAGACAATTATTGTCTGGTATAGTGCAGCAGCAGAACAATTTGCTGAGGGCTAT TGAGGCGCAACAGCATCTGTTGCAACTCACAGTCTGGGGCATCAAGCAGCTCCAGGCAAGAATCCTGGCTGTGGAAAGATACCTAAAGGATCAACAGCTC CTGGGGATTTGGGGTTGCTCTGGAAAACTCATTTGCACCACTGCTGTGCCTTGGAATGCTAGTTGGAGTAATAAATCTCTGGAACAGATTTGGAATCACA CGACCTGGATGGAGTGGGACAGAGAAATTAACAATTACACAAGCTTAATACACTCCTTAATTGAAGAATCGCAAAACCAGCAAGAAAAGAATGAACAAGA ATTATTGGAATTAGATAAATGGGCAAGTTTGTGGAATTGGTTTAACATAACAAATTGGCTGTGGTATATAAAATTATTCATAATGATAGTAGGAGGCTTG GTAGGTTTAAGAATAGTTTTTGCTGTACTTTCTATAGTGAATAGAGTTAGGCAGGGATATTCACCATTATCGTTTCAGACCCACCTCCCAACCCCGAGGG GACCCGACAGGCCCGAAGGAATAGAAGAAGAAGGTGGAGAGAGAGACAGAGACAGATCCATTCGATTAGTGAACGGATCCTTGGCACTTATCTGGGACGA TCTGCGGAGCCTGTGCCTCTTCAGCTACCACCGCTTGAGAGACTTACTCTTGATTGTAACGAGGATTGTGGAACTTCTGGGACGCAGGGGGTGGGAAGCC CTCAAATATTGGTGGAATCTCCTACAGTATTGGAGTCAGGAACTAAAGAATAGTGCTGTTAGCTTGCTCAATGCCACAGCCATAGCAGTAGCTGAGGGGA CAGATAGGGTTATAGAAGTAGTACAAGGAGCTTGTAGAGCTATTCGCCACATACCTAGAAGAATAAGACAGGGCTTGGAAAGGATTTTGCTATAAGATGG GTGGCAAGTGGTCAAAAAGTAGTGTGATTGGATGGCCTACTGTAAGGGAAAGAATGAGACGAGCTGAGCCAGCAGCAGATAGGGTGGGAGCAGCATCTCG AGACCTGGAAAAACATGGAGCAATCACAAGTAGCAATACAGCAGCTACCAATGCTGCTTGTGCCTGGCTAGAAGCACAAGAGGAGGAGGAGGTGGGTTTT CCAGTCACACCTCAGGTACCTTTAAGACCAATGACTTACAAGGCAGCTGTAGATCTTAGCCACTTTTTAAAAGAAAAGGGGGGACTGGAAGGGCTAATTC ACTCCCAAAGAAGACAAGATATCCTTGATCTGTGGATCTACCACACACAAGGCTACTTCCCTGATTAGCAGAACTACACACCAGGGCCAGGGGTCAGATA TCCACTGACCTTTGGATGGTGCTACAAGCTAGTACCAGTTGAGCCAGATAAGATAGAAGAGGCCAATAAAGGAGAGAACACCAGCTTGTTACACCCTGTG AGCCTGCATGGGATGGATGACCCGGAGAGAGAAGTGTTAGAGTGGAGGTTTGACAGCCGCCTAGCATTTCATCACGTGGCCCGAGAGCTGCATCCGGAGT ACTTCAAGAACTGCTGACATCGAGCTTGCTACAAGGGACTTTCCGCTGGGGACTTTCCAGGGAGGCGTGGCCTGGGCGGGACTGGGGAGTGGCGAGCCCT CAGATCCTGCATATAAGCAGCTGCTTTTTGCCTGTACTGGGTCTCTCTGGTTAGACCAGATCTGAGCCTGGGAGCTCTCTGGCTAACTAGGGAACCCACT GCTTAAGCCTCAATAAAGCTTGCCTTGAGTGCTTCAAGTAGTGTGTGCCCGTCTGTTGTGTGACTCTGGTAACTAGAGATCCCTCAGACCCTTTTAGTCA GTGTGGAAAATCTCTAGCA `HIV_B.K03455.HXB2.amplicons.fasta` ref|HIV_B.K03455.HXB2|reg|PRRT| CCCTCAGGTCACTCTTTGGCAACGACCCCTCGTCACAATAAAGATAGGGGGGCAACTAAAGGAAGCTCTATTAGATACAG GAGCAGATGATACAGTATTAGAAGAAATGAGTTTGCCAGGAAGATGGAAACCAAAAATGATAGGGGGAATTGGAGGTTTT ATCAAAGTAAGACAGTATGATCAGATACTCATAGAAATCTGTGGACATAAAGCTATAGGTACAGTATTAGTAGGACCTAC ACCTGTCAACATAATTGGAAGAAATCTGTTGACTCAGATTGGTTGCACTTTAAATTTTCCCATTAGCCCTATTGAGACTG TACCAGTAAAATTAAAGCCAGGAATGGATGGCCCAAAAGTTAAACAATGGCCATTGACAGAAGAAAAAATAAAAGCATTA GTAGAAATTTGTACAGAGATGGAAAAGGAAGGGAAAATTTCAAAAATTGGGCCTGAAAATCCATACAATACTCCAGTATT TGCCATAAAGAAAAAAGACAGTACTAAATGGAGAAAATTAGTAGATTTCAGAGAACTTAATAAGAGAACTCAAGACTTCT GGGAAGTTCAATTAGGAATACCACATCCCGCAGGGTTAAAAAAGAAAAAATCAGTAACAGTACTGGATGTGGGTGATGCA TATTTTTCAGTTCCCTTAGATGAAGACTTCAGGAAGTATACTGCATTTACCATACCTAGTATAAACAATGAGACACCAGG GATTAGATATCAGTACAATGTGCTTCCACAGGGATGGAAAGGATCACCAGCAATATTCCAAAGTAGCATGACAAAAATCT TAGAGCCTTTTAGAAAACAAAATCCAGACATAGTTATCTATCAATACATGGATGATTTGTATGTAGGATCTGACTTAGAA ATAGGGCAGCATAGAACAAAAATAGAGGAGCTGAGACAACATCTGTTGAGGTGGGGACTTACCACACCAGACAAAAAACA TCAGAAAGAACCTCCATTCCTTTGGATGGGTTATGAACTCCATCCTGATAAATGGACAGTACAGCCTATAGTGCTGCCAG AAAAAGACAGCTGGACTGTCAATGACATACAGAAGTTAGTGGGGAAATTGAATTGGGCAAGTCAGATTTACCCAGGGATT AAAGTAAGGCAATTATGTAAACTCCTTAGAGGAACCAAAGCACTAACAGAAGTAATACCACTAACAGAAGAAGCAGAGCT AGAACTGGCAGAAAACAGAGAGATTCTAAAAGAACCAGTACATGGAGTGTATTATGACCCATCAAAAGACTTAATAGCAG AAATACAGAAGCAGGGGCAAGGCCAATGGACATATCAAATTTATCAAGAGCCATTTAAAAATCTGAAAACAGGAAAATAT GCAAGAATGAGGGGTGCCCACACTAATGATGTAAAACAATTAACAGAGGCAGTGCAAAAAATAACCACAGAAAGCATAGT AATATGGGGAAAGACTCCTAAATTTAAACTGCCCATACAAAAGGAAACATGGGAAACATGGTGGACAGAGTATTGGCAAG CCACCTGGATTCCTGAGTGGGAGTTTGTTAATACCCCTCCCTTAGTGAAATTATGGTACCAGTTAGAGAAAGAACCCATA GTAGGAGCAGAAACCTTC ref|HIV_B.K03455.HXB2|reg|INT| TTTTTAGATGGAATAGATAAGGCCCAAGATGAACATGAGAAATATCACAGTAATTGGAGAGCAATGGCTAGTGATTTTAA CCTGCCACCTGTAGTAGCAAAAGAAATAGTAGCCAGCTGTGATAAATGTCAGCTAAAAGGAGAAGCCATGCATGGACAAG TAGACTGTAGTCCAGGAATATGGCAACTAGATTGTACACATTTAGAAGGAAAAGTTATCCTGGTAGCAGTTCATGTAGCC AGTGGATATATAGAAGCAGAAGTTATTCCAGCAGAAACAGGGCAGGAAACAGCATATTTTCTTTTAAAATTAGCAGGAAG ATGGCCAGTAAAAACAATACATACTGACAATGGCAGCAATTTCACCGGTGCTACGGTTAGGGCCGCCTGTTGGTGGGCGG GAATCAAGCAGGAATTTGGAATTCCCTACAATCCCCAAAGTCAAGGAGTAGTAGAATCTATGAATAAAGAATTAAAGAAA ATTATAGGACAGGTAAGAGATCAGGCTGAACATCTTAAGACAGCAGTACAAATGGCAGTATTCATCCACAATTTTAAAAG AAAAGGGGGGATTGGGGGGTACAGTGCAGGGGAAAGAATAGTAGACATAATAGCAACAGACATACAAACTAAAGAATTAC AAAAACAAATTACAAAAATTCAAAATTTTCGGGTTTATTACAGGGACAGCAGAAATCCACTTTGGAAAGGACCAGCAAAG CTCCTCTGGAAAGGTGAAGGGGCAGTAGTAATACAAGATAATAGTGACATAAAAGTAGTGCCAAGAAGAAAAGCAAAGAT CATTAGGGATTATGGAAAACAGATGGCAGGTGATGATTGTGTGGCAAGTAGACAGGATGAGGAT ref|HIV_B.K03455.HXB2|reg|gp120| CAGTAGATCCTAGACTAGAGCCCTGGAAGCATCCAGGAAGTCAGCCTAAAACTGCTTGTACCAATTGCTATTGTAAAAAG TGTTGCTTTCATTGCCAAGTTTGTTTCATAACAAAAGCCTTAGGCATCTCCTATGGCAGGAAGAAGCGGAGACAGCGACG AAGAGCTCATCAGAACAGTCAGACTCATCAAGCTTCTCTATCAAAGCAGTAAGTAGTACATGTAACGCAACCTATACCAA TAGTAGCAATAGTAGCATTAGTAGTAGCAATAATAATAGCAATAGTTGTGTGGTCCATAGTAATCATAGAATATAGGAAA ATATTAAGACAAAGAAAAATAGACAGGTTAATTGATAGACTAATAGAAAGAGCAGAAGACAGTGGCAATGAGAGTGAAGG AGAAATATCAGCACTTGTGGAGATGGGGGTGGAGATGGGGCACCATGCTCCTTGGGATGTTGATGATCTGTAGTGCTACA GAAAAATTGTGGGTCACAGTCTATTATGGGGTACCTGTGTGGAAGGAAGCAACCACCACTCTATTTTGTGCATCAGATGC TAAAGCATATGATACAGAGGTACATAATGTTTGGGCCACACATGCCTGTGTACCCACAGACCCCAACCCACAAGAAGTAG TATTGGTAAATGTGACAGAAAATTTTAACATGTGGAAAAATGACATGGTAGAACAGATGCATGAGGATATAATCAGTTTA TGGGATCAAAGCCTAAAGCCATGTGTAAAATTAACCCCACTCTGTGTTAGTTTAAAGTGCACTGATTTGAAGAATGATAC TAATACCAATAGTAGTAGCGGGAGAATGATAATGGAGAAAGGAGAGATAAAAAACTGCTCTTTCAATATCAGCACAAGCA TAAGAGGTAAGGTGCAGAAAGAATATGCATTTTTTTATAAACTTGATATAATACCAATAGATAATGATACTACCAGCTAT AAGTTGACAAGTTGTAACACCTCAGTCATTACACAGGCCTGTCCAAAGGTATCCTTTGAGCCAATTCCCATACATTATTG TGCCCCGGCTGGTTTTGCGATTCTAAAATGTAATAATAAGACGTTCAATGGAACAGGACCATGTACAAATGTCAGCACAG TACAATGTACACATGGAATTAGGCCAGTAGTATCAACTCAACTGCTGTTAAATGGCAGTCTAGCAGAAGAAGAGGTAGTA ATTAGATCTGTCAATTTCACGGACAATGCTAAAACCATAATAGTACAGCTGAACACATCTGTAGAAATTAATTGTACAAG ACCCAACAACAATACAAGAAAAAGAATCCGTATCCAGAGAGGACCAGGGAGAGCATTTGTTACAATAGGAAAAATAGGAA ATATGAGACAAGCACATTGTAACATTAGTAGAGCAAAATGGAATAACACTTTAAAACAGATAGCTAGCAAATTAAGAGAA CAATTTGGAAATAATAAAACAATAATCTTTAAGCAATCCTCAGGAGGGGACCCAGAAATTGTAACGCACAGTTTTAATTG TGGAGGGGAATTTTTCTACTGTAATTCAACACAACTGTTTAATAGTACTTGGTTTAATAGTACTTGGAGTACTGAAGGGT CAAATAACACTGAAGGAAGTGACACAATCACCCTCCCATGCAGAATAAAACAAATTATAAACATGTGGCAGAAAGTAGGA AAAGCAATGTATGCCCCTCCCATCAGTGGACAAATTAGATGTTCATCAAATATTACAGGGCTGCTATTAACAAGAGATGG TGGTAATAGCAACAATGAGTCCGAGATCTTCAGACCTGGAGGAGGAGATATGAGGGACAATTGGAGAAGTGAATTATATA AATATAAAGTAGTAAAAATTGAACCATTAGGAGTAGCACCCACCAAGGCAAAGAGAAGAGTGGTGCAGAGAGAAAAAAGA `HIV_B.K03455.HXB2.gtf` HIV_B.K03455.HXB2 LANL amplicon 2252 3869 . + 2 name PRRT ; primary_cds 2252-2549 ; alt_cds 2550-3869 ; HIV_B.K03455.HXB2 LANL amplicon 4230 5093 . + 0 name INT ; primary_cds 2085-5096 ; alt_cds 5098-5619 ; HIV_B.K03455.HXB2 LANL amplicon 6225 7757 . + 1 name gp120 ; primary_cds 6225-8795 ; alt_cds 5831-6223 ;","title":"Reference Files"},{"location":"phylo/","text":"hp_phylo includes phylogenomics stages. Phylo Quick-Start HAPHPIPE includes three stages for phylogenomics: multiple_align , model_test , and build_tree . These three stages are sufficient to turn your consensus and/or haplotype sequences from the other stages into a phylogenetic tree! For purposes of this quick-start guide, we will demonstrate the stages to create a tree from HIV pol consensus sequences. Step 1: Alignment After running either of the assembly pipelines, final.fna files will be located in directories named ./ SampleID /haphpipe_assemble_0[1|2] . For the multiple_align stage, we need to create a list of all of these directories. We can do so easily with one command (shown for haphpipe_assemble_01 output: ls -d ./SRR*/haphpipe_assemble_01 ./dir_list.txt Now, we will align all of these final.fna files: haphpipe multiple_align --dir_list dir_list.txt --ref_gtf refs/HIV_B.K03455.HXB2.gtf The output will be located in a new directory, hp_multiple_align . The alignment of pol sequences is the file alignment_region00.fasta . Step 2: Model Selection Now, we will use the model_test stage to determine the best-fit evolutionary model for our data. This is an input to the tree building stage. We will use this command to generate best-fit models available in RAxML: haphpipe model_test --seqs hp_multiple_align/alignment_region00.fasta --run_id alignment_region00 --template raxml The ModelTest output will be written to a file called modeltest_results.out and a summary of all the best models will be written to modeltest_results_summary.tsv . Examples of both are below. ModelTest-NG Output ``` -------------------------------------------------------------------------------- ModelTest-NG vx.y.z Input data: MSA: multiple_align/alignment.fasta Tree: Maximum likelihood file: - #taxa: 6 #sites: 1975 #patterns: 114 Max. thread mem: 0 MB Output: Log: /var/folders/lv/dqdkd8957_3fv6yxsyfsvn0r0000gn/T/tmpHP_model_testud4sc4ya/samp12_modeltest_results.log Starting tree: /var/folders/lv/dqdkd8957_3fv6yxsyfsvn0r0000gn/T/tmpHP_model_testud4sc4ya/samp12_modeltest_results.tree Results: /var/folders/lv/dqdkd8957_3fv6yxsyfsvn0r0000gn/T/tmpHP_model_testud4sc4ya/samp12_modeltest_results.out Selection options: # dna schemes: 11 # dna models: 88 include model parameters: Uniform: true p-inv (+I): true gamma (+G): true both (+I+G): true free rates (+R): false fixed freqs: true estimated freqs: true #categories: 4 gamma rates mode: mean asc bias: none epsilon (opt): 0.01 epsilon (par): 0.05 keep branches: false Additional options: verbosity: very low threads: 1/6 RNG seed: 12345 subtree repeats: enabled -------------------------------------------------------------------------------- BIC model K lnL score delta weight -------------------------------------------------------------------------------- 1 HKY 4 -5380.7535 10860.1551 0.0000 0.7049 2 TrN 5 -5378.2013 10862.6391 2.4839 0.2036 3 TPM1uf 5 -5379.8699 10865.9763 5.8211 0.0384 4 TPM3uf 5 -5380.7175 10867.6716 7.5164 0.0164 5 HKY+G4 5 -5380.8440 10867.9246 7.7694 0.0145 6 HKY+I 5 -5381.4917 10869.2200 9.0648 0.0076 7 TIM3 6 -5378.1637 10870.1523 9.9971 0.0048 8 TrN+G4 6 -5378.3069 10870.4387 10.2836 0.0041 9 TPM2uf+G4 6 -5378.9908 10871.8064 11.6512 0.0021 10 TrN+I 6 -5379.0181 10871.8610 11.7059 0.0020 -------------------------------------------------------------------------------- Best model according to BIC --------------------------- Model: HKY lnL: -5380.7535 Frequencies: 0.3695 0.1709 0.2219 0.2377 Subst. Rates: 1.0000 2.4741 1.0000 1.0000 2.4741 1.0000 Inv. sites prop: - Gamma shape: - Score: 10860.1551 Weight: 0.7049 --------------------------- Parameter importances --------------------------- P.Inv: 0.0100 Gamma: 0.0216 Gamma-Inv: 0.0002 Frequencies: 1.0000 --------------------------- Model averaged estimates --------------------------- P.Inv: 0.0215 Alpha: 94.2337 Alpha-P.Inv: 91.7960 P.Inv-Alpha: 0.0214 Frequencies: 0.3700 0.1702 0.2226 0.2372 Commands: > phyml -i multiple_align/alignment.fasta -m 010010 -f m -v 0 -a 0 -c 1 -o tlr > raxmlHPC-SSE3 -s multiple_align/alignment.fasta -c 1 -m GTRCATX -n EXEC_NAME -p PARSIMONY_SEED > raxml-ng --msa multiple_align/alignment.fasta --model HKY > paup -s multiple_align/alignment.fasta > iqtree -s multiple_align/alignment.fasta -m HKY AIC model K lnL score delta weight -------------------------------------------------------------------------------- 1 TrN 5 -5378.2013 10784.4026 0.0000 0.2246 2 TIM2+G4 7 -5376.4972 10784.9944 0.5919 0.1671 3 TIM3 6 -5378.1637 10786.3274 1.9249 0.0858 4 TIM2+I 7 -5377.2570 10786.5141 2.1115 0.0782 5 TrN+G4 6 -5378.3069 10786.6138 2.2113 0.0744 6 TIM1+G4 7 -5377.3549 10786.7098 2.3073 0.0709 7 HKY 4 -5380.7535 10787.5069 3.1044 0.0476 8 TPM1uf 5 -5379.8699 10787.7398 3.3372 0.0423 9 TPM2uf+G4 6 -5378.9908 10787.9815 3.5790 0.0375 10 TrN+I 6 -5379.0181 10788.0362 3.6336 0.0365 -------------------------------------------------------------------------------- Best model according to AIC --------------------------- Model: TrN lnL: -5378.2013 Frequencies: 0.3720 0.1679 0.2249 0.2352 Subst. Rates: 1.0000 2.2008 1.0000 1.0000 3.1065 1.0000 Inv. sites prop: - Gamma shape: - Score: 10784.4026 Weight: 0.2246 --------------------------- Parameter importances --------------------------- P.Inv: 0.1262 Gamma: 0.3943 Gamma-Inv: 0.0384 Frequencies: 1.0000 --------------------------- Model averaged estimates --------------------------- P.Inv: 0.0216 Alpha: 93.2595 Alpha-P.Inv: 94.4193 P.Inv-Alpha: 0.0216 Frequencies: 0.3718 0.1684 0.2238 0.2359 Commands: > phyml -i multiple_align/alignment.fasta -m 010020 -f m -v 0 -a 0 -c 1 -o tlr > raxmlHPC-SSE3 -s multiple_align/alignment.fasta -c 1 -m GTRCATX -n EXEC_NAME -p PARSIMONY_SEED > raxml-ng --msa multiple_align/alignment.fasta --model TrN > paup -s multiple_align/alignment.fasta > iqtree -s multiple_align/alignment.fasta -m TrN AICc model K lnL score delta weight -------------------------------------------------------------------------------- 1 TrN 5 -5378.2013 10784.4026 0.0000 0.2246 2 TIM2+G4 7 -5376.4972 10784.9944 0.5919 0.1671 3 TIM3 6 -5378.1637 10786.3274 1.9249 0.0858 4 TIM2+I 7 -5377.2570 10786.5141 2.1115 0.0782 5 TrN+G4 6 -5378.3069 10786.6138 2.2113 0.0744 6 TIM1+G4 7 -5377.3549 10786.7098 2.3073 0.0709 7 HKY 4 -5380.7535 10787.5069 3.1044 0.0476 8 TPM1uf 5 -5379.8699 10787.7398 3.3372 0.0423 9 TPM2uf+G4 6 -5378.9908 10787.9815 3.5790 0.0375 10 TrN+I 6 -5379.0181 10788.0362 3.6336 0.0365 -------------------------------------------------------------------------------- Best model according to AICc --------------------------- Model: TrN lnL: -5378.2013 Frequencies: 0.3720 0.1679 0.2249 0.2352 Subst. Rates: 1.0000 2.2008 1.0000 1.0000 3.1065 1.0000 Inv. sites prop: - Gamma shape: - Score: 10784.4026 Weight: 0.2246 --------------------------- Parameter importances --------------------------- P.Inv: 0.1262 Gamma: 0.3943 Gamma-Inv: 0.0384 Frequencies: 1.0000 --------------------------- Model averaged estimates --------------------------- P.Inv: 0.0216 Alpha: 93.2595 Alpha-P.Inv: 94.4193 P.Inv-Alpha: 0.0216 Frequencies: 0.3718 0.1684 0.2238 0.2359 Commands: > phyml -i multiple_align/alignment.fasta -m 010020 -f m -v 0 -a 0 -c 1 -o tlr > raxmlHPC-SSE3 -s multiple_align/alignment.fasta -c 1 -m GTRCATX -n EXEC_NAME -p PARSIMONY_SEED > raxml-ng --msa multiple_align/alignment.fasta --model TrN > paup -s multiple_align/alignment.fasta > iqtree -s multiple_align/alignment.fasta -m TrN Done ``` ModelTest-NG Output File Criteria Best Model multiple_align/alignment.fasta BIC HKY multiple_align/alignment.fasta AIC TrN multiple_align/alignment.fasta AICc TrN Step 3: Build a Tree Now, we will use build_tree to build our tree! You should use the best model outputted in model_test for the --model argument (here we are using GTRGAMMAX). The --run_full_analysis option will automatically run a full maximum likelihood bootstrapping analysis for us: haphpipe build_tree --seqs hp_multiple_align/alignment_region00.fasta --run_full_analysis --model GTRGAMMAX The output will be written to a new directory, hp_build_tree . The best tree file from RAxML will be outputted as RAxML_bipartitionsBranchLabels.build_tree.tre . This tree can then be annotated in programs such as FigTree or iTOL . Phylogenomics Pipelines For users who would like to build a full pipeline to run assembly and phylogenetics stages in one go, we recommend adapting the demo pipeline ( haphpipe_demo ) for this purpose. See the demo page for more details. multiple_align Align consensus sequences using MAFFT ( documentation ). Input can be a list of directories which contain final.fna and/or ph_haplotypes.fna files or a fasta file, or both (in which case the sequences in the FASTA file are combined with the final.fna and/or ph_haplotypes.fna files retreived before the alignment. Sequences will be separated by amplicons using a supplied GTF file before alignment (unless the --alignall option is specified). This module may also be used to separate files by amplicons (without aligning) by specifying the --fastaonly option. Alignments are by default outputted as FASTA files, although PHYLIP ( --phylipout ) or CLUSTAL ( --clustalout ) output options are also available. Many options from MAFFT are available in this module. Please refer to the MAFFT documentation above for information about these options. Usage: haphpipe multiple_align [MAFFT OPTIONS] [HAPHPIPE OPTIONS] --seqs FASTA --dir_list TXT --ref_gtf GTF [--outdir] (or): hp_multiple_align [MAFFT OPTIONS] [HAPHPIPE OPTIONS] --seqs FASTA --dir_list TXT --ref_gtf GTF [--outdir] Output files: alignment files in FASTA format (default), one per amplicon (or one alignment.fasta file if using --alignall option) Note: MAFFT stores intermediate files in a temporary directory located in /tmp. More information is available here . Input/Output Arguments: Option Description --seqs SEQS FASTA file with sequences to be aligned --dir_list DIR_LIST List of directories which include either a final.fna or ph_haplotypes.fna file, one on each line --ref_gtf REF_GTF Reference GTF file --out_align OUT_ALIGN Name for alignment file --nuc Assume nucleotide (default: False) --amino Assume amino (default: False) --clustalout Clustal output format (default: False) --phylipout PHYLIP output format (default: False) --inputorder Output order same as input (default: False) --reorder Output order aligned (default: False) --treeout Guide tree is output to the input.tree file (default:False) --quiet_mafft Do not report progress (default: False) --outdir OUTDIR Output directory MAFFT Options: Option Description --algo ALGO Use different algorithm in command: linsi, ginsi, einsi, fftnsi, fftns, nwns, nwnsi --auto Automatically select algorithm (default: False) --sixmerpair Calculate distance based on shared 6mers, on by default (default: False) --globalpair Use Needleman-Wunsch algorithm (default: False) --localpair Use Smith-Waterman algorithm (default: False) --genafpair Use local algorithm with generalized affine gap cost (default: False) --fastapair Use FASTA for pairwise alignment (default: False) --weighti WEIGHTI Weighting factor for consistency term --retree RETREE Number of times to build guide tree --maxiterate MAXITERATE Number of cycles for iterative refinement --noscore Do not check alignment score in iterative alignment (default: False) --memsave Use Myers-Miller algorithm (default: False) --parttree Use fast tree-building method with 6mer distance (default: False) --dpparttree Use PartTree algorithm with distances based on DP (default: False) --fastaparttree Use PartTree algorithm with distances based on FASTA (default: False) --partsize PARTSIZE Number of partitions for PartTree --groupsize GROUPSIZE Max number of sequences for PartTree MAFFT Parameters: Option Description --lop LOP Gap opening penalty --lep LEP Offset value --lexp LEXP Gap extension penalty --LOP LOP Gap opening penalty to skip alignment --LEXP LEXP Gap extension penalty to skip alignment --bl BL BLOSUM matrix: 30, 45, 62, or 80 --jtt JTT JTT PAM number 0 --tm TM Transmembrane PAM number 0 --aamatrix AAMATRIX Path to user-defined AA scoring matrix --fmodel Incorporate AA/nuc composition info into scoring matrix (default: False) Options: Option Description --ncpu NCPU Number of CPU to use (default: 1) --quiet Do not write output to console (silence stdout and stderr) (default: False) --logfile LOGFILE Name for log file (output) --debug Print commands but do not run (default: False) --fastaonly Output fasta files separated by region but do not align (default: False) --alignall Do not separate files by region, align entire file (default: False) Example usage: haphpipe multiple_align --dir_list demo_sra_list.txt --ref_gtf HIV_B.K03455.HXB2.gtf --phylipout --logfile demo_multiple_align.log model_test Select the best-fit model of evolution from an alignment file using ModelTest-NG ( documentation ). Input is an alignment in FASTA or PHYLIP format. Output is ModelTest-NG results (text file) containing information for the best performing models. Usage: haphpipe model_test [ModelTest-NG OPTIONS] [HAPHPIPE OPTIONS] --seqs FASTA [--outdir] (or): hp_model_test [ModelTest-NG OPTIONS] [HAPHPIPE OPTIONS] --seqs FASTA [--outdir] Output files: ModelTest-NG output file ( modeltest_results.out ). Input/Output Arguments: Option Description --seqs SEQS Alignment in FASTA or PHYLIP format --outname Name for output file --outdir OUTDIR Output directory ModelTest-NG Options: Option Description --data_type Data type: nt or aa (default: nt) --partitions Partitions file --seed Seed for random number generator --topology TOPOLOGY Starting topology: ml, mp, fixed-ml-jc, fixed-ml-gtr, fixed-mp, random, or user (default: ml) --utree User-defined starting tree --force Force output overriding (default: False) --asc_bias ASC_BIAS Ascertainment bias correction: lewis, felsenstein, or stamatakis --frequencies FREQUENCIES Candidate model frequencies: e (estimated) or f (fixed) --het HET Set rate heterogeneity: u (uniform), i (invariant sites +I), g (gamma +G), or f (bothinvariant sites and gamma +I+G) --models MODELS Text file with candidate models, one per line --schemes SCHEMES Number of predefined DNA substitution schemes evaluated: 3, 5, 7, 11, or 203 --template TEMPLATE Set candidate models according to a specified tool: raxml, phyml, mrbayes, or paup Options: Option Description --ncpu NCPU Number of CPU to use (default: 1) --quiet Do not write output to console (silence stdout and stderr) (default: False) --logfile LOGFILE Name for log file (output) --debug Print commands but do not run (default: False) --keep_tmp Keep temporary directory (default: False) Example usage: haphpipe model_test --seqs multiple_align/alignment.fasta build_tree Phylogeny reconstruction with RAxML ( documentation ). Input is an alignment (FASTA or PHYLIP format). Output is a tree file (TRE format). Please see the RAxML documentation for a full description of RAxML options. For convenience, we have included an option --run_full_analysis which will automatically find the best maximum likelihood tree, complete bootstrapping, and then merge output together for a final tree. Usage: haphpipe build_tree [RAxML OPTIONS] [HAPHPIPE OPTIONS] --seqs FASTA --output_name TXT [--outdir] (or): hp build_tree [RAxML OPTIONS] [HAPHPIPE OPTIONS] --seqs FASTA --output_name TXT [--outdir] Output files: File Description RaxML_info.build_tree.tre This file is written regardless of the command line option. It contains information about the model and algorithm used. If --InitialRearrangement is called, it will indicate the rearrangement setting used RAxML_log.build_tree.tre This file prints out the time, likelihood value of the current tree, and number of the checkpoint file (if called) after each iteration of the search algorithm. Not generated in the case of multiple bootstraps. RAxML_result.build_tree.tre Unless multiple bootstraps are executes, this file is written after each iteration of the search algorithm. It contians the final tree topology of the current run RAxML_parsimonyTree.build_tree.tre If a starting tree is not specified by --UserStartingTree, this file will contain the randomized parsimony starting tree RAxML_randomTree.build_tree.tre If --rand_starting_tree if called, this file will contain the completely random starting tree RAxML_checkpoint.build_tree.tre.checkpointNumber Generated if --print_intermediate_trees is called RAxML_bootstrap.build_tree.tre Consolidates final bootstrapped trees if called with --NumberofRuns RAxML_bipartitions.build_tree.tre Contain the input tree with confidence values at nodes if --algo_option is called RAxML_bipartitionsBranchLabels.build_tree.tre Support values are displayed as Newick branch labels rather than node labels AxML_bipartitionFrequencies.build_tree.tre If --algo_optio m is called, this file contains the pair-wise bipartition frequencies of all trees RAxML_perSiteLLs.build_tree.tre This file contains contains the per\u2013site log likelihood scores. Only generated if --algo_option g is called RAxML_bestTree.build_tree.tre Outputs the best-scoring ML tree RAxML_distances.build_tree.tre Contains the pair-wise ML-based distances between taxonpairs. This file is only generated when --algo_option x option is called. Input/Output Arguments: Option RAxML Equivalent Description --seqs SEQS -s Input alignment in PHYLIP or FASTA format --output_name NAME -n Run name for trees (default: build_tree.tre) --model MODEL -m Substitution Model (default: GTRGAMMAIX) --outdir OUTDIR -w Output directory (default: .) RAxML Options: Option RAxML Equivalent Description --run_full_analysis Run bootstrap search and find best ML tree --outgroup -o outgrpup for tree --parsimony_seed -p Parsimony Random Seed --wgtFile -a Column weight file name to assign individual weights to each column of the alignment --secsub -A Specify secondary structure substitution models, must also include file defining the secondary structure --bootstrap -b bootstrapRandomNumberSeed for non-parametric bootstrapping --bootstrap_threshold -B Threshold for bootstopping criteria --numCat -c Number of distinct rate categories for RAxML when model of rate heterogeneity is set to CAT --rand_starting_tree -d ML optimization from random starting tree --convergence_criterion -D ML search convergence criterion --likelihoodEpsilon -e Set model optimization precision in log likelihood units for final optimization of tree topology --excludeFileName -E File contains specifications of alignment positions to be excluded --algo_option -f Select what kind of algorithm RAxML shall execute --cat_model -F Enable ML tree searches under CAT model for very large trees --groupingFile -g File name of a multifurcating constraint tree --placementThreshold -G Threshold value for ML\u00adbased evolutionary placement algorithm heuristics --disable_pattern_compression -H Disable pattern compression --InitialRearrangement -i Radius for pruned sub-tree re-insertion --posteriori -I posteriori bootstopping analysis --print_intermediate_trees -j Print out a couple of intermediate trees --majorityrule -J Compute majority rule consensus tree --print_branch_length -k Bootstrapped trees should be printed with branch lengths --ICTCmetrics -L Compute the TC and IC metrics on a consensus tree --partition_branch_length -M Switch on estimation of individual per\u00adpartition branch lengths --disable_check -O Disable check for completely undetermined sequence in alignment --AAmodel -P Specify the file name of a user\u00addefined AA (Protein) substitution model --multiplemodelFile -q Specify the file name which contains the assignment of models to alignment partitions for multiple models of substitution --binarytree -r Specify the file name of a binary constraint tree --BinaryParameterFile -R Specify the file name of a binary model parameter file that has previously been generated with RAxML using the \u00adf e tree evaluation option. --SecondaryStructure -S Specify the name of a secondary structure file --UserStartingTree -t Specifies a user starting tree file name which must be in Newick format --median_GAMMA -u Use the median for the discrete approximation of the GAMMA model of rateheterogeneity --version_info -v Display version information --rate_heterogeneity -V Disable rate heterogeneity among site model and use one without rate heterogeneity instead --directory -w Full directory of output file --window -W Sliding window size for leave\u00adone\u00adout site\u00adspecific placement bias algorithm --RapidBootstrapNumSeed -x Specify an integer number (random seed) and turn on rapid bootstrapping --random_addition -X RAxML will only do a randomized stepwise addition order parsimony tree reconstruction without performing any additional SPRs --starting_tree -y Only for computing parsimony --quartetGroupingFileName -Y Pass a quartet grouping file name defining four groups from which to draw quartets --multipleTreeFile -z Specify the file name of a file containing multiple trees e.g. from a bootstrap that shall be used to draw bipartition values onto a tree provided with \u00adt. --NumberofRuns -N Specify the number of alternative runs on distinct starting trees --mesquite --mesquite Print output files that can be parsed by Mesquite --silent --silent Disables printout of warnings related to identical sequences and entirely undetermined sites in the alignment --noseqcheck --no-seq-check Disables checking the input MSA for identical sequences and entirely undetermined sites --nobfgs --no-bfgs Disables automatic usage of BFGS method to optimize GTR rates on unpartitioned DNA datasets --epaPlaceNum \u00ad\u00adepa\u00adkeep\u00adplacements= Specify the number of potential placements you want to keep for each read in the EPA algorithm --epaProbThreshold \u00ad\u00adepa\u00adprob\u00adthreshold= Specify a percent threshold for including potential placements of a read depending on the maximum placement weight for this read --epaLikelihood \u00ad\u00adepa\u00adaccumulated\u00adthreshold= Specify an accumulated likelihood weight threshold --HKY85 --HKY85 Specify that all DNA partitions will evolve under the HKY85 model --BootstrapPerm \u00ad\u00adbootstop\u00adperms= Specify the number of permutations to be conducted for the bootstopping/bootstrap convergence test; minimum 100 --option_help -h Display Help Options: Option Description --keep_tmp Keep temporary directory --quiet Do not write output to console (silence stdout and stderr) (default: False) --logfile LOGFILE Name for log file (output) --debug Print commands but do not run (default: False) Example usage: haphpipe build_tree --seqs multiple_align/alignment.fasta --run_full_analysis","title":"Phylo"},{"location":"phylo/#phylo-quick-start","text":"HAPHPIPE includes three stages for phylogenomics: multiple_align , model_test , and build_tree . These three stages are sufficient to turn your consensus and/or haplotype sequences from the other stages into a phylogenetic tree! For purposes of this quick-start guide, we will demonstrate the stages to create a tree from HIV pol consensus sequences. Step 1: Alignment After running either of the assembly pipelines, final.fna files will be located in directories named ./ SampleID /haphpipe_assemble_0[1|2] . For the multiple_align stage, we need to create a list of all of these directories. We can do so easily with one command (shown for haphpipe_assemble_01 output: ls -d ./SRR*/haphpipe_assemble_01 ./dir_list.txt Now, we will align all of these final.fna files: haphpipe multiple_align --dir_list dir_list.txt --ref_gtf refs/HIV_B.K03455.HXB2.gtf The output will be located in a new directory, hp_multiple_align . The alignment of pol sequences is the file alignment_region00.fasta . Step 2: Model Selection Now, we will use the model_test stage to determine the best-fit evolutionary model for our data. This is an input to the tree building stage. We will use this command to generate best-fit models available in RAxML: haphpipe model_test --seqs hp_multiple_align/alignment_region00.fasta --run_id alignment_region00 --template raxml The ModelTest output will be written to a file called modeltest_results.out and a summary of all the best models will be written to modeltest_results_summary.tsv . Examples of both are below. ModelTest-NG Output ``` -------------------------------------------------------------------------------- ModelTest-NG vx.y.z Input data: MSA: multiple_align/alignment.fasta Tree: Maximum likelihood file: - #taxa: 6 #sites: 1975 #patterns: 114 Max. thread mem: 0 MB Output: Log: /var/folders/lv/dqdkd8957_3fv6yxsyfsvn0r0000gn/T/tmpHP_model_testud4sc4ya/samp12_modeltest_results.log Starting tree: /var/folders/lv/dqdkd8957_3fv6yxsyfsvn0r0000gn/T/tmpHP_model_testud4sc4ya/samp12_modeltest_results.tree Results: /var/folders/lv/dqdkd8957_3fv6yxsyfsvn0r0000gn/T/tmpHP_model_testud4sc4ya/samp12_modeltest_results.out Selection options: # dna schemes: 11 # dna models: 88 include model parameters: Uniform: true p-inv (+I): true gamma (+G): true both (+I+G): true free rates (+R): false fixed freqs: true estimated freqs: true #categories: 4 gamma rates mode: mean asc bias: none epsilon (opt): 0.01 epsilon (par): 0.05 keep branches: false Additional options: verbosity: very low threads: 1/6 RNG seed: 12345 subtree repeats: enabled -------------------------------------------------------------------------------- BIC model K lnL score delta weight -------------------------------------------------------------------------------- 1 HKY 4 -5380.7535 10860.1551 0.0000 0.7049 2 TrN 5 -5378.2013 10862.6391 2.4839 0.2036 3 TPM1uf 5 -5379.8699 10865.9763 5.8211 0.0384 4 TPM3uf 5 -5380.7175 10867.6716 7.5164 0.0164 5 HKY+G4 5 -5380.8440 10867.9246 7.7694 0.0145 6 HKY+I 5 -5381.4917 10869.2200 9.0648 0.0076 7 TIM3 6 -5378.1637 10870.1523 9.9971 0.0048 8 TrN+G4 6 -5378.3069 10870.4387 10.2836 0.0041 9 TPM2uf+G4 6 -5378.9908 10871.8064 11.6512 0.0021 10 TrN+I 6 -5379.0181 10871.8610 11.7059 0.0020 -------------------------------------------------------------------------------- Best model according to BIC --------------------------- Model: HKY lnL: -5380.7535 Frequencies: 0.3695 0.1709 0.2219 0.2377 Subst. Rates: 1.0000 2.4741 1.0000 1.0000 2.4741 1.0000 Inv. sites prop: - Gamma shape: - Score: 10860.1551 Weight: 0.7049 --------------------------- Parameter importances --------------------------- P.Inv: 0.0100 Gamma: 0.0216 Gamma-Inv: 0.0002 Frequencies: 1.0000 --------------------------- Model averaged estimates --------------------------- P.Inv: 0.0215 Alpha: 94.2337 Alpha-P.Inv: 91.7960 P.Inv-Alpha: 0.0214 Frequencies: 0.3700 0.1702 0.2226 0.2372 Commands: > phyml -i multiple_align/alignment.fasta -m 010010 -f m -v 0 -a 0 -c 1 -o tlr > raxmlHPC-SSE3 -s multiple_align/alignment.fasta -c 1 -m GTRCATX -n EXEC_NAME -p PARSIMONY_SEED > raxml-ng --msa multiple_align/alignment.fasta --model HKY > paup -s multiple_align/alignment.fasta > iqtree -s multiple_align/alignment.fasta -m HKY AIC model K lnL score delta weight -------------------------------------------------------------------------------- 1 TrN 5 -5378.2013 10784.4026 0.0000 0.2246 2 TIM2+G4 7 -5376.4972 10784.9944 0.5919 0.1671 3 TIM3 6 -5378.1637 10786.3274 1.9249 0.0858 4 TIM2+I 7 -5377.2570 10786.5141 2.1115 0.0782 5 TrN+G4 6 -5378.3069 10786.6138 2.2113 0.0744 6 TIM1+G4 7 -5377.3549 10786.7098 2.3073 0.0709 7 HKY 4 -5380.7535 10787.5069 3.1044 0.0476 8 TPM1uf 5 -5379.8699 10787.7398 3.3372 0.0423 9 TPM2uf+G4 6 -5378.9908 10787.9815 3.5790 0.0375 10 TrN+I 6 -5379.0181 10788.0362 3.6336 0.0365 -------------------------------------------------------------------------------- Best model according to AIC --------------------------- Model: TrN lnL: -5378.2013 Frequencies: 0.3720 0.1679 0.2249 0.2352 Subst. Rates: 1.0000 2.2008 1.0000 1.0000 3.1065 1.0000 Inv. sites prop: - Gamma shape: - Score: 10784.4026 Weight: 0.2246 --------------------------- Parameter importances --------------------------- P.Inv: 0.1262 Gamma: 0.3943 Gamma-Inv: 0.0384 Frequencies: 1.0000 --------------------------- Model averaged estimates --------------------------- P.Inv: 0.0216 Alpha: 93.2595 Alpha-P.Inv: 94.4193 P.Inv-Alpha: 0.0216 Frequencies: 0.3718 0.1684 0.2238 0.2359 Commands: > phyml -i multiple_align/alignment.fasta -m 010020 -f m -v 0 -a 0 -c 1 -o tlr > raxmlHPC-SSE3 -s multiple_align/alignment.fasta -c 1 -m GTRCATX -n EXEC_NAME -p PARSIMONY_SEED > raxml-ng --msa multiple_align/alignment.fasta --model TrN > paup -s multiple_align/alignment.fasta > iqtree -s multiple_align/alignment.fasta -m TrN AICc model K lnL score delta weight -------------------------------------------------------------------------------- 1 TrN 5 -5378.2013 10784.4026 0.0000 0.2246 2 TIM2+G4 7 -5376.4972 10784.9944 0.5919 0.1671 3 TIM3 6 -5378.1637 10786.3274 1.9249 0.0858 4 TIM2+I 7 -5377.2570 10786.5141 2.1115 0.0782 5 TrN+G4 6 -5378.3069 10786.6138 2.2113 0.0744 6 TIM1+G4 7 -5377.3549 10786.7098 2.3073 0.0709 7 HKY 4 -5380.7535 10787.5069 3.1044 0.0476 8 TPM1uf 5 -5379.8699 10787.7398 3.3372 0.0423 9 TPM2uf+G4 6 -5378.9908 10787.9815 3.5790 0.0375 10 TrN+I 6 -5379.0181 10788.0362 3.6336 0.0365 -------------------------------------------------------------------------------- Best model according to AICc --------------------------- Model: TrN lnL: -5378.2013 Frequencies: 0.3720 0.1679 0.2249 0.2352 Subst. Rates: 1.0000 2.2008 1.0000 1.0000 3.1065 1.0000 Inv. sites prop: - Gamma shape: - Score: 10784.4026 Weight: 0.2246 --------------------------- Parameter importances --------------------------- P.Inv: 0.1262 Gamma: 0.3943 Gamma-Inv: 0.0384 Frequencies: 1.0000 --------------------------- Model averaged estimates --------------------------- P.Inv: 0.0216 Alpha: 93.2595 Alpha-P.Inv: 94.4193 P.Inv-Alpha: 0.0216 Frequencies: 0.3718 0.1684 0.2238 0.2359 Commands: > phyml -i multiple_align/alignment.fasta -m 010020 -f m -v 0 -a 0 -c 1 -o tlr > raxmlHPC-SSE3 -s multiple_align/alignment.fasta -c 1 -m GTRCATX -n EXEC_NAME -p PARSIMONY_SEED > raxml-ng --msa multiple_align/alignment.fasta --model TrN > paup -s multiple_align/alignment.fasta > iqtree -s multiple_align/alignment.fasta -m TrN Done ``` ModelTest-NG Output File Criteria Best Model multiple_align/alignment.fasta BIC HKY multiple_align/alignment.fasta AIC TrN multiple_align/alignment.fasta AICc TrN Step 3: Build a Tree Now, we will use build_tree to build our tree! You should use the best model outputted in model_test for the --model argument (here we are using GTRGAMMAX). The --run_full_analysis option will automatically run a full maximum likelihood bootstrapping analysis for us: haphpipe build_tree --seqs hp_multiple_align/alignment_region00.fasta --run_full_analysis --model GTRGAMMAX The output will be written to a new directory, hp_build_tree . The best tree file from RAxML will be outputted as RAxML_bipartitionsBranchLabels.build_tree.tre . This tree can then be annotated in programs such as FigTree or iTOL . Phylogenomics Pipelines For users who would like to build a full pipeline to run assembly and phylogenetics stages in one go, we recommend adapting the demo pipeline ( haphpipe_demo ) for this purpose. See the demo page for more details.","title":"Phylo Quick-Start"},{"location":"phylo/#multiple_align","text":"Align consensus sequences using MAFFT ( documentation ). Input can be a list of directories which contain final.fna and/or ph_haplotypes.fna files or a fasta file, or both (in which case the sequences in the FASTA file are combined with the final.fna and/or ph_haplotypes.fna files retreived before the alignment. Sequences will be separated by amplicons using a supplied GTF file before alignment (unless the --alignall option is specified). This module may also be used to separate files by amplicons (without aligning) by specifying the --fastaonly option. Alignments are by default outputted as FASTA files, although PHYLIP ( --phylipout ) or CLUSTAL ( --clustalout ) output options are also available. Many options from MAFFT are available in this module. Please refer to the MAFFT documentation above for information about these options. Usage: haphpipe multiple_align [MAFFT OPTIONS] [HAPHPIPE OPTIONS] --seqs FASTA --dir_list TXT --ref_gtf GTF [--outdir] (or): hp_multiple_align [MAFFT OPTIONS] [HAPHPIPE OPTIONS] --seqs FASTA --dir_list TXT --ref_gtf GTF [--outdir] Output files: alignment files in FASTA format (default), one per amplicon (or one alignment.fasta file if using --alignall option) Note: MAFFT stores intermediate files in a temporary directory located in /tmp. More information is available here . Input/Output Arguments: Option Description --seqs SEQS FASTA file with sequences to be aligned --dir_list DIR_LIST List of directories which include either a final.fna or ph_haplotypes.fna file, one on each line --ref_gtf REF_GTF Reference GTF file --out_align OUT_ALIGN Name for alignment file --nuc Assume nucleotide (default: False) --amino Assume amino (default: False) --clustalout Clustal output format (default: False) --phylipout PHYLIP output format (default: False) --inputorder Output order same as input (default: False) --reorder Output order aligned (default: False) --treeout Guide tree is output to the input.tree file (default:False) --quiet_mafft Do not report progress (default: False) --outdir OUTDIR Output directory MAFFT Options: Option Description --algo ALGO Use different algorithm in command: linsi, ginsi, einsi, fftnsi, fftns, nwns, nwnsi --auto Automatically select algorithm (default: False) --sixmerpair Calculate distance based on shared 6mers, on by default (default: False) --globalpair Use Needleman-Wunsch algorithm (default: False) --localpair Use Smith-Waterman algorithm (default: False) --genafpair Use local algorithm with generalized affine gap cost (default: False) --fastapair Use FASTA for pairwise alignment (default: False) --weighti WEIGHTI Weighting factor for consistency term --retree RETREE Number of times to build guide tree --maxiterate MAXITERATE Number of cycles for iterative refinement --noscore Do not check alignment score in iterative alignment (default: False) --memsave Use Myers-Miller algorithm (default: False) --parttree Use fast tree-building method with 6mer distance (default: False) --dpparttree Use PartTree algorithm with distances based on DP (default: False) --fastaparttree Use PartTree algorithm with distances based on FASTA (default: False) --partsize PARTSIZE Number of partitions for PartTree --groupsize GROUPSIZE Max number of sequences for PartTree MAFFT Parameters: Option Description --lop LOP Gap opening penalty --lep LEP Offset value --lexp LEXP Gap extension penalty --LOP LOP Gap opening penalty to skip alignment --LEXP LEXP Gap extension penalty to skip alignment --bl BL BLOSUM matrix: 30, 45, 62, or 80 --jtt JTT JTT PAM number 0 --tm TM Transmembrane PAM number 0 --aamatrix AAMATRIX Path to user-defined AA scoring matrix --fmodel Incorporate AA/nuc composition info into scoring matrix (default: False) Options: Option Description --ncpu NCPU Number of CPU to use (default: 1) --quiet Do not write output to console (silence stdout and stderr) (default: False) --logfile LOGFILE Name for log file (output) --debug Print commands but do not run (default: False) --fastaonly Output fasta files separated by region but do not align (default: False) --alignall Do not separate files by region, align entire file (default: False) Example usage: haphpipe multiple_align --dir_list demo_sra_list.txt --ref_gtf HIV_B.K03455.HXB2.gtf --phylipout --logfile demo_multiple_align.log","title":"multiple_align"},{"location":"phylo/#model_test","text":"Select the best-fit model of evolution from an alignment file using ModelTest-NG ( documentation ). Input is an alignment in FASTA or PHYLIP format. Output is ModelTest-NG results (text file) containing information for the best performing models. Usage: haphpipe model_test [ModelTest-NG OPTIONS] [HAPHPIPE OPTIONS] --seqs FASTA [--outdir] (or): hp_model_test [ModelTest-NG OPTIONS] [HAPHPIPE OPTIONS] --seqs FASTA [--outdir] Output files: ModelTest-NG output file ( modeltest_results.out ). Input/Output Arguments: Option Description --seqs SEQS Alignment in FASTA or PHYLIP format --outname Name for output file --outdir OUTDIR Output directory ModelTest-NG Options: Option Description --data_type Data type: nt or aa (default: nt) --partitions Partitions file --seed Seed for random number generator --topology TOPOLOGY Starting topology: ml, mp, fixed-ml-jc, fixed-ml-gtr, fixed-mp, random, or user (default: ml) --utree User-defined starting tree --force Force output overriding (default: False) --asc_bias ASC_BIAS Ascertainment bias correction: lewis, felsenstein, or stamatakis --frequencies FREQUENCIES Candidate model frequencies: e (estimated) or f (fixed) --het HET Set rate heterogeneity: u (uniform), i (invariant sites +I), g (gamma +G), or f (bothinvariant sites and gamma +I+G) --models MODELS Text file with candidate models, one per line --schemes SCHEMES Number of predefined DNA substitution schemes evaluated: 3, 5, 7, 11, or 203 --template TEMPLATE Set candidate models according to a specified tool: raxml, phyml, mrbayes, or paup Options: Option Description --ncpu NCPU Number of CPU to use (default: 1) --quiet Do not write output to console (silence stdout and stderr) (default: False) --logfile LOGFILE Name for log file (output) --debug Print commands but do not run (default: False) --keep_tmp Keep temporary directory (default: False) Example usage: haphpipe model_test --seqs multiple_align/alignment.fasta","title":"model_test"},{"location":"phylo/#build_tree","text":"Phylogeny reconstruction with RAxML ( documentation ). Input is an alignment (FASTA or PHYLIP format). Output is a tree file (TRE format). Please see the RAxML documentation for a full description of RAxML options. For convenience, we have included an option --run_full_analysis which will automatically find the best maximum likelihood tree, complete bootstrapping, and then merge output together for a final tree. Usage: haphpipe build_tree [RAxML OPTIONS] [HAPHPIPE OPTIONS] --seqs FASTA --output_name TXT [--outdir] (or): hp build_tree [RAxML OPTIONS] [HAPHPIPE OPTIONS] --seqs FASTA --output_name TXT [--outdir] Output files: File Description RaxML_info.build_tree.tre This file is written regardless of the command line option. It contains information about the model and algorithm used. If --InitialRearrangement is called, it will indicate the rearrangement setting used RAxML_log.build_tree.tre This file prints out the time, likelihood value of the current tree, and number of the checkpoint file (if called) after each iteration of the search algorithm. Not generated in the case of multiple bootstraps. RAxML_result.build_tree.tre Unless multiple bootstraps are executes, this file is written after each iteration of the search algorithm. It contians the final tree topology of the current run RAxML_parsimonyTree.build_tree.tre If a starting tree is not specified by --UserStartingTree, this file will contain the randomized parsimony starting tree RAxML_randomTree.build_tree.tre If --rand_starting_tree if called, this file will contain the completely random starting tree RAxML_checkpoint.build_tree.tre.checkpointNumber Generated if --print_intermediate_trees is called RAxML_bootstrap.build_tree.tre Consolidates final bootstrapped trees if called with --NumberofRuns RAxML_bipartitions.build_tree.tre Contain the input tree with confidence values at nodes if --algo_option is called RAxML_bipartitionsBranchLabels.build_tree.tre Support values are displayed as Newick branch labels rather than node labels AxML_bipartitionFrequencies.build_tree.tre If --algo_optio m is called, this file contains the pair-wise bipartition frequencies of all trees RAxML_perSiteLLs.build_tree.tre This file contains contains the per\u2013site log likelihood scores. Only generated if --algo_option g is called RAxML_bestTree.build_tree.tre Outputs the best-scoring ML tree RAxML_distances.build_tree.tre Contains the pair-wise ML-based distances between taxonpairs. This file is only generated when --algo_option x option is called. Input/Output Arguments: Option RAxML Equivalent Description --seqs SEQS -s Input alignment in PHYLIP or FASTA format --output_name NAME -n Run name for trees (default: build_tree.tre) --model MODEL -m Substitution Model (default: GTRGAMMAIX) --outdir OUTDIR -w Output directory (default: .) RAxML Options: Option RAxML Equivalent Description --run_full_analysis Run bootstrap search and find best ML tree --outgroup -o outgrpup for tree --parsimony_seed -p Parsimony Random Seed --wgtFile -a Column weight file name to assign individual weights to each column of the alignment --secsub -A Specify secondary structure substitution models, must also include file defining the secondary structure --bootstrap -b bootstrapRandomNumberSeed for non-parametric bootstrapping --bootstrap_threshold -B Threshold for bootstopping criteria --numCat -c Number of distinct rate categories for RAxML when model of rate heterogeneity is set to CAT --rand_starting_tree -d ML optimization from random starting tree --convergence_criterion -D ML search convergence criterion --likelihoodEpsilon -e Set model optimization precision in log likelihood units for final optimization of tree topology --excludeFileName -E File contains specifications of alignment positions to be excluded --algo_option -f Select what kind of algorithm RAxML shall execute --cat_model -F Enable ML tree searches under CAT model for very large trees --groupingFile -g File name of a multifurcating constraint tree --placementThreshold -G Threshold value for ML\u00adbased evolutionary placement algorithm heuristics --disable_pattern_compression -H Disable pattern compression --InitialRearrangement -i Radius for pruned sub-tree re-insertion --posteriori -I posteriori bootstopping analysis --print_intermediate_trees -j Print out a couple of intermediate trees --majorityrule -J Compute majority rule consensus tree --print_branch_length -k Bootstrapped trees should be printed with branch lengths --ICTCmetrics -L Compute the TC and IC metrics on a consensus tree --partition_branch_length -M Switch on estimation of individual per\u00adpartition branch lengths --disable_check -O Disable check for completely undetermined sequence in alignment --AAmodel -P Specify the file name of a user\u00addefined AA (Protein) substitution model --multiplemodelFile -q Specify the file name which contains the assignment of models to alignment partitions for multiple models of substitution --binarytree -r Specify the file name of a binary constraint tree --BinaryParameterFile -R Specify the file name of a binary model parameter file that has previously been generated with RAxML using the \u00adf e tree evaluation option. --SecondaryStructure -S Specify the name of a secondary structure file --UserStartingTree -t Specifies a user starting tree file name which must be in Newick format --median_GAMMA -u Use the median for the discrete approximation of the GAMMA model of rateheterogeneity --version_info -v Display version information --rate_heterogeneity -V Disable rate heterogeneity among site model and use one without rate heterogeneity instead --directory -w Full directory of output file --window -W Sliding window size for leave\u00adone\u00adout site\u00adspecific placement bias algorithm --RapidBootstrapNumSeed -x Specify an integer number (random seed) and turn on rapid bootstrapping --random_addition -X RAxML will only do a randomized stepwise addition order parsimony tree reconstruction without performing any additional SPRs --starting_tree -y Only for computing parsimony --quartetGroupingFileName -Y Pass a quartet grouping file name defining four groups from which to draw quartets --multipleTreeFile -z Specify the file name of a file containing multiple trees e.g. from a bootstrap that shall be used to draw bipartition values onto a tree provided with \u00adt. --NumberofRuns -N Specify the number of alternative runs on distinct starting trees --mesquite --mesquite Print output files that can be parsed by Mesquite --silent --silent Disables printout of warnings related to identical sequences and entirely undetermined sites in the alignment --noseqcheck --no-seq-check Disables checking the input MSA for identical sequences and entirely undetermined sites --nobfgs --no-bfgs Disables automatic usage of BFGS method to optimize GTR rates on unpartitioned DNA datasets --epaPlaceNum \u00ad\u00adepa\u00adkeep\u00adplacements= Specify the number of potential placements you want to keep for each read in the EPA algorithm --epaProbThreshold \u00ad\u00adepa\u00adprob\u00adthreshold= Specify a percent threshold for including potential placements of a read depending on the maximum placement weight for this read --epaLikelihood \u00ad\u00adepa\u00adaccumulated\u00adthreshold= Specify an accumulated likelihood weight threshold --HKY85 --HKY85 Specify that all DNA partitions will evolve under the HKY85 model --BootstrapPerm \u00ad\u00adbootstop\u00adperms= Specify the number of permutations to be conducted for the bootstopping/bootstrap convergence test; minimum 100 --option_help -h Display Help Options: Option Description --keep_tmp Keep temporary directory --quiet Do not write output to console (silence stdout and stderr) (default: False) --logfile LOGFILE Name for log file (output) --debug Print commands but do not run (default: False) Example usage: haphpipe build_tree --seqs multiple_align/alignment.fasta --run_full_analysis","title":"build_tree"}]}