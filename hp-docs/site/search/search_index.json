{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About HAPHPIPE HA plotype and PH ylodynamics pipeline for viral assembly, population genetics, and phylodynamics. In this User Guide, we assume basic familiarity with conda environments, basic bash knowledge and knowledge of general next-generation sequencing (NGS) concepts. For more information regarding all of these, see helpful links and FAQ . Remember that a directory is a simply a folder nesting system, similar to what you see on your computer. Where you see \"folder\" below you can also say \"directory.\" HAPHPIPE is intended only for Linux and Mac OS X platforms. If you are a Windows user, see this section on our install page . This User Guide was developed jointly by undergraduate and graduate students to be accessible for users at all stages. Margaret C. Steiner, Keylie M. Gibson, Matthew L. Bendall, Uzma Rentia, and Pengfei Li all contributed to developing this User Guide and testing HAPHPIPE. See our protocol paper (link coming soon) for more information and this paper for our validation study. Citing HAPHPIPE When using HAPHPIPE, please cite our article when it is available. For now, reference the GitHub website: https://github.com/gwcbi/haphpipe and the validation paper . The HAPHPIPE suite Each stage can be run on its own. Stages are grouped into 4 categories: hp_reads , hp_assemble , hp_haplotype , and hp_annotate . More detailed description of command line options for each stage are available in the below sections. To view all available stages in HAPHPIPE, run: haphpipe -h Output will look like: Program: haphpipe (haplotype and phylodynamics pipeline) Version: 1.0.2 Commands: -- Reads sample_reads subsample reads using seqtk trim_reads trim reads using Trimmomatic join_reads join reads using FLASh ec_reads error correct reads using SPAdes -- Assemble assemble_denovo assemble reads denovo assemble_amplicons assemble contigs to amplicon regions assemble_scaffold assemble contigs to genome align_reads align reads to reference call_variants call variants vcf_to_consensus create consensus sequence from VCF refine_assembly iterative refinement: align - variants - consensus finalize_assembly finalize consensus sequence -- Haplotype predict_haplo assemble haplotypes with PredictHaplo ph_parser parse output from PredictHaplo cliquesnv assemble haplotypes with CliqueSNV -- Description pairwise_align align consensus to an annotated reference extract_pairwise extract sequence regions from pairwise alignment summary_stats generates summary statistics for samples -- Phylo multiple_align multiple sequence alignment model_test tests for model of evolution using ModelTest build_tree_NG builds phylogenetic tree with RAxML-NG -- Miscellaneous demo setup demo directory and test data HAPHPIPE consists of a suite of sub-commands under each stage that are invoked as follows: haphpipe [stage] [sub-command] [options] For example, to join paired end reads, one would invoke the following: haphpipe join_reads --fq1 trimmed_1.fastq --fq2 trimmed_2.fastq","title":"Home"},{"location":"#about-haphpipe","text":"HA plotype and PH ylodynamics pipeline for viral assembly, population genetics, and phylodynamics. In this User Guide, we assume basic familiarity with conda environments, basic bash knowledge and knowledge of general next-generation sequencing (NGS) concepts. For more information regarding all of these, see helpful links and FAQ . Remember that a directory is a simply a folder nesting system, similar to what you see on your computer. Where you see \"folder\" below you can also say \"directory.\" HAPHPIPE is intended only for Linux and Mac OS X platforms. If you are a Windows user, see this section on our install page . This User Guide was developed jointly by undergraduate and graduate students to be accessible for users at all stages. Margaret C. Steiner, Keylie M. Gibson, Matthew L. Bendall, Uzma Rentia, and Pengfei Li all contributed to developing this User Guide and testing HAPHPIPE. See our protocol paper (link coming soon) for more information and this paper for our validation study.","title":"About HAPHPIPE"},{"location":"#citing-haphpipe","text":"When using HAPHPIPE, please cite our article when it is available. For now, reference the GitHub website: https://github.com/gwcbi/haphpipe and the validation paper .","title":"Citing HAPHPIPE"},{"location":"#the-haphpipe-suite","text":"Each stage can be run on its own. Stages are grouped into 4 categories: hp_reads , hp_assemble , hp_haplotype , and hp_annotate . More detailed description of command line options for each stage are available in the below sections. To view all available stages in HAPHPIPE, run: haphpipe -h Output will look like: Program: haphpipe (haplotype and phylodynamics pipeline) Version: 1.0.2 Commands: -- Reads sample_reads subsample reads using seqtk trim_reads trim reads using Trimmomatic join_reads join reads using FLASh ec_reads error correct reads using SPAdes -- Assemble assemble_denovo assemble reads denovo assemble_amplicons assemble contigs to amplicon regions assemble_scaffold assemble contigs to genome align_reads align reads to reference call_variants call variants vcf_to_consensus create consensus sequence from VCF refine_assembly iterative refinement: align - variants - consensus finalize_assembly finalize consensus sequence -- Haplotype predict_haplo assemble haplotypes with PredictHaplo ph_parser parse output from PredictHaplo cliquesnv assemble haplotypes with CliqueSNV -- Description pairwise_align align consensus to an annotated reference extract_pairwise extract sequence regions from pairwise alignment summary_stats generates summary statistics for samples -- Phylo multiple_align multiple sequence alignment model_test tests for model of evolution using ModelTest build_tree_NG builds phylogenetic tree with RAxML-NG -- Miscellaneous demo setup demo directory and test data HAPHPIPE consists of a suite of sub-commands under each stage that are invoked as follows: haphpipe [stage] [sub-command] [options] For example, to join paired end reads, one would invoke the following: haphpipe join_reads --fq1 trimmed_1.fastq --fq2 trimmed_2.fastq","title":"The HAPHPIPE suite"},{"location":"adv/","text":"Making your own pipelines with Bash This is an example of how to make your own pipeline with bash. This example uses four NGS samples for COVID-19 from NCBI and puts them through whole genome de novo assembly. SRA accession numbers: SRR11140744 SRR11140746 SRR11140748 SRR11140750 Step 0 - Obtaining samples. We will assume that you have downloaded the reads from NCBI or know how to use fastq-dump. We will show the fastq-dump command here that we used to download the files. This part is not included in the pipeline. for sra in SRR11140744 SRR11140746 SRR11140748 SRR11140750; do fastq-dump --outdir ${sra} --split-files --origfmt ${sra} done Output is: Read 503344 spots for SRR11140744 Written 503344 spots for SRR11140744 Read 358971 spots for SRR11140746 Written 358971 spots for SRR11140746 Read 421395 spots for SRR11140748 Written 421395 spots for SRR11140748 Read 17657 spots for SRR11140750 Written 17657 spots for SRR11140750 You're starting directory should look like for this example, whether the reads were obtained manually or downloaded with fastq-dump: . \u251c\u2500\u2500 SRR11140744 | \u251c\u2500\u2500 SRR11140744_1.fastq | \u2514\u2500\u2500 SRR11140744_2.fastq \u251c\u2500\u2500 SRR11140746 | \u251c\u2500\u2500 SRR11140746_1.fastq | \u2514\u2500\u2500 SRR11140746_2.fastq \u251c\u2500\u2500 SRR11140748 | \u251c\u2500\u2500 SRR11140748_1.fastq | \u2514\u2500\u2500 SRR11140748_2.fastq \u2514\u2500\u2500 SRR11140750 \u251c\u2500\u2500 SRR11140750_1.fastq \u2514\u2500\u2500 SRR11140750_2.fastq We also know we will need a reference genome to help scaffold the contigs. We have downloaded the COVID19 reference genome here as a FASTA file. Step 1 - Evaluate which modules you want to use. View all the module options using haphpipe -h . We have decided that we want to sample the reads ( sample_reads ), trim the reads ( trim_reads ), error correct the reads ( ec_reads ). We want to do genome de novo assembly, so we want to use assemble_denovo and assemble_scaffold . We then want to do refinement of the assembly ( refine_assembly ) and finalize the assembly ( finalize_assembly ). Step 2 - Document files and options needed for each module. As we go through each module, we will take note of what we need and which input is specific for an individual sample (i.e., the input fastq reads will be different per sample, but we probably want the same number of reads for each sample.) We know all modules have the option for a logfile. We'll include that in the bash scripting, but don't need to make notes of a logfile for each module. -- Part 2A - Sample reads. Upon viewing the haphpipe sample_reads -h we see that we need to provide fastq reads 1 and 2, an output directory, and number of reads desired. Sample specific options: fastq read 1 fastq read 2 output directory Not sample specific options: number of reads desired -- Part 2B - Trim reads. Upon viewing the haphpipe trim_reads -h we see that we need to provide fastq reads 1 and 2, an output directory, number of reads, and number of CPUs desired. There is an option to change the timming commands, but we'll keep default. Sample specific options: fastq read 1 -- we will want these to be the subsampled read file fastq read 2 -- we will want these to be the subsampled read file output directory Not sample specific options: number of reads desired ncpu -- Part 2C - Error correct reads. Upon viewing the haphpipe ec_reads -h we see that we need to provide fastq reads 1 and 2, an output directory, and number of CPUs desired. Sample specific options: fastq read 1 -- we will want these to be the trimmed read file fastq read 2 -- we will want these to be the trimmed read file output directory Not sample specific options: ncpu -- Part 2D - De novo assembly. Upon viewing the haphpipe assemble_denovo -h we see that we need to provide fastq reads 1 and 2, an output directory, and number of CPUs desired. Because we previously completed error correction. We will not want to include it here (i.e., we will invoke the option command --no_error_correction ). Sample specific options: fastq read 1 -- we will want these to be the error corrected reads fastq read 2 -- we will want these to be the error corrected reads output directory Not sample specific options: ncpu no error correction option -- Part 2E - Assemble scaffold. Upon viewing the haphpipe assemble_scaffold -h we see that we need to provide fasta file containing assembled contigs, an output directory, name to append to scaffold sequence and reference fasta desired. There is an option to change the timming commands, but we'll keep default. Sample specific options: assembled contigs file output directory sequence name Not sample specific options: reference fasta -- Part 2F - Refine assembly. Upon viewing the haphpipe refine_assembly -h we see that we need to provide fastq reads 1 and 2, an output directory, a reference sequence to refine, a sample ID and a maximum number of refinement steps, and number of CPUs desired. Sample specific options: fastq read 1 -- we will want these to be the error corrected reads fastq read 2 -- we will want these to be the error corrected reads output directory reference fasta -- we will want this to be the sample's assembled scaffold Sample ID Not sample specific options: ncpu maximum number of refinement steps (we'll do 3 for sake of simplicity and time) -- Part 2G - Finalize assembly. Upon viewing the haphpipe finalize_assembly -h we see that we need to provide fastq reads 1 and 2, an output directory, a reference sequence to finalize, a sample ID and a maximum number of refinement steps, and number of CPUs desired. We could replace the preset bowtie2 option, but we will leave it as default for this sample pipeline. Sample specific options: fastq read 1 -- we will want these to be the error corrected reads fastq read 2 -- we will want these to be the error corrected reads output directory reference fasta -- we will want this to be the sample's refined fasta sequence Sample ID Not sample specific options: ncpu -- Part 2H - Gather the needed initial input for each sample. We need to gather the necessary files that we will need to input for each sample so that we can make a script that takes in input files. We know from the User Guide that we can look at the output/input file types and names here . Because each module has a standard output file name, we will only need to be specific about the input for the raw fastq files for each sample. Therefore we need: input raw fastq read 1 input raw fastq read 2 reference genome to scaffold against (this will be a covid19 reference) output directory for each sample sample name The other options we can code into the bash script for each module, since they will be the same for every sample in this analysis. Step 3 - Create a bash script for each module. Now we will format a bash script for each module. -- Part 3A - Input options. Because we have a list of needed input options (specified by the user), we need to make a bash command to take in the inputs. First, we will specify the script name. We can do this one of two ways. i) explicitly or 2) through a command. i) SN='covid_genome' # this sets the script name (SN variable) to covid_genome ii) SN=$(basename $0) # this sets the script name (SN variable) to whatever the script filename is. If the script's file name is covid.sh then it is set as that. If the file name is this_is_file it will be set as that. Because we are in charge of this script, we will explicitly set it. Second, we want to set some input information for the user. We can do this as such: read -r -d '' USAGE <<EOF USAGE: $SN [read1] [read2] [reference_fasta] [samp_id] <outdir> ----- COVID19 Genome Assembly Pipeline ----- This pipeline implements genome assembly using a denovo approach. Reads are error-corrected and used to refine the scaffolded assembly, with up to 3 refinement steps. This pipeline is used as an example for advanced usage - making own pipeline in the User Guide. Input: read1: Fastq file for read 1. May be compressed (.gz) read2: Fastq file for read 2. May be compressed (.gz) reference_fasta: Reference sequence (fasta) samp_id: Sample ID outdir: Output directory (default is [sample_dir]/$SN) EOF Third, we want to provide help information for the script. We can do that using the avoce code, which has been saved in the variable $USAGE . Therefore, #--- Read command line args and if arg is -h, provide the usage information [[ -n \"$1\" ]] && [[ \"$1\" == '-h' ]] && echo \"$USAGE\" && exit 0 If pipeline.sh -h is invoked here, then the output is: USAGE: covid_genome_assembly [read1] [read2] [reference_fasta] [samp_id] <outdir> ----- COVID19 Genome Assembly Pipeline ----- This pipeline implements genome assembly using a denovo approach. Reads are error-corrected and used to refine the scaffolded assembly, with up to 3 refinement steps. This pipeline is used as an example for advanced usage - making own pipeline in the User Guide. Input: read1: Fastq file for read 1. May be compressed (.gz) read2: Fastq file for read 2. May be compressed (.gz) reference_fasta: Reference sequence (fasta) samp_id: Sample ID outdir: Output directory (default is [sample_dir]/covid_genome_assembly) Note that this output looks identical to the information we put into $USAGE variable above in the second part of this section. Fourth, we want to read in the command with the provided input options. Because we are using a bash script, the position of the input files is imparative. #--- Read command line args [[ -n \"$1\" ]] && raw1=\"$1\" [[ -n \"$2\" ]] && raw2=\"$2\" [[ -n \"$3\" ]] && refFA=\"$3\" [[ -n \"$4\" ]] && sampid=\"$4\" [[ -n \"$5\" ]] && outdir=\"$5\" Fifth, we want to check that the input files are provided and are not empty. We also want to set the outdirectory. #--- Check that files are provided and exist [[ -z ${raw1+x} ]] && echo \"FAILED: read1 is not set\" && echo \"$USAGE\" && exit 1 [[ ! -e \"$raw1\" ]] && echo \"[---$SN---] ($(date)) FAILED: file $raw1 does not exist\" && exit 1 [[ -z ${raw2+x} ]] && echo \"FAILED: read2 is not set\" && echo \"$USAGE\" && exit 1 [[ ! -e \"$raw2\" ]] && echo \"[---$SN---] ($(date)) FAILED: file $raw2 does not exist\" && exit 1 [[ -z ${refFA+x} ]] && echo \"FAILED: refFA is not set\" && echo \"$USAGE\" && exit 1 [[ ! -e \"$refFA\" ]] && echo \"[---$SN---] ($(date)) FAILED: file $refFA does not exist\" && exit 1 [[ -z ${sampid+x} ]] && echo \"FAILED: sampid is not set\" && echo \"$USAGE\" && exit 1 #--- Set outdirectory [[ -z ${outdir+x} ]] && outdir=$(dirname $raw1)/$SN mkdir -p $outdir Sixth, we want to set the number of CPUs to use throughout the script. #--- Determine CPUs to use # First examines NCPU environment variable, then nproc, finally sets to 1 [[ -n \"$NCPU\" ]] && ncpu=$NCPU [[ -z $ncpu ]] && ncpu=$(nproc 2> /dev/null) [[ -z $ncpu ]] && ncpu=1 Seventh, we want to print out the variables and files. echo \"[---$SN---] ($(date)) read1: $raw1\" echo \"[---$SN---] ($(date)) read2: $raw2\" echo \"[---$SN---] ($(date)) reference_fasta: $refFA\" echo \"[---$SN---] ($(date)) samp_id: $sampid\" echo \"[---$SN---] ($(date)) outdir: $outdir\" echo \"[---$SN---] ($(date)) num CPU: $ncpu\" Finally, because here at GWU CBI, we like to time everything, we include a line of code to start a timer and end the timer: #--- Start the timer t1=$(date +\"%s\") #### Put haphpipe module scripts here #---Complete job t2=$(date +\"%s\") diff=$(($t2-$t1)) echo \"[---$SN---] ($(date)) $(($diff / 60)) minutes and $(($diff % 60)) seconds elapsed.\" echo \"[---$SN---] ($(date)) $SN COMPLETE.\" After Part 3A, our pipeline file should read as such: Code for beginning of pipeline #!/usr/bin/env bash ############################################################################### # This pipeline implements genome assembly using a denovo approach. Reads are # error-corrected and used to refine the scaffolded assembly, with up to 3 # refinement steps. This pipeline is used as an example for advanced usage # - making own pipeline in the User Guide. ############################################################################### SN='covid_genome_assembly' read -r -d '' USAGE <<EOF USAGE: $SN [read1] [read2] [reference_fasta] [samp_id] <outdir> ----- COVID19 Genome Assembly Pipeline ----- This pipeline implements genome assembly using a denovo approach. Reads are error-corrected and used to refine the scaffolded assembly, with up to 3 refinement steps. This pipeline is used as an example for advanced usage - making own pipeline in the User Guide. Input: read1: Fastq file for read 1. May be compressed (.gz) read2: Fastq file for read 2. May be compressed (.gz) reference_fasta: Reference sequence (fasta) samp_id: Sample ID outdir: Output directory (default is [sample_dir]/$SN) EOF #--- Read command line args and if arg is -h, provide the usage information [[ -n \"$1\" ]] && [[ \"$1\" == '-h' ]] && echo \"$USAGE\" && exit 0 #--- Read command line args [[ -n \"$1\" ]] && raw1=\"$1\" [[ -n \"$2\" ]] && raw2=\"$2\" [[ -n \"$3\" ]] && refFA=\"$3\" [[ -n \"$4\" ]] && refGTF=\"$4\" [[ -n \"$5\" ]] && sampid=\"$5\" [[ -n \"$6\" ]] && outdir=\"$6\" #--- Check that files are provided and exist [[ -z ${raw1+x} ]] && echo \"FAILED: read1 is not set\" && echo \"$USAGE\" && exit 1 [[ ! -e \"$raw1\" ]] && echo \"[---$SN---] ($(date)) FAILED: file $raw1 does not exist\" && exit 1 [[ -z ${raw2+x} ]] && echo \"FAILED: read2 is not set\" && echo \"$USAGE\" && exit 1 [[ ! -e \"$raw2\" ]] && echo \"[---$SN---] ($(date)) FAILED: file $raw2 does not exist\" && exit 1 [[ -z ${refFA+x} ]] && echo \"FAILED: refFA is not set\" && echo \"$USAGE\" && exit 1 [[ ! -e \"$refFA\" ]] && echo \"[---$SN---] ($(date)) FAILED: file $refFA does not exist\" && exit 1 [[ -z ${refGTF+x} ]] && echo \"FAILED: refGTF is not set\" && echo \"$USAGE\" && exit 1 [[ ! -e \"$refGTF\" ]] && echo \"[---$SN---] ($(date)) FAILED: file $refGTF does not exist\" && exit 1 [[ -z ${sampid+x} ]] && echo \"FAILED: sampid is not set\" && echo \"$USAGE\" && exit 1 #--- Set outdirectory [[ -z ${outdir+x} ]] && outdir=$(dirname $raw1)/$SN mkdir -p $outdir #--- Determine CPUs to use # First examines NCPU environment variable, then nproc, finally sets to 1 [[ -n \"$NCPU\" ]] && ncpu=$NCPU [[ -z $ncpu ]] && ncpu=$(nproc 2> /dev/null) [[ -z $ncpu ]] && ncpu=1 echo \"[---$SN---] ($(date)) read1: $raw1\" echo \"[---$SN---] ($(date)) read2: $raw2\" echo \"[---$SN---] ($(date)) reference_fasta: $refFA\" echo \"[---$SN---] ($(date)) reference_gtf: $refGTF\" echo \"[---$SN---] ($(date)) samp_id: $sampid\" echo \"[---$SN---] ($(date)) outdir: $outdir\" echo \"[---$SN---] ($(date)) num CPU: $ncpu\" #--- Start the timer t1=$(date +\"%s\") #### Put haphpipe module scripts here #---Complete job t2=$(date +\"%s\") diff=$(($t2-$t1)) echo \"[---$SN---] ($(date)) $(($diff / 60)) minutes and $(($diff % 60)) seconds elapsed.\" echo \"[---$SN---] ($(date)) $SN COMPLETE.\" -- Part 3B - Sample reads. Now we will begin constructing bash scripts for each module. Each module will follow a similar trend. We will set the module name. We will echo module name to terminal. We will check to make sure the files that are the output of the module are not already present. If they are present, we will skip the module and continue on (no need to repeat the module). If the files are not present, we will complete the module and call the command. If the command completes, we will print to terminal. If command fails, we will print that to terminal and quit the script. Remember, you can find the output file names here . The ouput names for this module are: sample_1.fastq and sample_2.fastq . We have also decided to subsample the number of reads to 50,000 reads for each sample. Now this in the input for the base haphpipe command for this module: haphpipe sample_reads\\ --fq1 $raw1\\ --fq2 $raw2\\ --nreads 50000\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} The entire module's bash script is here: module=\"sample_reads\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" # if the sampled files are present, skip this module. Otherwise, call sample_reads if [[ -e $outdir/sample_1.fastq && -e ${outdir}/sample_2.fastq ]]; then echo \"[---$SN---] ($(date)) EXISTS: $module sample_1.fastq, sample_2.fastq\" else # this reads in the sample_reads command and saves it in the variable cmd read -r -d '' cmd <<EOF haphpipe sample_reads\\ --fq1 $raw1\\ --fq2 $raw2\\ --nreads 50000\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) fi -- Part 3C - Trim reads. Remember, you can find the output file names here . The ouput names for this module are: trimmed_1.fastq and trimmed_2.fastq . Now, because we are doing trimming after the sampled reads module, we need to change the input fastq reads for this module to be the fastq reads output from the previous step (sample reads). Therefore, the input fastq files are named sample_1.fastq and sample_2.fastq . Also remember that these files are now contained in the outdirectory specified by the input, so we have to list the path to the input fastq files. Now this in the input for the base haphpipe command for this module: haphpipe trim_reads\\ --ncpu $ncpu\\ --fq1 ${outdir}/sample_1.fastq\\ --fq2 ${outdir}/sample_2.fastq\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} The entire module's bash script is here: module=\"trim_reads\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" if [[ -e $outdir/trimmed_1.fastq && -e ${outdir}/trimmed_2.fastq ]]; then echo \"[---$SN---] ($(date)) EXISTS: $module trimmed_1.fastq,trimmed_2.fastq\" else read -r -d '' cmd <<EOF haphpipe trim_reads\\ --ncpu $ncpu\\ --fq1 ${outdir}/sample_1.fastq\\ --fq2 ${outdir}/sample_2.fastq\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) fi -- Part 3D - Error correct reads. Remember, you can find the output file names here . The ouput names for this module are: corrected_1.fastq and corrected_2.fastq . Now, because we are doing error correction after the trimming module, we need to change the input fastq reads for this module to be the fastq reads output from the previous step (trimmed reads). Therefore, the input fastq files are named trimmed_1.fastq and trimmed_2.fastq . Also remember that these files are now contained in the outdirectory specified by the input (just like the sampled reads in the previous step), so we have to list the path to the input fastq files. Now this in the input for the base haphpipe command for this module: haphpipe ec_reads\\ --ncpu $ncpu\\ --fq1 ${outdir}/trimmed_1.fastq\\ --fq2 ${outdir}/trimmed_2.fastq\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} The entire module's bash script is here: module=\"ec_reads\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" if [[ -e $outdir/corrected_1.fastq && -e $outdir/corrected_2.fastq ]]; then echo \"[---$SN---] ($(date)) EXISTS: $module corrected_1.fastq,corrected_2.fastq\" else read -r -d '' cmd <<EOF haphpipe ec_reads\\ --ncpu $ncpu\\ --fq1 ${outdir}/trimmed_1.fastq\\ --fq2 ${outdir}/trimmed_2.fastq\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) fi -- Part 3E - De novo assembly. Remember, you can find the output file names here . The ouput name for this module is denovo_contigs.fna . Now, because we are doing denovo assembly after the error correction module, we need to change the input fastq reads for this module to be the fastq reads output from the previous step (error corrected reads). Therefore, the input fastq files are named corrected_1.fastq and corrected_2.fastq . Also remember that these files are now contained in the outdirectory specified by the input, so we have to list the path to the input fastq files. Finally, remember we want to specify that we do NOT want to do another round of error correction. Now this in the input for the base haphpipe command for this module: haphpipe assemble_denovo\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --no_error_correction\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} The entire module's bash script is here: module=\"assemble_denovo\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" if [[ -e $outdir/denovo_contigs.fna ]]; then echo \"[---$SN---] ($(date)) EXISTS: $module denovo_contigs.fna\" else read -r -d '' cmd <<EOF haphpipe assemble_denovo\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --no_error_correction\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) fi -- Part 3F - Assemble scaffold. Remember, you can find the output file names here . The ouput name for this module is scaffold_assembly.fa . There are other outputs, but we focus on this one for further use in the refinement and finalize modules. Now, because we are doing scaffold assembly after the denovo assembly module, we need to specify that the input file is the output contig file ( denovo_contigs.fna ). Again, remember that this file is now contained in the outdirectory specified by the input, so we have to list the path too. We also have to use our input reference fasta file and input sampleID from the script command. Now this in the input for the base haphpipe command for this module: haphpipe assemble_scaffold\\ --contigs_fa ${outdir}/denovo_contigs.fna\\ --ref_fa ${refFA}\\ --seqname ${sampid}\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} The entire module's bash script is here: module=\"assemble_scaffold\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" if [[ -e $outdir/scaffold_assembly.fa ]]; then echo \"[---$SN---] ($(date)) EXISTS: $module scaffold_assembly.fa\" else read -r -d '' cmd <<EOF haphpipe assemble_scaffold\\ --contigs_fa ${outdir}/denovo_contigs.fna\\ --ref_fa ${refFA}\\ --seqname ${sampid}\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) fi -- Part 3G - Refine assembly. Remember, you can find the output file names here . The ouput name for this module is refined.fna . Now, because we are doing refining the assembly after the scaffold assembly module, we need to change the input fastq reads for this module to be the fastq reads output from the error correction step (error corrected reads). Therefore, the input fastq files are named corrected_1.fastq and corrected_2.fastq . Also remember that these files are now contained in the outdirectory specified by the input, so we have to list the path to the input fastq files. Finally, we need to specify that the input reference file is the scaffold_assembly.fa file from the previous scaffold assembly step (above), which is also located in the outdirectory. Now this in the input for the base haphpipe command for this module: hp_refine_assembly\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --ref_fa ${outdir}/scaffold_assembly.fa\\ --sample_id ${sampid}\\ --max_step 5\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} The entire module's bash script is here: module=\"refine_assembly\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" if [[ -e ${outdir}/refined.fna ]]; then echo \"[---$SN---] ($(date)) EXISTS: $module refined.fna\" else read -r -d '' cmd <<EOF hp_refine_assembly\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --ref_fa ${outdir}/scaffold_assembly.fa\\ --sample_id ${sampid}\\ --max_step 5\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) fi -- Part 3H - Finalize assembly. Remember, you can find the output file names here . The ouput files for this module are: final.fna , final.bam , final.vcf.gz . Now, because we are doing finalizing the assembly after the refinement module, we need to change the input fastq reads for this module to be the fastq reads output from the error correction step (error corrected reads). Therefore, the input fastq files are named corrected_1.fastq and corrected_2.fastq . Also remember that these files are now contained in the outdirectory specified by the input, so we have to list the path to the input fastq files. Finally, we need to specify that the input reference file is the refined.fna file from the previous refinement step (above), which is also located in the outdirectory. Now this in the input for the base haphpipe command for this module: hp_finalize_assembly\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --sample_id ${sampid}\\ --ref_fa ${outdir}/refined.fna\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} The entire module's bash script is here: module=\"finalize_assembly\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" if [[ -e ${outdir}/final.fna && -e ${outdir}/final.bam && -e ${outdir}/final.vcf.gz ]]; then echo \"[---$SN---] ($(date)) EXISTS: $module final.fna,final.bam,final.vcf.gz\" else read -r -d '' cmd <<EOF hp_finalize_assembly\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --sample_id ${sampid}\\ --ref_fa ${outdir}/refined.fna\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) fi -- Part 3I - Gather all the individual module scripts into the final pipeline script. For readable code, we will separate each module with ### like so: ############################################################################### # Step #: description here ############################################################################### insert code here .. Once we concatenate all our code, we end up with this: code with all modules ############################################################################### # Step 1: Sample Reads ############################################################################### module=\"sample_reads\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" # if the sampled files are present, skip this module. Otherwise, call sample_reads if [[ -e $outdir/sample_1.fastq && -e ${outdir}/sample_2.fastq ]]; then echo \"[---$SN---] ($(date)) EXISTS: $module sample_1.fastq, sample_2.fastq\" else # this reads in the sample_reads command and saves it in the variable cmd read -r -d '' cmd <<EOF haphpipe sample_reads\\ --fq1 $raw1\\ --fq2 $raw2\\ --nreads 50000\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) fi ############################################################################### # Step 2: Trim Reads ############################################################################### module=\"trim_reads\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" if [[ -e $outdir/trimmed_1.fastq && -e ${outdir}/trimmed_2.fastq ]]; then echo \"[---$SN---] ($(date)) EXISTS: $module trimmed_1.fastq,trimmed_2.fastq\" else read -r -d '' cmd <<EOF haphpipe trim_reads\\ --ncpu $ncpu\\ --fq1 $raw1\\ --fq2 $raw2\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) fi ############################################################################### # Step 3: Error correction using Spades ############################################################################### module=\"ec_reads\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" if [[ -e $outdir/corrected_1.fastq && -e $outdir/corrected_2.fastq ]]; then echo \"[---$SN---] ($(date)) EXISTS: $module corrected_1.fastq,corrected_2.fastq\" else read -r -d '' cmd <<EOF haphpipe ec_reads\\ --ncpu $ncpu\\ --fq1 ${outdir}/trimmed_1.fastq\\ --fq2 ${outdir}/trimmed_2.fastq\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) fi ############################################################################### # Step 4: Denovo assembly ############################################################################### module=\"assemble_denovo\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" if [[ -e $outdir/denovo_contigs.fna ]]; then echo \"[---$SN---] ($(date)) EXISTS: $module denovo_contigs.fna\" else read -r -d '' cmd <<EOF haphpipe assemble_denovo\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --no_error_correction\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) fi ############################################################################### # Step 5: Scaffold assembly ############################################################################### module=\"assemble_scaffold\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" if [[ -e $outdir/scaffold_assembly.fa ]]; then echo \"[---$SN---] ($(date)) EXISTS: $module scaffold_assembly.fa\" else read -r -d '' cmd <<EOF haphpipe assemble_scaffold\\ --contigs_fa ${outdir}/denovo_contigs.fna\\ --ref_fa ${refFA}\\ --seqname ${sampid}\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) fi ############################################################################### # Step 5: Refine assembly ############################################################################### module=\"refine_assembly\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" if [[ -e ${outdir}/refined.fna ]]; then echo \"[---$SN---] ($(date)) EXISTS: $module refined.fna\" else read -r -d '' cmd <<EOF hp_refine_assembly\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --ref_fa ${outdir}/scaffold_assembly.fa\\ --sample_id ${sampid}\\ --max_step 5\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) fi ############################################################################### # Step 5: Finalize assembly ############################################################################### module=\"finalize_assembly\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" if [[ -e ${outdir}/final.fna && -e ${outdir}/final.bam && -e ${outdir}/final.vcf.gz ]]; then echo \"[---$SN---] ($(date)) EXISTS: $module final.fna,final.bam,final.vcf.gz\" else read -r -d '' cmd <<EOF hp_finalize_assembly\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --sample_id ${sampid}\\ --ref_fa ${outdir}/refined.fna\\ ${quiet} --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) fi Step 4 - Combine both the input code and bash scripts for each module into a single script. We named this script covid_genome_assembly.sh and the entire code is: `covid_genome_assembly.sh` #!/usr/bin/env bash ############################################################################### # This pipeline implements genome assembly using a denovo approach. Reads are # error-corrected and used to refine the scaffolded assembly, with up to 3 # refinement steps. This pipeline is used as an example for advanced usage # - making own pipeline in the User Guide. ############################################################################### SN='covid_genome_assembly' read -r -d '' USAGE <<EOF USAGE: $SN [read1] [read2] [reference_fasta] [samp_id] <outdir> ----- COVID19 Genome Assembly Pipeline ----- This pipeline implements genome assembly using a denovo approach. Reads are error-corrected and used to refine the scaffolded assembly, with up to 3 refinement steps. This pipeline is used as an example for advanced usage - making own pipeline in the User Guide. Input: read1: Fastq file for read 1. May be compressed (.gz) read2: Fastq file for read 2. May be compressed (.gz) reference_fasta: Reference sequence (fasta) samp_id: Sample ID outdir: Output directory (default is [sample_dir]/$SN) EOF #--- Read command line args and if arg is -h, provide the usage information [[ -n \"$1\" ]] && [[ \"$1\" == '-h' ]] && echo \"$USAGE\" && exit 0 #--- Read command line args [[ -n \"$1\" ]] && raw1=\"$1\" [[ -n \"$2\" ]] && raw2=\"$2\" [[ -n \"$3\" ]] && refFA=\"$3\" [[ -n \"$4\" ]] && sampid=\"$4\" [[ -n \"$5\" ]] && outdir=\"$5\" #--- Check that files are provided and exist [[ -z ${raw1+x} ]] && echo \"FAILED: read1 is not set\" && echo \"$USAGE\" && exit 1 [[ ! -e \"$raw1\" ]] && echo \"[---$SN---] ($(date)) FAILED: file $raw1 does not exist\" && exit 1 [[ -z ${raw2+x} ]] && echo \"FAILED: read2 is not set\" && echo \"$USAGE\" && exit 1 [[ ! -e \"$raw2\" ]] && echo \"[---$SN---] ($(date)) FAILED: file $raw2 does not exist\" && exit 1 [[ -z ${refFA+x} ]] && echo \"FAILED: refFA is not set\" && echo \"$USAGE\" && exit 1 [[ ! -e \"$refFA\" ]] && echo \"[---$SN---] ($(date)) FAILED: file $refFA does not exist\" && exit 1 [[ -z ${sampid+x} ]] && echo \"FAILED: sampid is not set\" && echo \"$USAGE\" && exit 1 #--- Set outdirectory [[ -z ${outdir+x} ]] && outdir=$(dirname $raw1)/$SN mkdir -p $outdir #--- Determine CPUs to use # First examines NCPU environment variable, then nproc, finally sets to 1 [[ -n \"$NCPU\" ]] && ncpu=$NCPU [[ -z $ncpu ]] && ncpu=$(nproc 2> /dev/null) [[ -z $ncpu ]] && ncpu=1 echo \"[---$SN---] ($(date)) read1: $raw1\" echo \"[---$SN---] ($(date)) read2: $raw2\" echo \"[---$SN---] ($(date)) reference_fasta: $refFA\" echo \"[---$SN---] ($(date)) samp_id: $sampid\" echo \"[---$SN---] ($(date)) outdir: $outdir\" echo \"[---$SN---] ($(date)) num CPU: $ncpu\" #--- Start the timer t1=$(date +\"%s\") ############################################################################### # Step 1: Sample Reads ############################################################################### module=\"sample_reads\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" # if the sampled files are present, skip this module. Otherwise, call sample_reads if [[ -e $outdir/sample_1.fastq && -e ${outdir}/sample_2.fastq ]]; then echo \"[---$SN---] ($(date)) EXISTS: $module sample_1.fastq, sample_2.fastq\" else # this reads in the sample_reads command and saves it in the variable cmd read -r -d '' cmd <<EOF haphpipe sample_reads\\ --fq1 $raw1\\ --fq2 $raw2\\ --nreads 50000\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) fi ############################################################################### # Step 2: Trim Reads ############################################################################### module=\"trim_reads\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" if [[ -e $outdir/trimmed_1.fastq && -e ${outdir}/trimmed_2.fastq ]]; then echo \"[---$SN---] ($(date)) EXISTS: $module trimmed_1.fastq,trimmed_2.fastq\" else read -r -d '' cmd <<EOF haphpipe trim_reads\\ --ncpu $ncpu\\ --fq1 $raw1\\ --fq2 $raw2\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) fi ############################################################################### # Step 3: Error correction using Spades ############################################################################### module=\"ec_reads\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" if [[ -e $outdir/corrected_1.fastq && -e $outdir/corrected_2.fastq ]]; then echo \"[---$SN---] ($(date)) EXISTS: $module corrected_1.fastq,corrected_2.fastq\" else read -r -d '' cmd <<EOF haphpipe ec_reads\\ --ncpu $ncpu\\ --fq1 ${outdir}/trimmed_1.fastq\\ --fq2 ${outdir}/trimmed_2.fastq\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) fi ############################################################################### # Step 4: Denovo assembly ############################################################################### module=\"assemble_denovo\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" if [[ -e $outdir/denovo_contigs.fna ]]; then echo \"[---$SN---] ($(date)) EXISTS: $module denovo_contigs.fna\" else read -r -d '' cmd <<EOF haphpipe assemble_denovo\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --no_error_correction\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) fi ############################################################################### # Step 5: Scaffold assembly ############################################################################### module=\"assemble_scaffold\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" if [[ -e $outdir/scaffold_assembly.fa ]]; then echo \"[---$SN---] ($(date)) EXISTS: $module scaffold_assembly.fa\" else read -r -d '' cmd <<EOF haphpipe assemble_scaffold\\ --contigs_fa ${outdir}/denovo_contigs.fna\\ --ref_fa ${refFA}\\ --seqname ${sampid}\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) fi ############################################################################### # Step 5: Refine assembly ############################################################################### module=\"refine_assembly\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" if [[ -e ${outdir}/refined.fna ]]; then echo \"[---$SN---] ($(date)) EXISTS: $module refined.fna\" else read -r -d '' cmd <<EOF hp_refine_assembly\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --ref_fa ${outdir}/scaffold_assembly.fa\\ --sample_id ${sampid}\\ --max_step 5\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) fi ############################################################################### # Step 5: Finalize assembly ############################################################################### module=\"finalize_assembly\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" if [[ -e ${outdir}/final.fna && -e ${outdir}/final.bam && -e ${outdir}/final.vcf.gz ]]; then echo \"[---$SN---] ($(date)) EXISTS: $module final.fna,final.bam,final.vcf.gz\" else read -r -d '' cmd <<EOF hp_finalize_assembly\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --sample_id ${sampid}\\ --ref_fa ${outdir}/refined.fna\\ ${quiet} --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) fi #### Put haphpipe module scripts here #---Complete job t2=$(date +\"%s\") diff=$(($t2-$t1)) echo \"[---$SN---] ($(date)) $(($diff / 60)) minutes and $(($diff % 60)) seconds elapsed.\" echo \"[---$SN---] ($(date)) $SN COMPLETE.\" Step 5 - Executing the script. Example to run: bash covid_genome_assembly.sh SRR11140750/SRR11140750_1.fastq SRR11140750/SRR11140750_2.fastq SARSCoV2.NC_045512.COVID19.fasta SRR11140750 Example to run in a loop over all the samples: for sra in SRR11140744 SRR11140746 SRR11140748 SRR11140750; do bash covid_genome_assembly.sh ${sra}/${sra}_1.fastq ${sra}/${sra}_2.fastq SARSCoV2.NC_045512.COVID19.fasta ${sra} covid_genome_assembly done Directories should look like such after running this script: Directory structure . \u251c\u2500\u2500 SRR11140744 | \u251c\u2500\u2500 SRR11140744_1.fastq | \u251c\u2500\u2500 SRR11140744_2.fastq | \u2514\u2500\u2500 covid_genome_assembly | \u251c\u2500\u2500 corrected_1.fastq | \u251c\u2500\u2500 corrected_2.fastq | \u251c\u2500\u2500 corrected_U.fastq | \u251c\u2500\u2500 denovo_contigs.fna | \u251c\u2500\u2500 denovo_summary.txt | \u251c\u2500\u2500 haphpipe.out | \u251c\u2500\u2500 final.bam | \u251c\u2500\u2500 final.bam.bai | \u251c\u2500\u2500 final_bt2.out | \u251c\u2500\u2500 final.fna | \u251c\u2500\u2500 final.vcf.gz | \u251c\u2500\u2500 final.vcf.gz.tbi | \u251c\u2500\u2500 refined.01.fna | \u251c\u2500\u2500 refined_bt2.01.out | \u251c\u2500\u2500 refined.fna | \u251c\u2500\u2500 refined_bt2.out | \u251c\u2500\u2500 refined_summary.out | \u251c\u2500\u2500 sample_1.fastq | \u251c\u2500\u2500 sample_2.fastq | \u251c\u2500\u2500 scaffold_aligned.fa | \u251c\u2500\u2500 scaffold_assembly.fa | \u251c\u2500\u2500 scaffold_imputed.fa | \u251c\u2500\u2500 scaffold_padded.out | \u251c\u2500\u2500 trimmed_1.fastq | \u251c\u2500\u2500 trimmed_2.fastq | \u251c\u2500\u2500 trimmed_U.fastq | \u2514\u2500\u2500 trimmomatic_summary.out \u251c\u2500\u2500 SRR11140746 | \u251c\u2500\u2500 SRR11140746_1.fastq | \u251c\u2500\u2500 SRR11140746_2.fastq | \u2514\u2500\u2500 covid_genome_assembly .... Step 6 - Optional inclusions: PredictHaplo, multiple alignment, and building a phylogenetic tree. Here are some option modules to include within the pipeline. -- Adding PredictHaplo as a module. If you desire PredictHaplo, you can either utilize the option --interval_txt or you can rerun the pipeline with this gtf file. It is easier for PredictHaplo to run on smaller regions than the entire genome like we implemented above. $ cat SARSCoV2.NC_045512.COVID19.gtf SARSCoV2.NC_045512.COVID19 NCBI_refseq amplicon 265 13482 . + 0 name \"orf1a\"; SARSCoV2.NC_045512.COVID19 NCBI_refseq amplicon 21562 25383 . + 0 name \"surface\"; SARSCoV2.NC_045512.COVID19 NCBI_refseq amplicon 26244 26471 . + 0 name \"envelope\"; Remember, you can find the output file names here . The ouput name for this module is PH0#.best*.fas . We are doing this module after the finalize_assembly module, doing refining the assembly after the scaffold assembly module. Therefore, the input fastq files are named corrected_1.fastq and corrected_2.fastq . Also remember that these files are now contained in the outdirectory specified by the input, so we have to list the path to the input fastq files. Finally, we need to specify that the input reference file is the final.fna file from the finalize assembly module, which is also located in the outdirectory. We have to do loops for the PredictHaplo and parser modules because there are multiple haplotype regions. Now this in the input for the base haphpipe command for this module: hp_predict_haplo --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --ref_fa ${outdir}/final.fna\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} The entire module's bash script is here: predict_haplo module module=\"predict_haplo\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" for PH in ${outdir}/PH*; do if [[ -e \"${PH}\" ]]; then for PHbest in ${outdir}/PH*/*best*.fas; do if [[ -e \"${PHbest}\" ]]; then echo \"[---$SN---] ($(date)) EXISTS: $module $PHbest\" fi done else read -r -d '' cmd <<EOF hp_predict_haplo --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --ref_fa ${outdir}/final.fna\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) fi done -- Following up PredictHaplo with ph_parser In order to use the output from PredictHaplo (see description here ), we need to run the ph_parser module. hp_ph_parser --haplotypes_fa ${outdir}/${PH}/*best*.fas\\ --prefix ${sampID}_${PH}\\ --logfile ${outdir}/${PH}/haphpipe.out\\ --outdir ${outdir} Now we have to do a loop, because there are multiple regions (i.e., PH0# directories). The entire module's bash script is here: ph_parser module module=\"ph_parser\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" for PH in ${outdir}/PH*/ph_haplotypes.fna; do #if [[ -e \"${PH}\" ]]; then # echo \"[---$SN---] ($(date)) EXISTS: $module $PH\" #else read -r -d '' cmd <<EOF hp_ph_parser --haplotypes_fa $(dirname $PH)/*best*.fas\\ --prefix ${sampID}_$(dirname $(basename $PH))\\ --logfile ${outdir}/haphpipe.out\\ --outdir $(dirname $PH) EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) #fi done -- Adding multiple_align as a module Remember, you can find the output file names here . The ouput name for this module is alignment.fasta . We first need to make a text file that has a list of directories that contin final.fna . Because this is a genome assembly, we want to use the --alignall option to align the entire region and not use a GTF file. We also choose to output a phylip file using the option --phyipout . Now this in the input for the base haphpipe command for this module: haphpipe multiple_align --ncpu $ncpu\\ --dir_list dir_list.txt\\ --phylipout\\ --alignall\\ --logfile haphpipe.out The entire module's bash script is here: multiple_align module module=\"multiple_align\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" if [[ -e $outdir/hp_multiple_align/alignment.fasta ]] && [[ -e $outdir/hp_multiple_align/alignment.phy ]]; then echo \"[---$SN---] ($(date)) EXISTS: $module alignment.fasta,alignment.phy\" else read -r -d '' cmd <<EOF haphpipe multiple_align\\ --ncpu $ncpu\\ --dir_list dir_list.txt\\ --phylipout\\ --alignall\\ --logfile haphpipe.out EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) fi -- Adding model_test as a module Remember, you can find the output file names here . The ouput name for this module is modeltest_results.out . The input will be the alignment file from multiple_align . We'll also give the output an id of covid19_genome so the output file will be covid19_genome_modeltest_results.out . We will also chose the template of the output as raxml, since the next module build_tree uses RAxML. Now this in the input for the base haphpipe command for this module: haphpipe model_test --seqs ${outdir}/hp_multiple_align/alignment.fasta\\ --run_id covid_genome\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir}\\ --template raxml\\ --ncpu ${ncpu} The entire module's bash script is here: model_test module module=\"model_test\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" if [[ -e $outdir/covid19_genome_modeltest_results.out ]]; then echo \"[---$SN---] ($(date)) EXISTS: $module covid19_genome_modeltest_results.out\" else read -r -d '' cmd <<EOF haphpipe model_test --seqs ${outdir}/hp_multiple_align/alignment.fasta\\ --run_id covid_genome\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir}\\ --template raxml\\ --ncpu ${ncpu} EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) fi -- Adding build_tree as a module Remember, you can find the output file names here . The ouput name for this module is modeltest_results.out . The input will be the alignment file from multiple_align . We'll also give the output an id of covid19_genome so the output file will be covid19_genome_modeltest_results.out . Now this in the input for the base haphpipe command for this module: haphpipe build_tree --run_full_analysis\\ --seqs ${outdir}/hp_multiple_align/alignment.fasta\\ --run_id covid_genome.tre\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir}\\ --model GTRCATX The entire module's bash script is here: build_tree module module=\"model_test\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" if [[ -e $outdir/hp_build_tree/RAxML_bipartitionsBranchLabels.covid_genome.tre ]]; then echo \"[---$SN---] ($(date)) EXISTS: $module RAxML_bipartitionsBranchLabels.covid_genome.tre\" else read -r -d '' cmd <<EOF haphpipe build_tree --run_full_analysis\\ --seqs ${outdir}/hp_multiple_align/alignment.fasta\\ --run_id covid_genome.tre\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir}\\ --model GTRCATX EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) fi Finally, the tree file RAxML_bipartitionsBranchLabels.covid_genome.tre can be viewed in and annotated in programs such as FigTree or iTOL . Snakemake Advanced users may also be interested in constructing a Snakemake workflow. See the Snakemake documentation for details.","title":"Advanced Users"},{"location":"adv/#making-your-own-pipelines-with-bash","text":"This is an example of how to make your own pipeline with bash. This example uses four NGS samples for COVID-19 from NCBI and puts them through whole genome de novo assembly. SRA accession numbers: SRR11140744 SRR11140746 SRR11140748 SRR11140750 Step 0 - Obtaining samples. We will assume that you have downloaded the reads from NCBI or know how to use fastq-dump. We will show the fastq-dump command here that we used to download the files. This part is not included in the pipeline. for sra in SRR11140744 SRR11140746 SRR11140748 SRR11140750; do fastq-dump --outdir ${sra} --split-files --origfmt ${sra} done Output is: Read 503344 spots for SRR11140744 Written 503344 spots for SRR11140744 Read 358971 spots for SRR11140746 Written 358971 spots for SRR11140746 Read 421395 spots for SRR11140748 Written 421395 spots for SRR11140748 Read 17657 spots for SRR11140750 Written 17657 spots for SRR11140750 You're starting directory should look like for this example, whether the reads were obtained manually or downloaded with fastq-dump: . \u251c\u2500\u2500 SRR11140744 | \u251c\u2500\u2500 SRR11140744_1.fastq | \u2514\u2500\u2500 SRR11140744_2.fastq \u251c\u2500\u2500 SRR11140746 | \u251c\u2500\u2500 SRR11140746_1.fastq | \u2514\u2500\u2500 SRR11140746_2.fastq \u251c\u2500\u2500 SRR11140748 | \u251c\u2500\u2500 SRR11140748_1.fastq | \u2514\u2500\u2500 SRR11140748_2.fastq \u2514\u2500\u2500 SRR11140750 \u251c\u2500\u2500 SRR11140750_1.fastq \u2514\u2500\u2500 SRR11140750_2.fastq We also know we will need a reference genome to help scaffold the contigs. We have downloaded the COVID19 reference genome here as a FASTA file. Step 1 - Evaluate which modules you want to use. View all the module options using haphpipe -h . We have decided that we want to sample the reads ( sample_reads ), trim the reads ( trim_reads ), error correct the reads ( ec_reads ). We want to do genome de novo assembly, so we want to use assemble_denovo and assemble_scaffold . We then want to do refinement of the assembly ( refine_assembly ) and finalize the assembly ( finalize_assembly ). Step 2 - Document files and options needed for each module. As we go through each module, we will take note of what we need and which input is specific for an individual sample (i.e., the input fastq reads will be different per sample, but we probably want the same number of reads for each sample.) We know all modules have the option for a logfile. We'll include that in the bash scripting, but don't need to make notes of a logfile for each module. -- Part 2A - Sample reads. Upon viewing the haphpipe sample_reads -h we see that we need to provide fastq reads 1 and 2, an output directory, and number of reads desired. Sample specific options: fastq read 1 fastq read 2 output directory Not sample specific options: number of reads desired -- Part 2B - Trim reads. Upon viewing the haphpipe trim_reads -h we see that we need to provide fastq reads 1 and 2, an output directory, number of reads, and number of CPUs desired. There is an option to change the timming commands, but we'll keep default. Sample specific options: fastq read 1 -- we will want these to be the subsampled read file fastq read 2 -- we will want these to be the subsampled read file output directory Not sample specific options: number of reads desired ncpu -- Part 2C - Error correct reads. Upon viewing the haphpipe ec_reads -h we see that we need to provide fastq reads 1 and 2, an output directory, and number of CPUs desired. Sample specific options: fastq read 1 -- we will want these to be the trimmed read file fastq read 2 -- we will want these to be the trimmed read file output directory Not sample specific options: ncpu -- Part 2D - De novo assembly. Upon viewing the haphpipe assemble_denovo -h we see that we need to provide fastq reads 1 and 2, an output directory, and number of CPUs desired. Because we previously completed error correction. We will not want to include it here (i.e., we will invoke the option command --no_error_correction ). Sample specific options: fastq read 1 -- we will want these to be the error corrected reads fastq read 2 -- we will want these to be the error corrected reads output directory Not sample specific options: ncpu no error correction option -- Part 2E - Assemble scaffold. Upon viewing the haphpipe assemble_scaffold -h we see that we need to provide fasta file containing assembled contigs, an output directory, name to append to scaffold sequence and reference fasta desired. There is an option to change the timming commands, but we'll keep default. Sample specific options: assembled contigs file output directory sequence name Not sample specific options: reference fasta -- Part 2F - Refine assembly. Upon viewing the haphpipe refine_assembly -h we see that we need to provide fastq reads 1 and 2, an output directory, a reference sequence to refine, a sample ID and a maximum number of refinement steps, and number of CPUs desired. Sample specific options: fastq read 1 -- we will want these to be the error corrected reads fastq read 2 -- we will want these to be the error corrected reads output directory reference fasta -- we will want this to be the sample's assembled scaffold Sample ID Not sample specific options: ncpu maximum number of refinement steps (we'll do 3 for sake of simplicity and time) -- Part 2G - Finalize assembly. Upon viewing the haphpipe finalize_assembly -h we see that we need to provide fastq reads 1 and 2, an output directory, a reference sequence to finalize, a sample ID and a maximum number of refinement steps, and number of CPUs desired. We could replace the preset bowtie2 option, but we will leave it as default for this sample pipeline. Sample specific options: fastq read 1 -- we will want these to be the error corrected reads fastq read 2 -- we will want these to be the error corrected reads output directory reference fasta -- we will want this to be the sample's refined fasta sequence Sample ID Not sample specific options: ncpu -- Part 2H - Gather the needed initial input for each sample. We need to gather the necessary files that we will need to input for each sample so that we can make a script that takes in input files. We know from the User Guide that we can look at the output/input file types and names here . Because each module has a standard output file name, we will only need to be specific about the input for the raw fastq files for each sample. Therefore we need: input raw fastq read 1 input raw fastq read 2 reference genome to scaffold against (this will be a covid19 reference) output directory for each sample sample name The other options we can code into the bash script for each module, since they will be the same for every sample in this analysis. Step 3 - Create a bash script for each module. Now we will format a bash script for each module. -- Part 3A - Input options. Because we have a list of needed input options (specified by the user), we need to make a bash command to take in the inputs. First, we will specify the script name. We can do this one of two ways. i) explicitly or 2) through a command. i) SN='covid_genome' # this sets the script name (SN variable) to covid_genome ii) SN=$(basename $0) # this sets the script name (SN variable) to whatever the script filename is. If the script's file name is covid.sh then it is set as that. If the file name is this_is_file it will be set as that. Because we are in charge of this script, we will explicitly set it. Second, we want to set some input information for the user. We can do this as such: read -r -d '' USAGE <<EOF USAGE: $SN [read1] [read2] [reference_fasta] [samp_id] <outdir> ----- COVID19 Genome Assembly Pipeline ----- This pipeline implements genome assembly using a denovo approach. Reads are error-corrected and used to refine the scaffolded assembly, with up to 3 refinement steps. This pipeline is used as an example for advanced usage - making own pipeline in the User Guide. Input: read1: Fastq file for read 1. May be compressed (.gz) read2: Fastq file for read 2. May be compressed (.gz) reference_fasta: Reference sequence (fasta) samp_id: Sample ID outdir: Output directory (default is [sample_dir]/$SN) EOF Third, we want to provide help information for the script. We can do that using the avoce code, which has been saved in the variable $USAGE . Therefore, #--- Read command line args and if arg is -h, provide the usage information [[ -n \"$1\" ]] && [[ \"$1\" == '-h' ]] && echo \"$USAGE\" && exit 0 If pipeline.sh -h is invoked here, then the output is: USAGE: covid_genome_assembly [read1] [read2] [reference_fasta] [samp_id] <outdir> ----- COVID19 Genome Assembly Pipeline ----- This pipeline implements genome assembly using a denovo approach. Reads are error-corrected and used to refine the scaffolded assembly, with up to 3 refinement steps. This pipeline is used as an example for advanced usage - making own pipeline in the User Guide. Input: read1: Fastq file for read 1. May be compressed (.gz) read2: Fastq file for read 2. May be compressed (.gz) reference_fasta: Reference sequence (fasta) samp_id: Sample ID outdir: Output directory (default is [sample_dir]/covid_genome_assembly) Note that this output looks identical to the information we put into $USAGE variable above in the second part of this section. Fourth, we want to read in the command with the provided input options. Because we are using a bash script, the position of the input files is imparative. #--- Read command line args [[ -n \"$1\" ]] && raw1=\"$1\" [[ -n \"$2\" ]] && raw2=\"$2\" [[ -n \"$3\" ]] && refFA=\"$3\" [[ -n \"$4\" ]] && sampid=\"$4\" [[ -n \"$5\" ]] && outdir=\"$5\" Fifth, we want to check that the input files are provided and are not empty. We also want to set the outdirectory. #--- Check that files are provided and exist [[ -z ${raw1+x} ]] && echo \"FAILED: read1 is not set\" && echo \"$USAGE\" && exit 1 [[ ! -e \"$raw1\" ]] && echo \"[---$SN---] ($(date)) FAILED: file $raw1 does not exist\" && exit 1 [[ -z ${raw2+x} ]] && echo \"FAILED: read2 is not set\" && echo \"$USAGE\" && exit 1 [[ ! -e \"$raw2\" ]] && echo \"[---$SN---] ($(date)) FAILED: file $raw2 does not exist\" && exit 1 [[ -z ${refFA+x} ]] && echo \"FAILED: refFA is not set\" && echo \"$USAGE\" && exit 1 [[ ! -e \"$refFA\" ]] && echo \"[---$SN---] ($(date)) FAILED: file $refFA does not exist\" && exit 1 [[ -z ${sampid+x} ]] && echo \"FAILED: sampid is not set\" && echo \"$USAGE\" && exit 1 #--- Set outdirectory [[ -z ${outdir+x} ]] && outdir=$(dirname $raw1)/$SN mkdir -p $outdir Sixth, we want to set the number of CPUs to use throughout the script. #--- Determine CPUs to use # First examines NCPU environment variable, then nproc, finally sets to 1 [[ -n \"$NCPU\" ]] && ncpu=$NCPU [[ -z $ncpu ]] && ncpu=$(nproc 2> /dev/null) [[ -z $ncpu ]] && ncpu=1 Seventh, we want to print out the variables and files. echo \"[---$SN---] ($(date)) read1: $raw1\" echo \"[---$SN---] ($(date)) read2: $raw2\" echo \"[---$SN---] ($(date)) reference_fasta: $refFA\" echo \"[---$SN---] ($(date)) samp_id: $sampid\" echo \"[---$SN---] ($(date)) outdir: $outdir\" echo \"[---$SN---] ($(date)) num CPU: $ncpu\" Finally, because here at GWU CBI, we like to time everything, we include a line of code to start a timer and end the timer: #--- Start the timer t1=$(date +\"%s\") #### Put haphpipe module scripts here #---Complete job t2=$(date +\"%s\") diff=$(($t2-$t1)) echo \"[---$SN---] ($(date)) $(($diff / 60)) minutes and $(($diff % 60)) seconds elapsed.\" echo \"[---$SN---] ($(date)) $SN COMPLETE.\" After Part 3A, our pipeline file should read as such: Code for beginning of pipeline #!/usr/bin/env bash ############################################################################### # This pipeline implements genome assembly using a denovo approach. Reads are # error-corrected and used to refine the scaffolded assembly, with up to 3 # refinement steps. This pipeline is used as an example for advanced usage # - making own pipeline in the User Guide. ############################################################################### SN='covid_genome_assembly' read -r -d '' USAGE <<EOF USAGE: $SN [read1] [read2] [reference_fasta] [samp_id] <outdir> ----- COVID19 Genome Assembly Pipeline ----- This pipeline implements genome assembly using a denovo approach. Reads are error-corrected and used to refine the scaffolded assembly, with up to 3 refinement steps. This pipeline is used as an example for advanced usage - making own pipeline in the User Guide. Input: read1: Fastq file for read 1. May be compressed (.gz) read2: Fastq file for read 2. May be compressed (.gz) reference_fasta: Reference sequence (fasta) samp_id: Sample ID outdir: Output directory (default is [sample_dir]/$SN) EOF #--- Read command line args and if arg is -h, provide the usage information [[ -n \"$1\" ]] && [[ \"$1\" == '-h' ]] && echo \"$USAGE\" && exit 0 #--- Read command line args [[ -n \"$1\" ]] && raw1=\"$1\" [[ -n \"$2\" ]] && raw2=\"$2\" [[ -n \"$3\" ]] && refFA=\"$3\" [[ -n \"$4\" ]] && refGTF=\"$4\" [[ -n \"$5\" ]] && sampid=\"$5\" [[ -n \"$6\" ]] && outdir=\"$6\" #--- Check that files are provided and exist [[ -z ${raw1+x} ]] && echo \"FAILED: read1 is not set\" && echo \"$USAGE\" && exit 1 [[ ! -e \"$raw1\" ]] && echo \"[---$SN---] ($(date)) FAILED: file $raw1 does not exist\" && exit 1 [[ -z ${raw2+x} ]] && echo \"FAILED: read2 is not set\" && echo \"$USAGE\" && exit 1 [[ ! -e \"$raw2\" ]] && echo \"[---$SN---] ($(date)) FAILED: file $raw2 does not exist\" && exit 1 [[ -z ${refFA+x} ]] && echo \"FAILED: refFA is not set\" && echo \"$USAGE\" && exit 1 [[ ! -e \"$refFA\" ]] && echo \"[---$SN---] ($(date)) FAILED: file $refFA does not exist\" && exit 1 [[ -z ${refGTF+x} ]] && echo \"FAILED: refGTF is not set\" && echo \"$USAGE\" && exit 1 [[ ! -e \"$refGTF\" ]] && echo \"[---$SN---] ($(date)) FAILED: file $refGTF does not exist\" && exit 1 [[ -z ${sampid+x} ]] && echo \"FAILED: sampid is not set\" && echo \"$USAGE\" && exit 1 #--- Set outdirectory [[ -z ${outdir+x} ]] && outdir=$(dirname $raw1)/$SN mkdir -p $outdir #--- Determine CPUs to use # First examines NCPU environment variable, then nproc, finally sets to 1 [[ -n \"$NCPU\" ]] && ncpu=$NCPU [[ -z $ncpu ]] && ncpu=$(nproc 2> /dev/null) [[ -z $ncpu ]] && ncpu=1 echo \"[---$SN---] ($(date)) read1: $raw1\" echo \"[---$SN---] ($(date)) read2: $raw2\" echo \"[---$SN---] ($(date)) reference_fasta: $refFA\" echo \"[---$SN---] ($(date)) reference_gtf: $refGTF\" echo \"[---$SN---] ($(date)) samp_id: $sampid\" echo \"[---$SN---] ($(date)) outdir: $outdir\" echo \"[---$SN---] ($(date)) num CPU: $ncpu\" #--- Start the timer t1=$(date +\"%s\") #### Put haphpipe module scripts here #---Complete job t2=$(date +\"%s\") diff=$(($t2-$t1)) echo \"[---$SN---] ($(date)) $(($diff / 60)) minutes and $(($diff % 60)) seconds elapsed.\" echo \"[---$SN---] ($(date)) $SN COMPLETE.\" -- Part 3B - Sample reads. Now we will begin constructing bash scripts for each module. Each module will follow a similar trend. We will set the module name. We will echo module name to terminal. We will check to make sure the files that are the output of the module are not already present. If they are present, we will skip the module and continue on (no need to repeat the module). If the files are not present, we will complete the module and call the command. If the command completes, we will print to terminal. If command fails, we will print that to terminal and quit the script. Remember, you can find the output file names here . The ouput names for this module are: sample_1.fastq and sample_2.fastq . We have also decided to subsample the number of reads to 50,000 reads for each sample. Now this in the input for the base haphpipe command for this module: haphpipe sample_reads\\ --fq1 $raw1\\ --fq2 $raw2\\ --nreads 50000\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} The entire module's bash script is here: module=\"sample_reads\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" # if the sampled files are present, skip this module. Otherwise, call sample_reads if [[ -e $outdir/sample_1.fastq && -e ${outdir}/sample_2.fastq ]]; then echo \"[---$SN---] ($(date)) EXISTS: $module sample_1.fastq, sample_2.fastq\" else # this reads in the sample_reads command and saves it in the variable cmd read -r -d '' cmd <<EOF haphpipe sample_reads\\ --fq1 $raw1\\ --fq2 $raw2\\ --nreads 50000\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) fi -- Part 3C - Trim reads. Remember, you can find the output file names here . The ouput names for this module are: trimmed_1.fastq and trimmed_2.fastq . Now, because we are doing trimming after the sampled reads module, we need to change the input fastq reads for this module to be the fastq reads output from the previous step (sample reads). Therefore, the input fastq files are named sample_1.fastq and sample_2.fastq . Also remember that these files are now contained in the outdirectory specified by the input, so we have to list the path to the input fastq files. Now this in the input for the base haphpipe command for this module: haphpipe trim_reads\\ --ncpu $ncpu\\ --fq1 ${outdir}/sample_1.fastq\\ --fq2 ${outdir}/sample_2.fastq\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} The entire module's bash script is here: module=\"trim_reads\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" if [[ -e $outdir/trimmed_1.fastq && -e ${outdir}/trimmed_2.fastq ]]; then echo \"[---$SN---] ($(date)) EXISTS: $module trimmed_1.fastq,trimmed_2.fastq\" else read -r -d '' cmd <<EOF haphpipe trim_reads\\ --ncpu $ncpu\\ --fq1 ${outdir}/sample_1.fastq\\ --fq2 ${outdir}/sample_2.fastq\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) fi -- Part 3D - Error correct reads. Remember, you can find the output file names here . The ouput names for this module are: corrected_1.fastq and corrected_2.fastq . Now, because we are doing error correction after the trimming module, we need to change the input fastq reads for this module to be the fastq reads output from the previous step (trimmed reads). Therefore, the input fastq files are named trimmed_1.fastq and trimmed_2.fastq . Also remember that these files are now contained in the outdirectory specified by the input (just like the sampled reads in the previous step), so we have to list the path to the input fastq files. Now this in the input for the base haphpipe command for this module: haphpipe ec_reads\\ --ncpu $ncpu\\ --fq1 ${outdir}/trimmed_1.fastq\\ --fq2 ${outdir}/trimmed_2.fastq\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} The entire module's bash script is here: module=\"ec_reads\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" if [[ -e $outdir/corrected_1.fastq && -e $outdir/corrected_2.fastq ]]; then echo \"[---$SN---] ($(date)) EXISTS: $module corrected_1.fastq,corrected_2.fastq\" else read -r -d '' cmd <<EOF haphpipe ec_reads\\ --ncpu $ncpu\\ --fq1 ${outdir}/trimmed_1.fastq\\ --fq2 ${outdir}/trimmed_2.fastq\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) fi -- Part 3E - De novo assembly. Remember, you can find the output file names here . The ouput name for this module is denovo_contigs.fna . Now, because we are doing denovo assembly after the error correction module, we need to change the input fastq reads for this module to be the fastq reads output from the previous step (error corrected reads). Therefore, the input fastq files are named corrected_1.fastq and corrected_2.fastq . Also remember that these files are now contained in the outdirectory specified by the input, so we have to list the path to the input fastq files. Finally, remember we want to specify that we do NOT want to do another round of error correction. Now this in the input for the base haphpipe command for this module: haphpipe assemble_denovo\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --no_error_correction\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} The entire module's bash script is here: module=\"assemble_denovo\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" if [[ -e $outdir/denovo_contigs.fna ]]; then echo \"[---$SN---] ($(date)) EXISTS: $module denovo_contigs.fna\" else read -r -d '' cmd <<EOF haphpipe assemble_denovo\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --no_error_correction\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) fi -- Part 3F - Assemble scaffold. Remember, you can find the output file names here . The ouput name for this module is scaffold_assembly.fa . There are other outputs, but we focus on this one for further use in the refinement and finalize modules. Now, because we are doing scaffold assembly after the denovo assembly module, we need to specify that the input file is the output contig file ( denovo_contigs.fna ). Again, remember that this file is now contained in the outdirectory specified by the input, so we have to list the path too. We also have to use our input reference fasta file and input sampleID from the script command. Now this in the input for the base haphpipe command for this module: haphpipe assemble_scaffold\\ --contigs_fa ${outdir}/denovo_contigs.fna\\ --ref_fa ${refFA}\\ --seqname ${sampid}\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} The entire module's bash script is here: module=\"assemble_scaffold\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" if [[ -e $outdir/scaffold_assembly.fa ]]; then echo \"[---$SN---] ($(date)) EXISTS: $module scaffold_assembly.fa\" else read -r -d '' cmd <<EOF haphpipe assemble_scaffold\\ --contigs_fa ${outdir}/denovo_contigs.fna\\ --ref_fa ${refFA}\\ --seqname ${sampid}\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) fi -- Part 3G - Refine assembly. Remember, you can find the output file names here . The ouput name for this module is refined.fna . Now, because we are doing refining the assembly after the scaffold assembly module, we need to change the input fastq reads for this module to be the fastq reads output from the error correction step (error corrected reads). Therefore, the input fastq files are named corrected_1.fastq and corrected_2.fastq . Also remember that these files are now contained in the outdirectory specified by the input, so we have to list the path to the input fastq files. Finally, we need to specify that the input reference file is the scaffold_assembly.fa file from the previous scaffold assembly step (above), which is also located in the outdirectory. Now this in the input for the base haphpipe command for this module: hp_refine_assembly\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --ref_fa ${outdir}/scaffold_assembly.fa\\ --sample_id ${sampid}\\ --max_step 5\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} The entire module's bash script is here: module=\"refine_assembly\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" if [[ -e ${outdir}/refined.fna ]]; then echo \"[---$SN---] ($(date)) EXISTS: $module refined.fna\" else read -r -d '' cmd <<EOF hp_refine_assembly\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --ref_fa ${outdir}/scaffold_assembly.fa\\ --sample_id ${sampid}\\ --max_step 5\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) fi -- Part 3H - Finalize assembly. Remember, you can find the output file names here . The ouput files for this module are: final.fna , final.bam , final.vcf.gz . Now, because we are doing finalizing the assembly after the refinement module, we need to change the input fastq reads for this module to be the fastq reads output from the error correction step (error corrected reads). Therefore, the input fastq files are named corrected_1.fastq and corrected_2.fastq . Also remember that these files are now contained in the outdirectory specified by the input, so we have to list the path to the input fastq files. Finally, we need to specify that the input reference file is the refined.fna file from the previous refinement step (above), which is also located in the outdirectory. Now this in the input for the base haphpipe command for this module: hp_finalize_assembly\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --sample_id ${sampid}\\ --ref_fa ${outdir}/refined.fna\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} The entire module's bash script is here: module=\"finalize_assembly\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" if [[ -e ${outdir}/final.fna && -e ${outdir}/final.bam && -e ${outdir}/final.vcf.gz ]]; then echo \"[---$SN---] ($(date)) EXISTS: $module final.fna,final.bam,final.vcf.gz\" else read -r -d '' cmd <<EOF hp_finalize_assembly\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --sample_id ${sampid}\\ --ref_fa ${outdir}/refined.fna\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) fi -- Part 3I - Gather all the individual module scripts into the final pipeline script. For readable code, we will separate each module with ### like so: ############################################################################### # Step #: description here ############################################################################### insert code here .. Once we concatenate all our code, we end up with this: code with all modules ############################################################################### # Step 1: Sample Reads ############################################################################### module=\"sample_reads\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" # if the sampled files are present, skip this module. Otherwise, call sample_reads if [[ -e $outdir/sample_1.fastq && -e ${outdir}/sample_2.fastq ]]; then echo \"[---$SN---] ($(date)) EXISTS: $module sample_1.fastq, sample_2.fastq\" else # this reads in the sample_reads command and saves it in the variable cmd read -r -d '' cmd <<EOF haphpipe sample_reads\\ --fq1 $raw1\\ --fq2 $raw2\\ --nreads 50000\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) fi ############################################################################### # Step 2: Trim Reads ############################################################################### module=\"trim_reads\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" if [[ -e $outdir/trimmed_1.fastq && -e ${outdir}/trimmed_2.fastq ]]; then echo \"[---$SN---] ($(date)) EXISTS: $module trimmed_1.fastq,trimmed_2.fastq\" else read -r -d '' cmd <<EOF haphpipe trim_reads\\ --ncpu $ncpu\\ --fq1 $raw1\\ --fq2 $raw2\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) fi ############################################################################### # Step 3: Error correction using Spades ############################################################################### module=\"ec_reads\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" if [[ -e $outdir/corrected_1.fastq && -e $outdir/corrected_2.fastq ]]; then echo \"[---$SN---] ($(date)) EXISTS: $module corrected_1.fastq,corrected_2.fastq\" else read -r -d '' cmd <<EOF haphpipe ec_reads\\ --ncpu $ncpu\\ --fq1 ${outdir}/trimmed_1.fastq\\ --fq2 ${outdir}/trimmed_2.fastq\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) fi ############################################################################### # Step 4: Denovo assembly ############################################################################### module=\"assemble_denovo\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" if [[ -e $outdir/denovo_contigs.fna ]]; then echo \"[---$SN---] ($(date)) EXISTS: $module denovo_contigs.fna\" else read -r -d '' cmd <<EOF haphpipe assemble_denovo\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --no_error_correction\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) fi ############################################################################### # Step 5: Scaffold assembly ############################################################################### module=\"assemble_scaffold\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" if [[ -e $outdir/scaffold_assembly.fa ]]; then echo \"[---$SN---] ($(date)) EXISTS: $module scaffold_assembly.fa\" else read -r -d '' cmd <<EOF haphpipe assemble_scaffold\\ --contigs_fa ${outdir}/denovo_contigs.fna\\ --ref_fa ${refFA}\\ --seqname ${sampid}\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) fi ############################################################################### # Step 5: Refine assembly ############################################################################### module=\"refine_assembly\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" if [[ -e ${outdir}/refined.fna ]]; then echo \"[---$SN---] ($(date)) EXISTS: $module refined.fna\" else read -r -d '' cmd <<EOF hp_refine_assembly\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --ref_fa ${outdir}/scaffold_assembly.fa\\ --sample_id ${sampid}\\ --max_step 5\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) fi ############################################################################### # Step 5: Finalize assembly ############################################################################### module=\"finalize_assembly\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" if [[ -e ${outdir}/final.fna && -e ${outdir}/final.bam && -e ${outdir}/final.vcf.gz ]]; then echo \"[---$SN---] ($(date)) EXISTS: $module final.fna,final.bam,final.vcf.gz\" else read -r -d '' cmd <<EOF hp_finalize_assembly\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --sample_id ${sampid}\\ --ref_fa ${outdir}/refined.fna\\ ${quiet} --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) fi Step 4 - Combine both the input code and bash scripts for each module into a single script. We named this script covid_genome_assembly.sh and the entire code is: `covid_genome_assembly.sh` #!/usr/bin/env bash ############################################################################### # This pipeline implements genome assembly using a denovo approach. Reads are # error-corrected and used to refine the scaffolded assembly, with up to 3 # refinement steps. This pipeline is used as an example for advanced usage # - making own pipeline in the User Guide. ############################################################################### SN='covid_genome_assembly' read -r -d '' USAGE <<EOF USAGE: $SN [read1] [read2] [reference_fasta] [samp_id] <outdir> ----- COVID19 Genome Assembly Pipeline ----- This pipeline implements genome assembly using a denovo approach. Reads are error-corrected and used to refine the scaffolded assembly, with up to 3 refinement steps. This pipeline is used as an example for advanced usage - making own pipeline in the User Guide. Input: read1: Fastq file for read 1. May be compressed (.gz) read2: Fastq file for read 2. May be compressed (.gz) reference_fasta: Reference sequence (fasta) samp_id: Sample ID outdir: Output directory (default is [sample_dir]/$SN) EOF #--- Read command line args and if arg is -h, provide the usage information [[ -n \"$1\" ]] && [[ \"$1\" == '-h' ]] && echo \"$USAGE\" && exit 0 #--- Read command line args [[ -n \"$1\" ]] && raw1=\"$1\" [[ -n \"$2\" ]] && raw2=\"$2\" [[ -n \"$3\" ]] && refFA=\"$3\" [[ -n \"$4\" ]] && sampid=\"$4\" [[ -n \"$5\" ]] && outdir=\"$5\" #--- Check that files are provided and exist [[ -z ${raw1+x} ]] && echo \"FAILED: read1 is not set\" && echo \"$USAGE\" && exit 1 [[ ! -e \"$raw1\" ]] && echo \"[---$SN---] ($(date)) FAILED: file $raw1 does not exist\" && exit 1 [[ -z ${raw2+x} ]] && echo \"FAILED: read2 is not set\" && echo \"$USAGE\" && exit 1 [[ ! -e \"$raw2\" ]] && echo \"[---$SN---] ($(date)) FAILED: file $raw2 does not exist\" && exit 1 [[ -z ${refFA+x} ]] && echo \"FAILED: refFA is not set\" && echo \"$USAGE\" && exit 1 [[ ! -e \"$refFA\" ]] && echo \"[---$SN---] ($(date)) FAILED: file $refFA does not exist\" && exit 1 [[ -z ${sampid+x} ]] && echo \"FAILED: sampid is not set\" && echo \"$USAGE\" && exit 1 #--- Set outdirectory [[ -z ${outdir+x} ]] && outdir=$(dirname $raw1)/$SN mkdir -p $outdir #--- Determine CPUs to use # First examines NCPU environment variable, then nproc, finally sets to 1 [[ -n \"$NCPU\" ]] && ncpu=$NCPU [[ -z $ncpu ]] && ncpu=$(nproc 2> /dev/null) [[ -z $ncpu ]] && ncpu=1 echo \"[---$SN---] ($(date)) read1: $raw1\" echo \"[---$SN---] ($(date)) read2: $raw2\" echo \"[---$SN---] ($(date)) reference_fasta: $refFA\" echo \"[---$SN---] ($(date)) samp_id: $sampid\" echo \"[---$SN---] ($(date)) outdir: $outdir\" echo \"[---$SN---] ($(date)) num CPU: $ncpu\" #--- Start the timer t1=$(date +\"%s\") ############################################################################### # Step 1: Sample Reads ############################################################################### module=\"sample_reads\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" # if the sampled files are present, skip this module. Otherwise, call sample_reads if [[ -e $outdir/sample_1.fastq && -e ${outdir}/sample_2.fastq ]]; then echo \"[---$SN---] ($(date)) EXISTS: $module sample_1.fastq, sample_2.fastq\" else # this reads in the sample_reads command and saves it in the variable cmd read -r -d '' cmd <<EOF haphpipe sample_reads\\ --fq1 $raw1\\ --fq2 $raw2\\ --nreads 50000\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) fi ############################################################################### # Step 2: Trim Reads ############################################################################### module=\"trim_reads\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" if [[ -e $outdir/trimmed_1.fastq && -e ${outdir}/trimmed_2.fastq ]]; then echo \"[---$SN---] ($(date)) EXISTS: $module trimmed_1.fastq,trimmed_2.fastq\" else read -r -d '' cmd <<EOF haphpipe trim_reads\\ --ncpu $ncpu\\ --fq1 $raw1\\ --fq2 $raw2\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) fi ############################################################################### # Step 3: Error correction using Spades ############################################################################### module=\"ec_reads\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" if [[ -e $outdir/corrected_1.fastq && -e $outdir/corrected_2.fastq ]]; then echo \"[---$SN---] ($(date)) EXISTS: $module corrected_1.fastq,corrected_2.fastq\" else read -r -d '' cmd <<EOF haphpipe ec_reads\\ --ncpu $ncpu\\ --fq1 ${outdir}/trimmed_1.fastq\\ --fq2 ${outdir}/trimmed_2.fastq\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) fi ############################################################################### # Step 4: Denovo assembly ############################################################################### module=\"assemble_denovo\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" if [[ -e $outdir/denovo_contigs.fna ]]; then echo \"[---$SN---] ($(date)) EXISTS: $module denovo_contigs.fna\" else read -r -d '' cmd <<EOF haphpipe assemble_denovo\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --no_error_correction\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) fi ############################################################################### # Step 5: Scaffold assembly ############################################################################### module=\"assemble_scaffold\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" if [[ -e $outdir/scaffold_assembly.fa ]]; then echo \"[---$SN---] ($(date)) EXISTS: $module scaffold_assembly.fa\" else read -r -d '' cmd <<EOF haphpipe assemble_scaffold\\ --contigs_fa ${outdir}/denovo_contigs.fna\\ --ref_fa ${refFA}\\ --seqname ${sampid}\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) fi ############################################################################### # Step 5: Refine assembly ############################################################################### module=\"refine_assembly\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" if [[ -e ${outdir}/refined.fna ]]; then echo \"[---$SN---] ($(date)) EXISTS: $module refined.fna\" else read -r -d '' cmd <<EOF hp_refine_assembly\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --ref_fa ${outdir}/scaffold_assembly.fa\\ --sample_id ${sampid}\\ --max_step 5\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) fi ############################################################################### # Step 5: Finalize assembly ############################################################################### module=\"finalize_assembly\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" if [[ -e ${outdir}/final.fna && -e ${outdir}/final.bam && -e ${outdir}/final.vcf.gz ]]; then echo \"[---$SN---] ($(date)) EXISTS: $module final.fna,final.bam,final.vcf.gz\" else read -r -d '' cmd <<EOF hp_finalize_assembly\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --sample_id ${sampid}\\ --ref_fa ${outdir}/refined.fna\\ ${quiet} --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) fi #### Put haphpipe module scripts here #---Complete job t2=$(date +\"%s\") diff=$(($t2-$t1)) echo \"[---$SN---] ($(date)) $(($diff / 60)) minutes and $(($diff % 60)) seconds elapsed.\" echo \"[---$SN---] ($(date)) $SN COMPLETE.\" Step 5 - Executing the script. Example to run: bash covid_genome_assembly.sh SRR11140750/SRR11140750_1.fastq SRR11140750/SRR11140750_2.fastq SARSCoV2.NC_045512.COVID19.fasta SRR11140750 Example to run in a loop over all the samples: for sra in SRR11140744 SRR11140746 SRR11140748 SRR11140750; do bash covid_genome_assembly.sh ${sra}/${sra}_1.fastq ${sra}/${sra}_2.fastq SARSCoV2.NC_045512.COVID19.fasta ${sra} covid_genome_assembly done Directories should look like such after running this script: Directory structure . \u251c\u2500\u2500 SRR11140744 | \u251c\u2500\u2500 SRR11140744_1.fastq | \u251c\u2500\u2500 SRR11140744_2.fastq | \u2514\u2500\u2500 covid_genome_assembly | \u251c\u2500\u2500 corrected_1.fastq | \u251c\u2500\u2500 corrected_2.fastq | \u251c\u2500\u2500 corrected_U.fastq | \u251c\u2500\u2500 denovo_contigs.fna | \u251c\u2500\u2500 denovo_summary.txt | \u251c\u2500\u2500 haphpipe.out | \u251c\u2500\u2500 final.bam | \u251c\u2500\u2500 final.bam.bai | \u251c\u2500\u2500 final_bt2.out | \u251c\u2500\u2500 final.fna | \u251c\u2500\u2500 final.vcf.gz | \u251c\u2500\u2500 final.vcf.gz.tbi | \u251c\u2500\u2500 refined.01.fna | \u251c\u2500\u2500 refined_bt2.01.out | \u251c\u2500\u2500 refined.fna | \u251c\u2500\u2500 refined_bt2.out | \u251c\u2500\u2500 refined_summary.out | \u251c\u2500\u2500 sample_1.fastq | \u251c\u2500\u2500 sample_2.fastq | \u251c\u2500\u2500 scaffold_aligned.fa | \u251c\u2500\u2500 scaffold_assembly.fa | \u251c\u2500\u2500 scaffold_imputed.fa | \u251c\u2500\u2500 scaffold_padded.out | \u251c\u2500\u2500 trimmed_1.fastq | \u251c\u2500\u2500 trimmed_2.fastq | \u251c\u2500\u2500 trimmed_U.fastq | \u2514\u2500\u2500 trimmomatic_summary.out \u251c\u2500\u2500 SRR11140746 | \u251c\u2500\u2500 SRR11140746_1.fastq | \u251c\u2500\u2500 SRR11140746_2.fastq | \u2514\u2500\u2500 covid_genome_assembly .... Step 6 - Optional inclusions: PredictHaplo, multiple alignment, and building a phylogenetic tree. Here are some option modules to include within the pipeline. -- Adding PredictHaplo as a module. If you desire PredictHaplo, you can either utilize the option --interval_txt or you can rerun the pipeline with this gtf file. It is easier for PredictHaplo to run on smaller regions than the entire genome like we implemented above. $ cat SARSCoV2.NC_045512.COVID19.gtf SARSCoV2.NC_045512.COVID19 NCBI_refseq amplicon 265 13482 . + 0 name \"orf1a\"; SARSCoV2.NC_045512.COVID19 NCBI_refseq amplicon 21562 25383 . + 0 name \"surface\"; SARSCoV2.NC_045512.COVID19 NCBI_refseq amplicon 26244 26471 . + 0 name \"envelope\"; Remember, you can find the output file names here . The ouput name for this module is PH0#.best*.fas . We are doing this module after the finalize_assembly module, doing refining the assembly after the scaffold assembly module. Therefore, the input fastq files are named corrected_1.fastq and corrected_2.fastq . Also remember that these files are now contained in the outdirectory specified by the input, so we have to list the path to the input fastq files. Finally, we need to specify that the input reference file is the final.fna file from the finalize assembly module, which is also located in the outdirectory. We have to do loops for the PredictHaplo and parser modules because there are multiple haplotype regions. Now this in the input for the base haphpipe command for this module: hp_predict_haplo --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --ref_fa ${outdir}/final.fna\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} The entire module's bash script is here: predict_haplo module module=\"predict_haplo\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" for PH in ${outdir}/PH*; do if [[ -e \"${PH}\" ]]; then for PHbest in ${outdir}/PH*/*best*.fas; do if [[ -e \"${PHbest}\" ]]; then echo \"[---$SN---] ($(date)) EXISTS: $module $PHbest\" fi done else read -r -d '' cmd <<EOF hp_predict_haplo --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --ref_fa ${outdir}/final.fna\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) fi done -- Following up PredictHaplo with ph_parser In order to use the output from PredictHaplo (see description here ), we need to run the ph_parser module. hp_ph_parser --haplotypes_fa ${outdir}/${PH}/*best*.fas\\ --prefix ${sampID}_${PH}\\ --logfile ${outdir}/${PH}/haphpipe.out\\ --outdir ${outdir} Now we have to do a loop, because there are multiple regions (i.e., PH0# directories). The entire module's bash script is here: ph_parser module module=\"ph_parser\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" for PH in ${outdir}/PH*/ph_haplotypes.fna; do #if [[ -e \"${PH}\" ]]; then # echo \"[---$SN---] ($(date)) EXISTS: $module $PH\" #else read -r -d '' cmd <<EOF hp_ph_parser --haplotypes_fa $(dirname $PH)/*best*.fas\\ --prefix ${sampID}_$(dirname $(basename $PH))\\ --logfile ${outdir}/haphpipe.out\\ --outdir $(dirname $PH) EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) #fi done -- Adding multiple_align as a module Remember, you can find the output file names here . The ouput name for this module is alignment.fasta . We first need to make a text file that has a list of directories that contin final.fna . Because this is a genome assembly, we want to use the --alignall option to align the entire region and not use a GTF file. We also choose to output a phylip file using the option --phyipout . Now this in the input for the base haphpipe command for this module: haphpipe multiple_align --ncpu $ncpu\\ --dir_list dir_list.txt\\ --phylipout\\ --alignall\\ --logfile haphpipe.out The entire module's bash script is here: multiple_align module module=\"multiple_align\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" if [[ -e $outdir/hp_multiple_align/alignment.fasta ]] && [[ -e $outdir/hp_multiple_align/alignment.phy ]]; then echo \"[---$SN---] ($(date)) EXISTS: $module alignment.fasta,alignment.phy\" else read -r -d '' cmd <<EOF haphpipe multiple_align\\ --ncpu $ncpu\\ --dir_list dir_list.txt\\ --phylipout\\ --alignall\\ --logfile haphpipe.out EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) fi -- Adding model_test as a module Remember, you can find the output file names here . The ouput name for this module is modeltest_results.out . The input will be the alignment file from multiple_align . We'll also give the output an id of covid19_genome so the output file will be covid19_genome_modeltest_results.out . We will also chose the template of the output as raxml, since the next module build_tree uses RAxML. Now this in the input for the base haphpipe command for this module: haphpipe model_test --seqs ${outdir}/hp_multiple_align/alignment.fasta\\ --run_id covid_genome\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir}\\ --template raxml\\ --ncpu ${ncpu} The entire module's bash script is here: model_test module module=\"model_test\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" if [[ -e $outdir/covid19_genome_modeltest_results.out ]]; then echo \"[---$SN---] ($(date)) EXISTS: $module covid19_genome_modeltest_results.out\" else read -r -d '' cmd <<EOF haphpipe model_test --seqs ${outdir}/hp_multiple_align/alignment.fasta\\ --run_id covid_genome\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir}\\ --template raxml\\ --ncpu ${ncpu} EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) fi -- Adding build_tree as a module Remember, you can find the output file names here . The ouput name for this module is modeltest_results.out . The input will be the alignment file from multiple_align . We'll also give the output an id of covid19_genome so the output file will be covid19_genome_modeltest_results.out . Now this in the input for the base haphpipe command for this module: haphpipe build_tree --run_full_analysis\\ --seqs ${outdir}/hp_multiple_align/alignment.fasta\\ --run_id covid_genome.tre\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir}\\ --model GTRCATX The entire module's bash script is here: build_tree module module=\"model_test\" echo -e \"\\n[---$SN---] ($(date)) module: $module\" if [[ -e $outdir/hp_build_tree/RAxML_bipartitionsBranchLabels.covid_genome.tre ]]; then echo \"[---$SN---] ($(date)) EXISTS: $module RAxML_bipartitionsBranchLabels.covid_genome.tre\" else read -r -d '' cmd <<EOF haphpipe build_tree --run_full_analysis\\ --seqs ${outdir}/hp_multiple_align/alignment.fasta\\ --run_id covid_genome.tre\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir}\\ --model GTRCATX EOF echo -e \"[---$SN---] ($(date)) $module command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $module\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $module\" && exit 1 ) fi Finally, the tree file RAxML_bipartitionsBranchLabels.covid_genome.tre can be viewed in and annotated in programs such as FigTree or iTOL .","title":"Making your own pipelines with Bash"},{"location":"adv/#snakemake","text":"Advanced users may also be interested in constructing a Snakemake workflow. See the Snakemake documentation for details.","title":"Snakemake"},{"location":"demos/","text":"Demo Module After successful installation, the demo dataset can be run to ensure HAPHPIPE is installed and set up correctly. These are the steps completed in the demo: Download Sample SRR8525886 using fastq-dump Run haphpipe_assemble_02 If PredictHaplo is installed, run PredictHaplo Download Sample SRR8525933 using fastq-dump Run haphpipe_assemble_02 If PredictHaplo is installed, run PredictHaplo Download Sample SRR8525938 using fastq-dump Run haphpipe_assemble_02 If PredictHaplo is installed, run PredictHaplo Download Sample SRR8525939 using fastq-dump Run haphpipe_assemble_02 If PredictHaplo is installed, run PredictHaplo Download Sample SRR8525940 using fastq-dump Run haphpipe_assemble_02 If PredictHaplo is installed, run PredictHaplo Run multiple sequence alignment with MAFFT Run ModelTest with modeltest-NG Buid a phylogenetic tree with RAxML We split up the samples, instead of looping through them for fastq-dump because sometimes the fastq-dump/NCBI doesn't work. It was more helpful to see when a sample failed this way rather than a loop for fastq-dump then a loop for running haphpipe_assemble_02 . If fastq-dump doesn't work, check your wifi connection, sometimes HPCs have trouble downloading SRA data with the connection. Downloading on own computer should work just fine. If for some reason the fastq-dump doesn't work, you can download directly from the SRA database on a webpage. Description of data used Accession numbers: SRR8525886, SRR8525933, SRR8525938, SRR8525939,and SRR8525940 These data were first presented in this paper . The SRA project data can be found here . This data was a part of a cohort study. The viral samples were amplified for three gene regions: PRRT , int , and env . This data was analyzed using HAPHPIPE and PredictHaplo in this paper . Results will vary from this 2020 cross-sectional study because here, for the demo, we only use 10,000 reads for each of these 5 samples compared to the study which uses all the available NGS reads. Running Demo Automatically Running the demo is simple and requires a single command: hp_demo or haphpipe demo A specific outdirectory can be specified by: hp_demo --outdir $outdir_name The output of the entire demo is as such If running the entire demo is not desired, this command can be executed to just pull the references included in HAPHPIPE into the directory that is specified (default outdir is . ). hp_demo --refonly Output on the terminal is as such, and the three HIV reference files are located in the subdirectory refs . See the User Guide for more information regarding these reference files. /base/directory/path/of/haphpipe) Demo was run with --refonly. References are now in outdirectory: $outdir_name/haphpipe_demo/refs. Output files If the entire demo is run (i.e., no use of --refonly ) then these are all the files for output if PredictHaplo is NOT installed . The files are listed for a single sample as example, not all 5 samples. The other files from multiple_align , model_test , and build_tree are all listed. Directory structure without PH haphpipe_demo \u251c\u2500\u2500 SRR8525886 | \u251c\u2500\u2500 SRR8525886_1.fastq | \u251c\u2500\u2500 SRR8525886_2.fastq | \u2514\u2500\u2500 haphpipe_assemble_02 | \u251c\u2500\u2500 corrected_1.fastq | \u251c\u2500\u2500 corrected_2.fastq | \u251c\u2500\u2500 corrected_U.fastq | \u251c\u2500\u2500 final.bam | \u251c\u2500\u2500 final.bam.bai | \u251c\u2500\u2500 final_bt2.out | \u251c\u2500\u2500 final.fna | \u251c\u2500\u2500 final.vcf.gz | \u251c\u2500\u2500 final.vcf.gz.tbi | \u251c\u2500\u2500 haphpipe.out | \u251c\u2500\u2500 refined.01.fna | \u251c\u2500\u2500 refined.02.fna | \u251c\u2500\u2500 refined.03.fna | \u251c\u2500\u2500 refined.fna | \u251c\u2500\u2500 refined_bt2.01.out | \u251c\u2500\u2500 refined_bt2.02.out | \u251c\u2500\u2500 refined_bt2.03.out | \u251c\u2500\u2500 refined_bt2.out | \u251c\u2500\u2500 refined_summary.out | \u251c\u2500\u2500 trimmed_1.fastq | \u251c\u2500\u2500 trimmed_2.fastq | \u251c\u2500\u2500 trimmed_U.fastq | \u2514\u2500\u2500 trimmomatic_summary.out | \u251c\u2500\u2500 dir_list.txt \u251c\u2500\u2500 haphpipe.out \u251c\u2500\u2500 hp_multiple_align | \u251c\u2500\u2500 alignment_region00.fasta | \u251c\u2500\u2500 alignment_region00.phy | \u251c\u2500\u2500 alignment_region01.fasta | \u251c\u2500\u2500 alignment_region01.phy | \u251c\u2500\u2500 alignment_region02.fasta | \u251c\u2500\u2500 alignment_region02.phy | \u251c\u2500\u2500 all_sequences.fasta | \u251c\u2500\u2500 all_sequences_region00.fasta | \u251c\u2500\u2500 all_sequences_region01.fasta | \u2514\u2500\u2500 all_sequences_region02.fasta | \u251c\u2500\u2500 alignment_region00_modeltest_results.out \u251c\u2500\u2500 alignment_region00_modeltest_results_summary.tsv \u251c\u2500\u2500 alignment_region01_modeltest_results.out \u251c\u2500\u2500 alignment_region01_modeltest_results_summary.tsv \u251c\u2500\u2500 alignment_region02_modeltest_results.out \u251c\u2500\u2500 alignment_region02_modeltest_results_summary.tsv | \u251c\u2500\u2500 hp_build_tree | \u251c\u2500\u2500 RAxML_bestTree.alignment_region00 | \u251c\u2500\u2500 RAxML_bestTree.alignment_region01 | \u251c\u2500\u2500 RAxML_bipartitions.alignment_region00 | \u251c\u2500\u2500 RAxML_bipartitions.alignment_region01 | \u251c\u2500\u2500 RAxML_bipartitionsBranchLabels.alignment_region00 | \u251c\u2500\u2500 RAxML_bipartitionsBranchLabels.alignment_region01 | \u251c\u2500\u2500 RAxML_bootstrap.alignment_region00 | \u251c\u2500\u2500 RAxML_bootstrap.alignment_region01 | \u251c\u2500\u2500 RAxML_info.alignment_region00 | \u2514\u2500\u2500 RAxML_info.alignment_region01 | \u251c\u2500\u2500 sample# | \u251c\u2500\u2500 sample#_1.fastq | \u251c\u2500\u2500 sample#_2.fastq | \u2514\u2500\u2500 haphpipe_assemble_02 .... If the entire demo is run (i.e., no use of --refonly ) then these are all the files for output if PredictHaplo is installed. Directory structure with PH haphpipe_demo \u251c\u2500\u2500 SRR8525886 | \u251c\u2500\u2500 SRR8525886_1.fastq | \u251c\u2500\u2500 SRR8525886_2.fastq | \u2514\u2500\u2500 haphpipe_assemble_02 | \u251c\u2500\u2500 corrected_1.fastq | \u251c\u2500\u2500 corrected_2.fastq | \u251c\u2500\u2500 corrected_U.fastq | \u251c\u2500\u2500 final.bam | \u251c\u2500\u2500 final.bam.bai | \u251c\u2500\u2500 final_bt2.out | \u251c\u2500\u2500 final.fna | \u251c\u2500\u2500 final.vcf.gz | \u251c\u2500\u2500 final.vcf.gz.tbi | \u251c\u2500\u2500 haphpipe.out | \u251c\u2500\u2500 refined.01.fna | \u251c\u2500\u2500 refined.02.fna | \u251c\u2500\u2500 refined.03.fna | \u251c\u2500\u2500 refined.fna | \u251c\u2500\u2500 refined_bt2.01.out | \u251c\u2500\u2500 refined_bt2.02.out | \u251c\u2500\u2500 refined_bt2.03.out | \u251c\u2500\u2500 refined_bt2.out | \u251c\u2500\u2500 refined_summary.out | \u251c\u2500\u2500 trimmed_1.fastq | \u251c\u2500\u2500 trimmed_2.fastq | \u251c\u2500\u2500 trimmed_U.fastq | \u251c\u2500\u2500 trimmomatic_summary.out | \u251c\u2500\u2500 ph_haplotypes_comb.fna | \u251c\u2500\u2500 PH01_PRRT | | \u251c\u2500\u2500 PH01_PRRT.best_1_1197.fas | | \u251c\u2500\u2500 PH01_PRRT.best_1_1197.html | | \u251c\u2500\u2500 PH01_PRRT.config.log | | \u251c\u2500\u2500 ph_haplotypes.fna | | \u2514\u2500\u2500 ph_summary.txt | \u251c\u2500\u2500 PH02_INT | | \u251c\u2500\u2500 PH02_INT.best_1_964.fas | | \u251c\u2500\u2500 PH02_INT.best_1_964.html | | \u251c\u2500\u2500 PH02_INT.config.log | | \u251c\u2500\u2500 ph_haplotypes.fna | | \u2514\u2500\u2500 ph_summary.txt | \u2514\u2500\u2500 PH03_gp120 | \u251c\u2500\u2500 PH03_gp120.best_1_1641.fas | \u251c\u2500\u2500 PH03_gp120.best_1_1641.html | \u251c\u2500\u2500 PH03_gp120.config.log | \u251c\u2500\u2500 ph_haplotypes.fna | \u2514\u2500\u2500 ph_summary.txt | \u251c\u2500\u2500 dir_list.txt \u251c\u2500\u2500 haphpipe.out \u251c\u2500\u2500 hp_multiple_align | \u251c\u2500\u2500 alignment_region00.fasta | \u251c\u2500\u2500 alignment_region00.phy | \u251c\u2500\u2500 alignment_region01.fasta | \u251c\u2500\u2500 alignment_region01.phy | \u251c\u2500\u2500 alignment_region02.fasta | \u251c\u2500\u2500 alignment_region02.phy | \u251c\u2500\u2500 all_sequences.fasta | \u251c\u2500\u2500 all_sequences_region00.fasta | \u251c\u2500\u2500 all_sequences_region01.fasta | \u2514\u2500\u2500 all_sequences_region02.fasta | \u251c\u2500\u2500 alignment_region00_modeltest_results.out \u251c\u2500\u2500 alignment_region00_modeltest_results_summary.tsv \u251c\u2500\u2500 alignment_region01_modeltest_results.out \u251c\u2500\u2500 alignment_region01_modeltest_results_summary.tsv \u251c\u2500\u2500 alignment_region02_modeltest_results.out \u251c\u2500\u2500 alignment_region02_modeltest_results_summary.tsv | \u251c\u2500\u2500 hp_build_tree | \u251c\u2500\u2500 RAxML_bestTree.alignment_region00 | \u251c\u2500\u2500 RAxML_bestTree.alignment_region01 | \u251c\u2500\u2500 RAxML_bestTree.alignment_region01 | \u251c\u2500\u2500 RAxML_bipartitions.alignment_region00 | \u251c\u2500\u2500 RAxML_bipartitions.alignment_region01 | \u251c\u2500\u2500 RAxML_bipartitions.alignment_region02 | \u251c\u2500\u2500 RAxML_bipartitionsBranchLabels.alignment_region00 | \u251c\u2500\u2500 RAxML_bipartitionsBranchLabels.alignment_region01 | \u251c\u2500\u2500 RAxML_bipartitionsBranchLabels.alignment_region02 | \u251c\u2500\u2500 RAxML_bootstrap.alignment_region00 | \u251c\u2500\u2500 RAxML_bootstrap.alignment_region01 | \u251c\u2500\u2500 RAxML_bootstrap.alignment_region02 | \u251c\u2500\u2500 RAxML_info.alignment_region00 | \u251c\u2500\u2500 RAxML_info.alignment_region01 | \u2514\u2500\u2500 RAxML_info.alignment_region02 | \u251c\u2500\u2500 sample# | \u251c\u2500\u2500 sample#_1.fastq | \u251c\u2500\u2500 sample#_2.fastq | \u2514\u2500\u2500 haphpipe_assemble_02 .... Running Demo Interactively The demo pipeline bash script: haphpipe_demo #!/usr/bin/env bash # Copyright (C) 2020 Keylie M. Gibson # This program is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation, either version 3 of the License, or # (at your option) any later version. # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # You should have received a copy of the GNU General Public License # along with this program. If not, see <http://www.gnu.org/licenses/>. ############################################################################### # Demo pipeline implementing amplicon assembly using a reference-based approach #(haphpipe_assemble_02). Reads are error-corrected and used to refine # the initial assembly, with up to 5 refinement steps. PredictHaplo runs if it # is loaded, and multiple sequence alignment is conducted with the final.fna # (and haplotypes if there). ############################################################################### SN=$(basename $0) read -r -d '' USAGE <<EOF USAGE: $SN <outdir> ----- HAPHPIPE demo ----- This demo implements amplicon assembly using a reference-based approach. Five SRA samples are pulled with fastq-dump, assembled with haphpipe_assemble_02, multiple aligned and a phylogeny estimated. If PredictHaplo is installed, haplotypes are also predicted followed by MSA and a phylogeny estimated. outdir: Output directory (default is demo) EOF #--- Read command line args [[ -n \"$1\" ]] && [[ \"$1\" == '-h' ]] && echo \"$USAGE\" && exit 0 [[ -n \"$1\" ]] && outdir=\"$1\" [[ -z ${outdir+x} ]] && outdir=$SN mkdir -p $outdir/refs #--- Determine CPUs to use # First examines NCPU environment variable, then nproc, finally sets to 1 [[ -n \"$NCPU\" ]] && ncpu=$NCPU [[ -z $ncpu ]] && ncpu=$(nproc 2> /dev/null) [[ -z $ncpu ]] && ncpu=1 #--- Determine whether verbose [[ -n \"$VERBOSE\" ]] && quiet=\"\" || quiet='--quiet' echo \"[---$SN---] ($(date)) outdir: $outdir\" echo \"[---$SN---] ($(date)) num CPU: $ncpu\" #--- Start the timer t1=$(date +\"%s\") ### We split up the samples, instead of looping through them for fastq-dump ### ### because sometimes the fastq-dump/NCBI doesn't work. It was more helpful ### ### to see when a sample failed this way rather than a loop for fastq-dump ### ### then a loop for running haphpipe_assemble_02. ### ############################################################################### # Step 2a: Sample 1: SRR8525886 ############################################################################### # Step 1: Fastq-dump stage=\"Demo Sample 1: SRR8525886\" sra='SRR8525886' echo -e \"\\n[---$SN---] ($(date)) Stage: $stage\" # checking for the 5 directories and fastq files if [[ -e ${outdir}/${sra}/${sra}_1.fastq ]] &&\\ [[ -e ${outdir}/${sra}/${sra}_2.fastq ]]; then echo \"[---$SN---] ($(date)) EXISTS: Demo Sample ${sra} paired fastq files\" else read -r -d '' cmd1 <<EOF fastq-dump --outdir ${outdir}/${sra} \\ --split-files\\ --origfmt\\ --minSpotId 30000\\ --maxSpotId 40000\\ ${sra} EOF echo -e \"[---$SN---] ($(date)) $stage for ${sra} command:\\n\\n$cmd1\\n\" eval $cmd1 [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $stage\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $stage fastq-dump failed for sample ${sra}\" && exit 1 ) fi if [[ ! -e ${outdir}/${sra}/${sra}_1.fastq ]] &&\\ [[ ! -e ${outdir}/${sra}/${sra}_2.fastq ]]; then echo \"[---$SN---] ($(date)) FAILED: file downloading SRA data for ${sra} did not complete\" && exit 1 fi # Step 2: Haphpipe_assemble_02 pipeline=\"haphpipe_assemble_02\" echo -e \"\\n[---$SN---] ($(date)) Stage: $pipeline\" # checking for both fastq files and final.fna if [[ -e ${outdir}/${sra}/${sra}_1.fastq ]] &&\\ [[ -e ${outdir}/${sra}/${sra}_2.fastq ]] &&\\ [[ -e $outdir/${sra}/${pipeline}/final.fna ]]; then echo \"[---$SN---] ($(date)) EXISTS: $pipeline final.fna for sample ${sra}\" else read -r -d '' cmd2 <<EOF ${pipeline} ${outdir}/${sra}/${sra}_1.fastq\\ ${outdir}/${sra}/${sra}_2.fastq\\ ${outdir}/refs/HIV_B.K03455.HXB2.amplicons.fasta\\ ${sra}\\ ${outdir}/${sra}/${pipeline} EOF echo -e \"[---$SN---] ($(date)) $pipeline command for ${sra}:\\n\\n$cmd2\\n\" eval $cmd2 [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $pipeline for ${sra} samples\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $pipeline\" && exit 1 ) fi # Step 3: PredictHaplo stage=\"predict_haplo\" echo -e \"\\n[---$SN---] ($(date)) Stage: $stage\" if [[ -e ${outdir}/${sra}/${pipeline}/ph_haplotypes_comb.fna ]]; then echo \"[---$SN---] ($(date)) EXISTS: $stage corrected_1.fastq,corrected_2.fastq\" else command -v PredictHaplo-Paired >/dev/null 2>&1 || if [[ $? -eq 0 ]] ; then echo -e \"[---$SN---] ($(date)) $stage PredictHaplo present\" read -r -d '' cmd1 <<EOF haphpipe predict_haplo\\ --fq1 ${outdir}/${sra}/${pipeline}/corrected_1.fastq\\ --fq2 ${outdir}/${sra}/${pipeline}/corrected_2.fastq\\ --ref_fa ${outdir}/${sra}/${pipeline}/final.fna\\ --logfile ${outdir}/${sra}/${pipeline}/haphpipe_PH.out\\ --outdir ${outdir}/${sra}/${pipeline} EOF echo -e \"[---$SN---] ($(date)) $stage command:\\n\\n$cmd1\\n\" eval $cmd1 for ph in ${outdir}/${sra}/${pipeline}/PH*; do read -r -d '' cmd2 <<EOF haphpipe ph_parser\\ --haplotypes_fa $ph/PH*.best_*.fas\\ --logfile ${outdir}/${sra}/${pipeline}/haphpipe_PH.out\\ --outdir ${ph}\\ --prefix ${sra}_$(basename $ph) EOF echo -e \"[---$SN---] ($(date)) $stage command:\\n\\n$cmd2\\n\" eval $cmd2 cat ${outdir}/${sra}/${pipeline}/PH*/ph_haplotypes.fna > ${outdir}/${sra}/${pipeline}/ph_haplotypes_comb.fna done fi [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $stage\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $stage\" && exit 1 ) fi ############################################################################### # Step 2b: Sample 2: SRR8525933 ############################################################################### # Step 1: Fastq-dump stage=\"Demo Sample 2: SRR8525933\" sra='SRR8525933' echo -e \"\\n[---$SN---] ($(date)) Stage: $stage\" # checking for the 5 directories and fastq files if [[ -e ${outdir}/${sra}/${sra}_1.fastq ]] &&\\ [[ -e ${outdir}/${sra}/${sra}_2.fastq ]]; then echo \"[---$SN---] ($(date)) EXISTS: Demo Sample ${sra} paired fastq files\" else read -r -d '' cmd1 <<EOF fastq-dump --outdir ${outdir}/${sra} \\ --split-files\\ --origfmt\\ --minSpotId 30000\\ --maxSpotId 40000\\ ${sra} EOF echo -e \"[---$SN---] ($(date)) $stage for ${sra} command:\\n\\n$cmd1\\n\" eval $cmd1 [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $stage\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $stage fastq-dump failed for sample ${sra}\" && exit 1 ) fi if [[ ! -e ${outdir}/${sra}/${sra}_1.fastq ]] &&\\ [[ ! -e ${outdir}/${sra}/${sra}_2.fastq ]]; then echo \"[---$SN---] ($(date)) FAILED: file downloading SRA data for ${sra} did not complete\" && exit 1 fi # Step 2: Haphpipe_assemble_02 pipeline=\"haphpipe_assemble_02\" echo -e \"\\n[---$SN---] ($(date)) Stage: $pipeline\" # checking for both fastq files and final.fna if [[ -e ${outdir}/${sra}/${sra}_1.fastq ]] &&\\ [[ -e ${outdir}/${sra}/${sra}_2.fastq ]] &&\\ [[ -e $outdir/${sra}/${pipeline}/final.fna ]]; then echo \"[---$SN---] ($(date)) EXISTS: $pipeline final.fna for sample ${sra}\" else read -r -d '' cmd2 <<EOF ${pipeline} ${outdir}/${sra}/${sra}_1.fastq\\ ${outdir}/${sra}/${sra}_2.fastq\\ ${outdir}/refs/HIV_B.K03455.HXB2.amplicons.fasta\\ ${sra}\\ ${outdir}/${sra}/${pipeline} EOF echo -e \"[---$SN---] ($(date)) $pipeline command for ${sra}:\\n\\n$cmd2\\n\" eval $cmd2 [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $pipeline for ${sra} samples\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $pipeline\" && exit 1 ) fi # Step 3: PredictHaplo stage=\"predict_haplo\" echo -e \"\\n[---$SN---] ($(date)) Stage: $stage\" if [[ -e ${outdir}/${sra}/${pipeline}/ph_haplotypes_comb.fna ]]; then echo \"[---$SN---] ($(date)) EXISTS: $stage corrected_1.fastq,corrected_2.fastq\" else command -v PredictHaplo-Paired >/dev/null 2>&1 if [[ $? -eq 0 ]] ; then echo -e \"[---$SN---] ($(date)) $stage PredictHaplo present\" read -r -d '' cmd1 <<EOF haphpipe predict_haplo\\ --fq1 ${outdir}/${sra}/${pipeline}/corrected_1.fastq\\ --fq2 ${outdir}/${sra}/${pipeline}/corrected_2.fastq\\ --ref_fa ${outdir}/${sra}/${pipeline}/final.fna\\ --logfile ${outdir}/${sra}/${pipeline}/haphpipe_PH.out\\ --outdir ${outdir}/${sra}/${pipeline} EOF echo -e \"[---$SN---] ($(date)) $stage command:\\n\\n$cmd1\\n\" eval $cmd1 for ph in ${outdir}/${sra}/${pipeline}/PH*; do read -r -d '' cmd2 <<EOF haphpipe ph_parser\\ --haplotypes_fa $ph/PH*.best_*.fas\\ --logfile ${outdir}/${sra}/${pipeline}/haphpipe_PH.out\\ --outdir ${ph}\\ --prefix ${sra}_$(basename $ph) EOF echo -e \"[---$SN---] ($(date)) $stage command:\\n\\n$cmd2\\n\" eval $cmd2 cat ${outdir}/${sra}/${pipeline}/PH*/ph_haplotypes.fna > ${outdir}/${sra}/${pipeline}/ph_haplotypes_comb.fna done fi [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $stage\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $stage\" && exit 1 ) fi ############################################################################### # Step 2c: Sample 3: SRR8525938 ############################################################################### # Step 1: Fastq-dump stage=\"Demo Sample 3: SRR8525938\" sra='SRR8525938' echo -e \"\\n[---$SN---] ($(date)) Stage: $stage\" # checking for the 5 directories and fastq files if [[ -e ${outdir}/${sra}/${sra}_1.fastq ]] &&\\ [[ -e ${outdir}/${sra}/${sra}_2.fastq ]]; then echo \"[---$SN---] ($(date)) EXISTS: Demo Sample ${sra} paired fastq files\" else read -r -d '' cmd1 <<EOF fastq-dump --outdir ${outdir}/${sra} \\ --split-files\\ --origfmt\\ --minSpotId 30000\\ --maxSpotId 40000\\ ${sra} EOF echo -e \"[---$SN---] ($(date)) $stage for ${sra} command:\\n\\n$cmd1\\n\" eval $cmd1 [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $stage\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $stage fastq-dump failed for sample ${sra}\" && exit 1 ) fi if [[ ! -e ${outdir}/${sra}/${sra}_1.fastq ]] &&\\ [[ ! -e ${outdir}/${sra}/${sra}_2.fastq ]]; then echo \"[---$SN---] ($(date)) FAILED: file downloading SRA data for ${sra} did not complete\" && exit 1 fi # Step 2: Haphpipe_assemble_02 pipeline=\"haphpipe_assemble_02\" echo -e \"\\n[---$SN---] ($(date)) Stage: $pipeline\" # checking for both fastq files and final.fna if [[ -e ${outdir}/${sra}/${sra}_1.fastq ]] &&\\ [[ -e ${outdir}/${sra}/${sra}_2.fastq ]] &&\\ [[ -e $outdir/${sra}/${pipeline}/final.fna ]]; then echo \"[---$SN---] ($(date)) EXISTS: $pipeline final.fna for sample ${sra}\" else read -r -d '' cmd2 <<EOF ${pipeline} ${outdir}/${sra}/${sra}_1.fastq\\ ${outdir}/${sra}/${sra}_2.fastq\\ ${outdir}/refs/HIV_B.K03455.HXB2.amplicons.fasta\\ ${sra}\\ ${outdir}/${sra}/${pipeline} EOF echo -e \"[---$SN---] ($(date)) $pipeline command for ${sra}:\\n\\n$cmd2\\n\" eval $cmd2 [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $pipeline for ${sra} samples\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $pipeline\" && exit 1 ) fi # Step 3: PredictHaplo stage=\"predict_haplo\" echo -e \"\\n[---$SN---] ($(date)) Stage: $stage\" if [[ -e ${outdir}/${sra}/${pipeline}/ph_haplotypes_comb.fna ]]; then echo \"[---$SN---] ($(date)) EXISTS: $stage corrected_1.fastq,corrected_2.fastq\" else command -v PredictHaplo-Paired >/dev/null 2>&1 if [[ $? -eq 0 ]] ; then echo -e \"[---$SN---] ($(date)) $stage PredictHaplo present\" read -r -d '' cmd1 <<EOF haphpipe predict_haplo\\ --fq1 ${outdir}/${sra}/${pipeline}/corrected_1.fastq\\ --fq2 ${outdir}/${sra}/${pipeline}/corrected_2.fastq\\ --ref_fa ${outdir}/${sra}/${pipeline}/final.fna\\ --logfile ${outdir}/${sra}/${pipeline}/haphpipe_PH.out\\ --outdir ${outdir}/${sra}/${pipeline} EOF echo -e \"[---$SN---] ($(date)) $stage command:\\n\\n$cmd1\\n\" eval $cmd1 for ph in ${outdir}/${sra}/${pipeline}/PH*; do read -r -d '' cmd2 <<EOF haphpipe ph_parser\\ --haplotypes_fa $ph/PH*.best_*.fas\\ --logfile ${outdir}/${sra}/${pipeline}/haphpipe_PH.out\\ --outdir ${ph}\\ --prefix ${sra}_$(basename $ph) EOF echo -e \"[---$SN---] ($(date)) $stage command:\\n\\n$cmd2\\n\" eval $cmd2 cat ${outdir}/${sra}/${pipeline}/PH*/ph_haplotypes.fna > ${outdir}/${sra}/${pipeline}/ph_haplotypes_comb.fna done fi [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $stage\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $stage\" && exit 1 ) fi ############################################################################### # Step 2d: Sample 4: SRR8525939 ############################################################################### # Step 1: Fastq-dump stage=\"Demo Sample 4: SRR8525939\" sra='SRR8525939' echo -e \"\\n[---$SN---] ($(date)) Stage: $stage\" # checking for the 5 directories and fastq files if [[ -e ${outdir}/${sra}/${sra}_1.fastq ]] &&\\ [[ -e ${outdir}/${sra}/${sra}_2.fastq ]]; then echo \"[---$SN---] ($(date)) EXISTS: Demo Sample ${sra} paired fastq files\" else read -r -d '' cmd1 <<EOF fastq-dump --outdir ${outdir}/${sra} \\ --split-files\\ --origfmt\\ --minSpotId 30000\\ --maxSpotId 40000\\ ${sra} EOF echo -e \"[---$SN---] ($(date)) $stage for ${sra} command:\\n\\n$cmd1\\n\" eval $cmd1 [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $stage\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $stage fastq-dump failed for sample ${sra}\" && exit 1 ) fi if [[ ! -e ${outdir}/${sra}/${sra}_1.fastq ]] &&\\ [[ ! -e ${outdir}/${sra}/${sra}_2.fastq ]]; then echo \"[---$SN---] ($(date)) FAILED: file downloading SRA data for ${sra} did not complete\" && exit 1 fi # Step 2: Haphpipe_assemble_02 pipeline=\"haphpipe_assemble_02\" echo -e \"\\n[---$SN---] ($(date)) Stage: $pipeline\" # checking for both fastq files and final.fna if [[ -e ${outdir}/${sra}/${sra}_1.fastq ]] &&\\ [[ -e ${outdir}/${sra}/${sra}_2.fastq ]] &&\\ [[ -e $outdir/${sra}/${pipeline}/final.fna ]]; then echo \"[---$SN---] ($(date)) EXISTS: $pipeline final.fna for sample ${sra}\" else read -r -d '' cmd2 <<EOF ${pipeline} ${outdir}/${sra}/${sra}_1.fastq\\ ${outdir}/${sra}/${sra}_2.fastq\\ ${outdir}/refs/HIV_B.K03455.HXB2.amplicons.fasta\\ ${sra}\\ ${outdir}/${sra}/${pipeline} EOF echo -e \"[---$SN---] ($(date)) $pipeline command for ${sra}:\\n\\n$cmd2\\n\" eval $cmd2 [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $pipeline for ${sra} samples\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $pipeline\" && exit 1 ) fi # Step 3: PredictHaplo stage=\"predict_haplo\" echo -e \"\\n[---$SN---] ($(date)) Stage: $stage\" if [[ -e ${outdir}/${sra}/${pipeline}/ph_haplotypes_comb.fna ]]; then echo \"[---$SN---] ($(date)) EXISTS: $stage corrected_1.fastq,corrected_2.fastq\" else command -v PredictHaplo-Paired >/dev/null 2>&1 if [[ $? -eq 0 ]] ; then echo -e \"[---$SN---] ($(date)) $stage PredictHaplo present\" read -r -d '' cmd1 <<EOF haphpipe predict_haplo\\ --fq1 ${outdir}/${sra}/${pipeline}/corrected_1.fastq\\ --fq2 ${outdir}/${sra}/${pipeline}/corrected_2.fastq\\ --ref_fa ${outdir}/${sra}/${pipeline}/final.fna\\ --logfile ${outdir}/${sra}/${pipeline}/haphpipe_PH.out\\ --outdir ${outdir}/${sra}/${pipeline} EOF echo -e \"[---$SN---] ($(date)) $stage command:\\n\\n$cmd1\\n\" eval $cmd1 for ph in ${outdir}/${sra}/${pipeline}/PH*; do read -r -d '' cmd2 <<EOF haphpipe ph_parser\\ --haplotypes_fa $ph/PH*.best_*.fas\\ --logfile ${outdir}/${sra}/${pipeline}/haphpipe_PH.out\\ --outdir ${ph}\\ --prefix ${sra}_$(basename $ph) EOF echo -e \"[---$SN---] ($(date)) $stage command:\\n\\n$cmd2\\n\" eval $cmd2 cat ${outdir}/${sra}/${pipeline}/PH*/ph_haplotypes.fna > ${outdir}/${sra}/${pipeline}/ph_haplotypes_comb.fna done fi [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $stage\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $stage\" && exit 1 ) fi ############################################################################### # Step 2e: Sample 5: SRR8525940 ############################################################################### # Step 1: Fastq-dump stage=\"Demo Sample 5: SRR8525940\" sra='SRR8525940' echo -e \"\\n[---$SN---] ($(date)) Stage: $stage\" # checking for the 5 directories and fastq files if [[ -e ${outdir}/${sra}/${sra}_1.fastq ]] &&\\ [[ -e ${outdir}/${sra}/${sra}_2.fastq ]]; then echo \"[---$SN---] ($(date)) EXISTS: Demo Sample ${sra} paired fastq files\" else read -r -d '' cmd1 <<EOF fastq-dump --outdir ${outdir}/${sra} \\ --split-files\\ --origfmt\\ --minSpotId 30000\\ --maxSpotId 40000\\ ${sra} EOF echo -e \"[---$SN---] ($(date)) $stage for ${sra} command:\\n\\n$cmd1\\n\" eval $cmd1 [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $stage\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $stage fastq-dump failed for sample ${sra}\" && exit 1 ) fi if [[ ! -e ${outdir}/${sra}/${sra}_1.fastq ]] &&\\ [[ ! -e ${outdir}/${sra}/${sra}_2.fastq ]]; then echo \"[---$SN---] ($(date)) FAILED: file downloading SRA data for ${sra} did not complete\" && exit 1 fi # Step 2: Haphpipe_assemble_02 pipeline=\"haphpipe_assemble_02\" echo -e \"\\n[---$SN---] ($(date)) Stage: $pipeline\" # checking for both fastq files and final.fna if [[ -e ${outdir}/${sra}/${sra}_1.fastq ]] &&\\ [[ -e ${outdir}/${sra}/${sra}_2.fastq ]] &&\\ [[ -e $outdir/${sra}/${pipeline}/final.fna ]]; then echo \"[---$SN---] ($(date)) EXISTS: $pipeline final.fna for sample ${sra}\" else read -r -d '' cmd2 <<EOF ${pipeline} ${outdir}/${sra}/${sra}_1.fastq\\ ${outdir}/${sra}/${sra}_2.fastq\\ ${outdir}/refs/HIV_B.K03455.HXB2.amplicons.fasta\\ ${sra}\\ ${outdir}/${sra}/${pipeline} EOF echo -e \"[---$SN---] ($(date)) $pipeline command for ${sra}:\\n\\n$cmd2\\n\" eval $cmd2 [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $pipeline for ${sra} samples\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $pipeline\" && exit 1 ) fi # Step 3: PredictHaplo stage=\"predict_haplo\" echo -e \"\\n[---$SN---] ($(date)) Stage: $stage\" if [[ -e ${outdir}/${sra}/${pipeline}/ph_haplotypes_comb.fna ]]; then echo \"[---$SN---] ($(date)) EXISTS: $stage corrected_1.fastq,corrected_2.fastq\" else command -v PredictHaplo-Paired >/dev/null 2>&1 if [[ $? -eq 0 ]] ; then echo -e \"[---$SN---] ($(date)) $stage PredictHaplo present\" read -r -d '' cmd1 <<EOF haphpipe predict_haplo\\ --fq1 ${outdir}/${sra}/${pipeline}/corrected_1.fastq\\ --fq2 ${outdir}/${sra}/${pipeline}/corrected_2.fastq\\ --ref_fa ${outdir}/${sra}/${pipeline}/final.fna\\ --logfile ${outdir}/${sra}/${pipeline}/haphpipe_PH.out\\ --outdir ${outdir}/${sra}/${pipeline} EOF echo -e \"[---$SN---] ($(date)) $stage command:\\n\\n$cmd1\\n\" eval $cmd1 for ph in ${outdir}/${sra}/${pipeline}/PH*; do read -r -d '' cmd2 <<EOF haphpipe ph_parser\\ --haplotypes_fa $ph/PH*.best_*.fas\\ --logfile ${outdir}/${sra}/${pipeline}/haphpipe_PH.out\\ --outdir ${ph}\\ --prefix ${sra}_$(basename $ph) EOF echo -e \"[---$SN---] ($(date)) $stage command:\\n\\n$cmd2\\n\" eval $cmd2 cat ${outdir}/${sra}/${pipeline}/PH*/ph_haplotypes.fna > ${outdir}/${sra}/${pipeline}/ph_haplotypes_comb.fna done fi [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $stage\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $stage\" && exit 1 ) fi ############################################################################### # Step 3: Run MSA with MAFFT ############################################################################### stage=\"multiple_align\" echo -e \"\\n[---$SN---] ($(date)) Stage: $stage\" if [[ -e ${outdir}/hp_alignments/alignment_region00.fasta ]] &&\\ [[ -e ${outdir}/hp_alignments/alignment_region01.fasta ]] &&\\ [[ -e ${outdir}/hp_alignments/alignment_region02.fasta ]]; then echo \"[---$SN---] ($(date)) EXISTS: $stage alignment_region00.fasta,alignment_region01.fasta,alignment_region02.fasta\" else # check for PredictHaplo command -v PredictHaplo-Paired >/dev/null 2>&1 if [[ $? -eq 0 ]] ; then read -r -d '' cmd1 <<EOF ls -d ${outdir}/SRR*/${pipeline} > ${outdir}/dir_list.txt &&\\ ls -d ${outdir}/SRR*/${pipeline}/PH0* >> ${outdir}/dir_list.txt EOF echo -e \"[---$SN---] ($(date)) $stage command:\\n\\n$cmd1\\n\" eval $cmd1 read -r -d '' cmd2 <<EOF haphpipe multiple_align\\ --ncpu $ncpu\\ --dir_list ${outdir}/dir_list.txt\\ --ref_gtf ${outdir}/refs/HIV_B.K03455.HXB2.gtf\\ --logfile haphpipe.out\\ --phylipout\\ --outdir ${outdir} EOF echo -e \"[---$SN---] ($(date)) $stage command:\\n\\n$cmd2\\n\" eval $cmd2 [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $stage\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $stage\" && exit 1 ) # if no PredictHaplo, execute these commands else read -r -d '' cmd1 <<EOF ls -d ${outdir}/SRR*/${pipeline} > ${outdir}/dir_list.txt EOF echo -e \"[---$SN---] ($(date)) $stage command:\\n\\n$cmd1\\n\" eval $cmd1 read -r -d '' cmd2 <<EOF haphpipe multiple_align\\ --ncpu $ncpu\\ --dir_list ${outdir}/dir_list.txt\\ --ref_gtf ${outdir}/refs/HIV_B.K03455.HXB2.gtf\\ --logfile haphpipe.out\\ --phylipout\\ --outdir ${outdir} EOF echo -e \"[---$SN---] ($(date)) $stage command:\\n\\n$cmd2\\n\" eval $cmd2 [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $stage\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $stage\" && exit 1 ) fi fi ############################################################################### # Step 4: ModelTest ############################################################################### stage=\"model_test\" echo -e \"\\n[---$SN---] ($(date)) Stage: $stage\" if [[ -e ${outdir}/alignment_region00_modeltest_results.out ]] &&\\ [[ -e ${outdir}/alignment_region01_modeltest_results.out ]] &&\\ [[ -e ${outdir}/alignment_region02_modeltest_results.out ]]; then echo \"[---$SN---] ($(date)) EXISTS: $stage alignment_region00_modeltest_results.out,alignment_region01_modeltest_results.out,alignment_region02_modeltest_results.out\" else for region in ${outdir}/hp_alignments/alignment_region??.fasta; do reg=${region%.fasta} read -r -d '' cmd <<EOF haphpipe model_test\\ --seqs ${region}\\ --run_id $(basename $reg)\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir}\\ --template raxml\\ --ncpu ${ncpu} EOF echo -e \"[---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $stage for $region\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $stage for $region\" && exit 1 ) done fi ############################################################################### # Step 5: Build tree with RAxML ############################################################################### stage=\"build_tree_NG\" echo -e \"\\n[---$SN---] ($(date)) Stage: $stage\" if [[ -e ${outdir}/hp_tree/alignment_region00.raxml.support ]] &&\\ [[ -e ${outdir}/hp_tree/alignment_region01.raxml.support ]] &&\\ [[ -e ${outdir}/hp_tree/alignment_region02.raxml.support ]]; then echo \"[---$SN---] ($(date)) EXISTS: $stage alignment_region00.raxml.support, alignment_region01.raxml.support, alignment_region02.raxml.support\" else # check for PredictHaplo command -v PredictHaplo-Paired >/dev/null 2>&1 if [[ $? -eq 0 ]] ; then for alignment in ${outdir}/hp_alignments/alignment_region??.phy; do reg=${alignment%.phy} read -r -d '' cmd <<EOF haphpipe build_tree_NG\\ --all\\ --seqs ${alignment}\\ --output_name $(basename $reg)\\ --model GTR+I\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir}\\ --in_type PHYLIP EOF echo -e \"[---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $stage\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $stage\" && exit 1 ) done else for alignment in ${outdir}/hp_alignments/alignment_region00.phy ${outdir}/hp_alignments/alignment_region01.phy; do reg=${alignment%.phy} read -r -d '' cmd <<EOF haphpipe build_tree_NG\\ --all\\ --seqs ${alignment}\\ --output_name $(basename $reg)\\ --model GTR+I\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir}\\ --in_type PHYLIP EOF echo -e \"[---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $stage\" && echo \"NOTE: region02 did not run because there are only 3 sequences in the alignment\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $stage\" && exit 1 ) done fi fi #---Complete job t2=$(date +\"%s\") diff=$(($t2-$t1)) echo \"[---$SN---] ($(date)) $(($diff / 60)) minutes and $(($diff % 60)) seconds elapsed.\" echo \"[---$SN---] ($(date)) $SN COMPLETE.\" The structure of this interactive demo is such: One sample (SRR8525886) will be used an example, and the single bash command for each step will be shown. Step 0: Get the reference data needed for this interactive demo. hp_demo --refonly Step 1: Pull sample data from SRA with fastq-dump. fastq-dump --outdir haphpipe_demo/SRR8525886 \\ --split-files \\ --origfmt \\ --minSpotId 30000 \\ --maxSpotId 40000 \\ SRR8525886 Reminder that the above code is the same as this command on a single line - the \\ just makes it more visually readable. fastq-dump --outdir haphpipe_demo/SRR8525886 --split-files --origfmt --minSpotId 30000 --maxSpotId 40000 SRR8525886 Step 2: Run haphpipe_assemble_02 haphpipe_assemble_02 \\ haphpipe_demo/SRR8525886/SRR8525886_1.fastq \\ haphpipe_demo/SRR8525886/SRR8525886_2.fastq \\ haphpipe_demo/refs/HIV_B.K03455.HXB2.amplicons.fasta \\ SRR8525886 \\ haphpipe_demo/SRR8525886/haphpipe_assemble_02 (if PredictHaplo is installed) Step 3a: Run PredictHaplo haphpipe predict_haplo\\ --fq1 haphpipe_demo/SRR8525886/haphpipe_assemble_02/corrected_1.fastq \\ --fq2 haphpipe_demo/SRR8525886/haphpipe_assemble_02/corrected_2.fastq \\ --ref_fa haphpipe_demo/SRR8525886/haphpipe_assemble_02/final.fna \\ --logfile haphpipe_demo/SRR8525886/haphpipe_assemble_02/haphpipe_PH.out \\ --outdir haphpipe_demo/SRR8525886/haphpipe_assemble_02 Step 3b: Run PredictHaplo Parser for each amplicon region Individually: # PH01 haphpipe ph_parser \\ --haplotypes_fa haphpipe_demo/SRR8525886/haphpipe_assemble_02/PH01_PRRT/PH*.best_*.fas \\ --logfile haphpipe_demo/SRR8525886/haphpipe_assemble_02/haphpipe_PH.out \\ --outdir haphpipe_demo/SRR8525886/haphpipe_assemble_02/PH01_PRRT \\ --prefix ${sra}_PH01_PRRT # PH02 haphpipe ph_parser \\ --haplotypes_fa haphpipe_demo/SRR8525886/haphpipe_assemble_02/PH02_INT/PH*.best_*.fas \\ --logfile haphpipe_demo/SRR8525886/haphpipe_assemble_02/haphpipe_PH.out \\ --outdir haphpipe_demo/SRR8525886/haphpipe_assemble_02/PH02_INT \\ --prefix SRR8525886_PH02_INT # PH03 haphpipe ph_parser \\ --haplotypes_fa haphpipe_demo/SRR8525886/haphpipe_assemble_02/PH03_gp120/PH*.best_*.fas \\ --logfile haphpipe_demo/SRR8525886/haphpipe_assemble_02/haphpipe_PH.out \\ --outdir haphpipe_demo/SRR8525886/haphpipe_assemble_02/PH03_gp120 \\ --prefix SRR8525886_PH03_gp120 Or you can run ph_parser in a loop for all the haplotype regions: for ph in haphpipe_demo/SRR8525886/haphpipe_assemble_02/PH*; do haphpipe ph_parser \\ --haplotypes_fa ${ph}/PH*.best_*.fas \\ --logfile $(dirname $ph)/haphpipe_PH.out \\ --outdir ${ph} \\ --prefix SRR8525886_$(basename $ph) done Step 4: Rerun steps 1-3 for all samples, make sure to replace the accession number. Step 5: Run multiple sequence alignment using MAFFT. Make a file with a list of directories containing the final.fna files and the ph_haplotype.fna : ls -d haphpipe_demo/SRR*/haphpipe_assemble_02 > haphpipe_demo/dir_list.txt &&\\ ls -d haphpipe_demo/SRR*/haphpipe_assemble_02/PH0* >> haphpipe_demo/dir_list.txt If PredictHaplo was not installed, you only need the first line: ls -d haphpipe_demo/SRR*/haphpipe_assemble_02 > haphpipe_demo/dir_list.txt Now, run multiple_align haphpipe multiple_align \\ --ncpu 1 \\ --dir_list haphpipe_demo/dir_list.txt \\ --ref_gtf haphpipe_demo/refs/HIV_B.K03455.HXB2.gtf \\ --logfile haphpipe.out \\ --phylipout \\ --outdir haphpipe_demo Step 6: Estimate best-fit model of evolution using ModelTest-NG # Region00 - this is amplicon PRRT haphpipe model_test \\ --seqs haphpipe_demo/hp_alignments/alignment_region00.fasta \\ --run_id alignment_region00 \\ --logfile haphpipe_demo/haphpipe.out \\ --outdir haphpipe_demo \\ --template raxml \\ --ncpu 1 # Region01 - this is amplicon INT haphpipe model_test \\ --seqs haphpipe_demo/hp_alignments/alignment_region01.fasta \\ --run_id alignment_region01 \\ --logfile haphpipe_demo/haphpipe.out \\ --outdir haphpipe_demo \\ --template raxml \\ --ncpu 1 # Region02 - this is amplicon gp120 haphpipe model_test \\ --seqs haphpipe_demo/hp_alignments/alignment_region02.fasta \\ --run_id alignment_region02 \\ --logfile haphpipe_demo/haphpipe.out \\ --outdir haphpipe_demo \\ --template raxml \\ --ncpu 1 Step 7: Build a phylogenetic tree for each region using RAXML. The models can be changed according to the output of ModelTest (step 6 - above). Here we just show with model GTRGAMMAX. # Region00 - this is amplicon PRRT haphpipe build_tree_NG \\ --all \\ --seqs haphpipe_demo/hp_alignments/alignment_region00.fasta \\ --output_name alignment_region00 \\ --model GTRGAMMAX\\ --logfile haphpipe_demo/haphpipe.out\\ --outdir haphpipe_demo # Region01 - this is amplicon INT haphpipe build_tree_NG \\ --all \\ --seqs haphpipe_demo/hp_alignments/alignment_region01.fasta \\ --output_name alignment_region01 \\ --model GTRGAMMAX\\ --logfile haphpipe_demo/haphpipe.out\\ --outdir haphpipe_demo If PredictHaplo is not run, there is not enough taxa - only 3 - to build a tree for region02 gp120. # Region02 - this is amplicon gp120 haphpipe build_tree_NG \\ --all \\ --seqs haphpipe_demo/hp_alignments/alignment_region02.fasta \\ --output_name alignment_region02 \\ --model GTRGAMMAX\\ --logfile haphpipe_demo/haphpipe.out\\ --outdir haphpipe_demo","title":"Demo"},{"location":"demos/#demo-module","text":"After successful installation, the demo dataset can be run to ensure HAPHPIPE is installed and set up correctly. These are the steps completed in the demo: Download Sample SRR8525886 using fastq-dump Run haphpipe_assemble_02 If PredictHaplo is installed, run PredictHaplo Download Sample SRR8525933 using fastq-dump Run haphpipe_assemble_02 If PredictHaplo is installed, run PredictHaplo Download Sample SRR8525938 using fastq-dump Run haphpipe_assemble_02 If PredictHaplo is installed, run PredictHaplo Download Sample SRR8525939 using fastq-dump Run haphpipe_assemble_02 If PredictHaplo is installed, run PredictHaplo Download Sample SRR8525940 using fastq-dump Run haphpipe_assemble_02 If PredictHaplo is installed, run PredictHaplo Run multiple sequence alignment with MAFFT Run ModelTest with modeltest-NG Buid a phylogenetic tree with RAxML We split up the samples, instead of looping through them for fastq-dump because sometimes the fastq-dump/NCBI doesn't work. It was more helpful to see when a sample failed this way rather than a loop for fastq-dump then a loop for running haphpipe_assemble_02 . If fastq-dump doesn't work, check your wifi connection, sometimes HPCs have trouble downloading SRA data with the connection. Downloading on own computer should work just fine. If for some reason the fastq-dump doesn't work, you can download directly from the SRA database on a webpage.","title":"Demo Module"},{"location":"demos/#description-of-data-used","text":"Accession numbers: SRR8525886, SRR8525933, SRR8525938, SRR8525939,and SRR8525940 These data were first presented in this paper . The SRA project data can be found here . This data was a part of a cohort study. The viral samples were amplified for three gene regions: PRRT , int , and env . This data was analyzed using HAPHPIPE and PredictHaplo in this paper . Results will vary from this 2020 cross-sectional study because here, for the demo, we only use 10,000 reads for each of these 5 samples compared to the study which uses all the available NGS reads.","title":"Description of data used"},{"location":"demos/#running-demo-automatically","text":"Running the demo is simple and requires a single command: hp_demo or haphpipe demo A specific outdirectory can be specified by: hp_demo --outdir $outdir_name The output of the entire demo is as such If running the entire demo is not desired, this command can be executed to just pull the references included in HAPHPIPE into the directory that is specified (default outdir is . ). hp_demo --refonly Output on the terminal is as such, and the three HIV reference files are located in the subdirectory refs . See the User Guide for more information regarding these reference files. /base/directory/path/of/haphpipe) Demo was run with --refonly. References are now in outdirectory: $outdir_name/haphpipe_demo/refs.","title":"Running Demo Automatically"},{"location":"demos/#output-files","text":"If the entire demo is run (i.e., no use of --refonly ) then these are all the files for output if PredictHaplo is NOT installed . The files are listed for a single sample as example, not all 5 samples. The other files from multiple_align , model_test , and build_tree are all listed. Directory structure without PH haphpipe_demo \u251c\u2500\u2500 SRR8525886 | \u251c\u2500\u2500 SRR8525886_1.fastq | \u251c\u2500\u2500 SRR8525886_2.fastq | \u2514\u2500\u2500 haphpipe_assemble_02 | \u251c\u2500\u2500 corrected_1.fastq | \u251c\u2500\u2500 corrected_2.fastq | \u251c\u2500\u2500 corrected_U.fastq | \u251c\u2500\u2500 final.bam | \u251c\u2500\u2500 final.bam.bai | \u251c\u2500\u2500 final_bt2.out | \u251c\u2500\u2500 final.fna | \u251c\u2500\u2500 final.vcf.gz | \u251c\u2500\u2500 final.vcf.gz.tbi | \u251c\u2500\u2500 haphpipe.out | \u251c\u2500\u2500 refined.01.fna | \u251c\u2500\u2500 refined.02.fna | \u251c\u2500\u2500 refined.03.fna | \u251c\u2500\u2500 refined.fna | \u251c\u2500\u2500 refined_bt2.01.out | \u251c\u2500\u2500 refined_bt2.02.out | \u251c\u2500\u2500 refined_bt2.03.out | \u251c\u2500\u2500 refined_bt2.out | \u251c\u2500\u2500 refined_summary.out | \u251c\u2500\u2500 trimmed_1.fastq | \u251c\u2500\u2500 trimmed_2.fastq | \u251c\u2500\u2500 trimmed_U.fastq | \u2514\u2500\u2500 trimmomatic_summary.out | \u251c\u2500\u2500 dir_list.txt \u251c\u2500\u2500 haphpipe.out \u251c\u2500\u2500 hp_multiple_align | \u251c\u2500\u2500 alignment_region00.fasta | \u251c\u2500\u2500 alignment_region00.phy | \u251c\u2500\u2500 alignment_region01.fasta | \u251c\u2500\u2500 alignment_region01.phy | \u251c\u2500\u2500 alignment_region02.fasta | \u251c\u2500\u2500 alignment_region02.phy | \u251c\u2500\u2500 all_sequences.fasta | \u251c\u2500\u2500 all_sequences_region00.fasta | \u251c\u2500\u2500 all_sequences_region01.fasta | \u2514\u2500\u2500 all_sequences_region02.fasta | \u251c\u2500\u2500 alignment_region00_modeltest_results.out \u251c\u2500\u2500 alignment_region00_modeltest_results_summary.tsv \u251c\u2500\u2500 alignment_region01_modeltest_results.out \u251c\u2500\u2500 alignment_region01_modeltest_results_summary.tsv \u251c\u2500\u2500 alignment_region02_modeltest_results.out \u251c\u2500\u2500 alignment_region02_modeltest_results_summary.tsv | \u251c\u2500\u2500 hp_build_tree | \u251c\u2500\u2500 RAxML_bestTree.alignment_region00 | \u251c\u2500\u2500 RAxML_bestTree.alignment_region01 | \u251c\u2500\u2500 RAxML_bipartitions.alignment_region00 | \u251c\u2500\u2500 RAxML_bipartitions.alignment_region01 | \u251c\u2500\u2500 RAxML_bipartitionsBranchLabels.alignment_region00 | \u251c\u2500\u2500 RAxML_bipartitionsBranchLabels.alignment_region01 | \u251c\u2500\u2500 RAxML_bootstrap.alignment_region00 | \u251c\u2500\u2500 RAxML_bootstrap.alignment_region01 | \u251c\u2500\u2500 RAxML_info.alignment_region00 | \u2514\u2500\u2500 RAxML_info.alignment_region01 | \u251c\u2500\u2500 sample# | \u251c\u2500\u2500 sample#_1.fastq | \u251c\u2500\u2500 sample#_2.fastq | \u2514\u2500\u2500 haphpipe_assemble_02 .... If the entire demo is run (i.e., no use of --refonly ) then these are all the files for output if PredictHaplo is installed. Directory structure with PH haphpipe_demo \u251c\u2500\u2500 SRR8525886 | \u251c\u2500\u2500 SRR8525886_1.fastq | \u251c\u2500\u2500 SRR8525886_2.fastq | \u2514\u2500\u2500 haphpipe_assemble_02 | \u251c\u2500\u2500 corrected_1.fastq | \u251c\u2500\u2500 corrected_2.fastq | \u251c\u2500\u2500 corrected_U.fastq | \u251c\u2500\u2500 final.bam | \u251c\u2500\u2500 final.bam.bai | \u251c\u2500\u2500 final_bt2.out | \u251c\u2500\u2500 final.fna | \u251c\u2500\u2500 final.vcf.gz | \u251c\u2500\u2500 final.vcf.gz.tbi | \u251c\u2500\u2500 haphpipe.out | \u251c\u2500\u2500 refined.01.fna | \u251c\u2500\u2500 refined.02.fna | \u251c\u2500\u2500 refined.03.fna | \u251c\u2500\u2500 refined.fna | \u251c\u2500\u2500 refined_bt2.01.out | \u251c\u2500\u2500 refined_bt2.02.out | \u251c\u2500\u2500 refined_bt2.03.out | \u251c\u2500\u2500 refined_bt2.out | \u251c\u2500\u2500 refined_summary.out | \u251c\u2500\u2500 trimmed_1.fastq | \u251c\u2500\u2500 trimmed_2.fastq | \u251c\u2500\u2500 trimmed_U.fastq | \u251c\u2500\u2500 trimmomatic_summary.out | \u251c\u2500\u2500 ph_haplotypes_comb.fna | \u251c\u2500\u2500 PH01_PRRT | | \u251c\u2500\u2500 PH01_PRRT.best_1_1197.fas | | \u251c\u2500\u2500 PH01_PRRT.best_1_1197.html | | \u251c\u2500\u2500 PH01_PRRT.config.log | | \u251c\u2500\u2500 ph_haplotypes.fna | | \u2514\u2500\u2500 ph_summary.txt | \u251c\u2500\u2500 PH02_INT | | \u251c\u2500\u2500 PH02_INT.best_1_964.fas | | \u251c\u2500\u2500 PH02_INT.best_1_964.html | | \u251c\u2500\u2500 PH02_INT.config.log | | \u251c\u2500\u2500 ph_haplotypes.fna | | \u2514\u2500\u2500 ph_summary.txt | \u2514\u2500\u2500 PH03_gp120 | \u251c\u2500\u2500 PH03_gp120.best_1_1641.fas | \u251c\u2500\u2500 PH03_gp120.best_1_1641.html | \u251c\u2500\u2500 PH03_gp120.config.log | \u251c\u2500\u2500 ph_haplotypes.fna | \u2514\u2500\u2500 ph_summary.txt | \u251c\u2500\u2500 dir_list.txt \u251c\u2500\u2500 haphpipe.out \u251c\u2500\u2500 hp_multiple_align | \u251c\u2500\u2500 alignment_region00.fasta | \u251c\u2500\u2500 alignment_region00.phy | \u251c\u2500\u2500 alignment_region01.fasta | \u251c\u2500\u2500 alignment_region01.phy | \u251c\u2500\u2500 alignment_region02.fasta | \u251c\u2500\u2500 alignment_region02.phy | \u251c\u2500\u2500 all_sequences.fasta | \u251c\u2500\u2500 all_sequences_region00.fasta | \u251c\u2500\u2500 all_sequences_region01.fasta | \u2514\u2500\u2500 all_sequences_region02.fasta | \u251c\u2500\u2500 alignment_region00_modeltest_results.out \u251c\u2500\u2500 alignment_region00_modeltest_results_summary.tsv \u251c\u2500\u2500 alignment_region01_modeltest_results.out \u251c\u2500\u2500 alignment_region01_modeltest_results_summary.tsv \u251c\u2500\u2500 alignment_region02_modeltest_results.out \u251c\u2500\u2500 alignment_region02_modeltest_results_summary.tsv | \u251c\u2500\u2500 hp_build_tree | \u251c\u2500\u2500 RAxML_bestTree.alignment_region00 | \u251c\u2500\u2500 RAxML_bestTree.alignment_region01 | \u251c\u2500\u2500 RAxML_bestTree.alignment_region01 | \u251c\u2500\u2500 RAxML_bipartitions.alignment_region00 | \u251c\u2500\u2500 RAxML_bipartitions.alignment_region01 | \u251c\u2500\u2500 RAxML_bipartitions.alignment_region02 | \u251c\u2500\u2500 RAxML_bipartitionsBranchLabels.alignment_region00 | \u251c\u2500\u2500 RAxML_bipartitionsBranchLabels.alignment_region01 | \u251c\u2500\u2500 RAxML_bipartitionsBranchLabels.alignment_region02 | \u251c\u2500\u2500 RAxML_bootstrap.alignment_region00 | \u251c\u2500\u2500 RAxML_bootstrap.alignment_region01 | \u251c\u2500\u2500 RAxML_bootstrap.alignment_region02 | \u251c\u2500\u2500 RAxML_info.alignment_region00 | \u251c\u2500\u2500 RAxML_info.alignment_region01 | \u2514\u2500\u2500 RAxML_info.alignment_region02 | \u251c\u2500\u2500 sample# | \u251c\u2500\u2500 sample#_1.fastq | \u251c\u2500\u2500 sample#_2.fastq | \u2514\u2500\u2500 haphpipe_assemble_02 ....","title":"Output files"},{"location":"demos/#running-demo-interactively","text":"The demo pipeline bash script: haphpipe_demo #!/usr/bin/env bash # Copyright (C) 2020 Keylie M. Gibson # This program is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation, either version 3 of the License, or # (at your option) any later version. # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # You should have received a copy of the GNU General Public License # along with this program. If not, see <http://www.gnu.org/licenses/>. ############################################################################### # Demo pipeline implementing amplicon assembly using a reference-based approach #(haphpipe_assemble_02). Reads are error-corrected and used to refine # the initial assembly, with up to 5 refinement steps. PredictHaplo runs if it # is loaded, and multiple sequence alignment is conducted with the final.fna # (and haplotypes if there). ############################################################################### SN=$(basename $0) read -r -d '' USAGE <<EOF USAGE: $SN <outdir> ----- HAPHPIPE demo ----- This demo implements amplicon assembly using a reference-based approach. Five SRA samples are pulled with fastq-dump, assembled with haphpipe_assemble_02, multiple aligned and a phylogeny estimated. If PredictHaplo is installed, haplotypes are also predicted followed by MSA and a phylogeny estimated. outdir: Output directory (default is demo) EOF #--- Read command line args [[ -n \"$1\" ]] && [[ \"$1\" == '-h' ]] && echo \"$USAGE\" && exit 0 [[ -n \"$1\" ]] && outdir=\"$1\" [[ -z ${outdir+x} ]] && outdir=$SN mkdir -p $outdir/refs #--- Determine CPUs to use # First examines NCPU environment variable, then nproc, finally sets to 1 [[ -n \"$NCPU\" ]] && ncpu=$NCPU [[ -z $ncpu ]] && ncpu=$(nproc 2> /dev/null) [[ -z $ncpu ]] && ncpu=1 #--- Determine whether verbose [[ -n \"$VERBOSE\" ]] && quiet=\"\" || quiet='--quiet' echo \"[---$SN---] ($(date)) outdir: $outdir\" echo \"[---$SN---] ($(date)) num CPU: $ncpu\" #--- Start the timer t1=$(date +\"%s\") ### We split up the samples, instead of looping through them for fastq-dump ### ### because sometimes the fastq-dump/NCBI doesn't work. It was more helpful ### ### to see when a sample failed this way rather than a loop for fastq-dump ### ### then a loop for running haphpipe_assemble_02. ### ############################################################################### # Step 2a: Sample 1: SRR8525886 ############################################################################### # Step 1: Fastq-dump stage=\"Demo Sample 1: SRR8525886\" sra='SRR8525886' echo -e \"\\n[---$SN---] ($(date)) Stage: $stage\" # checking for the 5 directories and fastq files if [[ -e ${outdir}/${sra}/${sra}_1.fastq ]] &&\\ [[ -e ${outdir}/${sra}/${sra}_2.fastq ]]; then echo \"[---$SN---] ($(date)) EXISTS: Demo Sample ${sra} paired fastq files\" else read -r -d '' cmd1 <<EOF fastq-dump --outdir ${outdir}/${sra} \\ --split-files\\ --origfmt\\ --minSpotId 30000\\ --maxSpotId 40000\\ ${sra} EOF echo -e \"[---$SN---] ($(date)) $stage for ${sra} command:\\n\\n$cmd1\\n\" eval $cmd1 [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $stage\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $stage fastq-dump failed for sample ${sra}\" && exit 1 ) fi if [[ ! -e ${outdir}/${sra}/${sra}_1.fastq ]] &&\\ [[ ! -e ${outdir}/${sra}/${sra}_2.fastq ]]; then echo \"[---$SN---] ($(date)) FAILED: file downloading SRA data for ${sra} did not complete\" && exit 1 fi # Step 2: Haphpipe_assemble_02 pipeline=\"haphpipe_assemble_02\" echo -e \"\\n[---$SN---] ($(date)) Stage: $pipeline\" # checking for both fastq files and final.fna if [[ -e ${outdir}/${sra}/${sra}_1.fastq ]] &&\\ [[ -e ${outdir}/${sra}/${sra}_2.fastq ]] &&\\ [[ -e $outdir/${sra}/${pipeline}/final.fna ]]; then echo \"[---$SN---] ($(date)) EXISTS: $pipeline final.fna for sample ${sra}\" else read -r -d '' cmd2 <<EOF ${pipeline} ${outdir}/${sra}/${sra}_1.fastq\\ ${outdir}/${sra}/${sra}_2.fastq\\ ${outdir}/refs/HIV_B.K03455.HXB2.amplicons.fasta\\ ${sra}\\ ${outdir}/${sra}/${pipeline} EOF echo -e \"[---$SN---] ($(date)) $pipeline command for ${sra}:\\n\\n$cmd2\\n\" eval $cmd2 [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $pipeline for ${sra} samples\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $pipeline\" && exit 1 ) fi # Step 3: PredictHaplo stage=\"predict_haplo\" echo -e \"\\n[---$SN---] ($(date)) Stage: $stage\" if [[ -e ${outdir}/${sra}/${pipeline}/ph_haplotypes_comb.fna ]]; then echo \"[---$SN---] ($(date)) EXISTS: $stage corrected_1.fastq,corrected_2.fastq\" else command -v PredictHaplo-Paired >/dev/null 2>&1 || if [[ $? -eq 0 ]] ; then echo -e \"[---$SN---] ($(date)) $stage PredictHaplo present\" read -r -d '' cmd1 <<EOF haphpipe predict_haplo\\ --fq1 ${outdir}/${sra}/${pipeline}/corrected_1.fastq\\ --fq2 ${outdir}/${sra}/${pipeline}/corrected_2.fastq\\ --ref_fa ${outdir}/${sra}/${pipeline}/final.fna\\ --logfile ${outdir}/${sra}/${pipeline}/haphpipe_PH.out\\ --outdir ${outdir}/${sra}/${pipeline} EOF echo -e \"[---$SN---] ($(date)) $stage command:\\n\\n$cmd1\\n\" eval $cmd1 for ph in ${outdir}/${sra}/${pipeline}/PH*; do read -r -d '' cmd2 <<EOF haphpipe ph_parser\\ --haplotypes_fa $ph/PH*.best_*.fas\\ --logfile ${outdir}/${sra}/${pipeline}/haphpipe_PH.out\\ --outdir ${ph}\\ --prefix ${sra}_$(basename $ph) EOF echo -e \"[---$SN---] ($(date)) $stage command:\\n\\n$cmd2\\n\" eval $cmd2 cat ${outdir}/${sra}/${pipeline}/PH*/ph_haplotypes.fna > ${outdir}/${sra}/${pipeline}/ph_haplotypes_comb.fna done fi [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $stage\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $stage\" && exit 1 ) fi ############################################################################### # Step 2b: Sample 2: SRR8525933 ############################################################################### # Step 1: Fastq-dump stage=\"Demo Sample 2: SRR8525933\" sra='SRR8525933' echo -e \"\\n[---$SN---] ($(date)) Stage: $stage\" # checking for the 5 directories and fastq files if [[ -e ${outdir}/${sra}/${sra}_1.fastq ]] &&\\ [[ -e ${outdir}/${sra}/${sra}_2.fastq ]]; then echo \"[---$SN---] ($(date)) EXISTS: Demo Sample ${sra} paired fastq files\" else read -r -d '' cmd1 <<EOF fastq-dump --outdir ${outdir}/${sra} \\ --split-files\\ --origfmt\\ --minSpotId 30000\\ --maxSpotId 40000\\ ${sra} EOF echo -e \"[---$SN---] ($(date)) $stage for ${sra} command:\\n\\n$cmd1\\n\" eval $cmd1 [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $stage\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $stage fastq-dump failed for sample ${sra}\" && exit 1 ) fi if [[ ! -e ${outdir}/${sra}/${sra}_1.fastq ]] &&\\ [[ ! -e ${outdir}/${sra}/${sra}_2.fastq ]]; then echo \"[---$SN---] ($(date)) FAILED: file downloading SRA data for ${sra} did not complete\" && exit 1 fi # Step 2: Haphpipe_assemble_02 pipeline=\"haphpipe_assemble_02\" echo -e \"\\n[---$SN---] ($(date)) Stage: $pipeline\" # checking for both fastq files and final.fna if [[ -e ${outdir}/${sra}/${sra}_1.fastq ]] &&\\ [[ -e ${outdir}/${sra}/${sra}_2.fastq ]] &&\\ [[ -e $outdir/${sra}/${pipeline}/final.fna ]]; then echo \"[---$SN---] ($(date)) EXISTS: $pipeline final.fna for sample ${sra}\" else read -r -d '' cmd2 <<EOF ${pipeline} ${outdir}/${sra}/${sra}_1.fastq\\ ${outdir}/${sra}/${sra}_2.fastq\\ ${outdir}/refs/HIV_B.K03455.HXB2.amplicons.fasta\\ ${sra}\\ ${outdir}/${sra}/${pipeline} EOF echo -e \"[---$SN---] ($(date)) $pipeline command for ${sra}:\\n\\n$cmd2\\n\" eval $cmd2 [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $pipeline for ${sra} samples\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $pipeline\" && exit 1 ) fi # Step 3: PredictHaplo stage=\"predict_haplo\" echo -e \"\\n[---$SN---] ($(date)) Stage: $stage\" if [[ -e ${outdir}/${sra}/${pipeline}/ph_haplotypes_comb.fna ]]; then echo \"[---$SN---] ($(date)) EXISTS: $stage corrected_1.fastq,corrected_2.fastq\" else command -v PredictHaplo-Paired >/dev/null 2>&1 if [[ $? -eq 0 ]] ; then echo -e \"[---$SN---] ($(date)) $stage PredictHaplo present\" read -r -d '' cmd1 <<EOF haphpipe predict_haplo\\ --fq1 ${outdir}/${sra}/${pipeline}/corrected_1.fastq\\ --fq2 ${outdir}/${sra}/${pipeline}/corrected_2.fastq\\ --ref_fa ${outdir}/${sra}/${pipeline}/final.fna\\ --logfile ${outdir}/${sra}/${pipeline}/haphpipe_PH.out\\ --outdir ${outdir}/${sra}/${pipeline} EOF echo -e \"[---$SN---] ($(date)) $stage command:\\n\\n$cmd1\\n\" eval $cmd1 for ph in ${outdir}/${sra}/${pipeline}/PH*; do read -r -d '' cmd2 <<EOF haphpipe ph_parser\\ --haplotypes_fa $ph/PH*.best_*.fas\\ --logfile ${outdir}/${sra}/${pipeline}/haphpipe_PH.out\\ --outdir ${ph}\\ --prefix ${sra}_$(basename $ph) EOF echo -e \"[---$SN---] ($(date)) $stage command:\\n\\n$cmd2\\n\" eval $cmd2 cat ${outdir}/${sra}/${pipeline}/PH*/ph_haplotypes.fna > ${outdir}/${sra}/${pipeline}/ph_haplotypes_comb.fna done fi [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $stage\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $stage\" && exit 1 ) fi ############################################################################### # Step 2c: Sample 3: SRR8525938 ############################################################################### # Step 1: Fastq-dump stage=\"Demo Sample 3: SRR8525938\" sra='SRR8525938' echo -e \"\\n[---$SN---] ($(date)) Stage: $stage\" # checking for the 5 directories and fastq files if [[ -e ${outdir}/${sra}/${sra}_1.fastq ]] &&\\ [[ -e ${outdir}/${sra}/${sra}_2.fastq ]]; then echo \"[---$SN---] ($(date)) EXISTS: Demo Sample ${sra} paired fastq files\" else read -r -d '' cmd1 <<EOF fastq-dump --outdir ${outdir}/${sra} \\ --split-files\\ --origfmt\\ --minSpotId 30000\\ --maxSpotId 40000\\ ${sra} EOF echo -e \"[---$SN---] ($(date)) $stage for ${sra} command:\\n\\n$cmd1\\n\" eval $cmd1 [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $stage\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $stage fastq-dump failed for sample ${sra}\" && exit 1 ) fi if [[ ! -e ${outdir}/${sra}/${sra}_1.fastq ]] &&\\ [[ ! -e ${outdir}/${sra}/${sra}_2.fastq ]]; then echo \"[---$SN---] ($(date)) FAILED: file downloading SRA data for ${sra} did not complete\" && exit 1 fi # Step 2: Haphpipe_assemble_02 pipeline=\"haphpipe_assemble_02\" echo -e \"\\n[---$SN---] ($(date)) Stage: $pipeline\" # checking for both fastq files and final.fna if [[ -e ${outdir}/${sra}/${sra}_1.fastq ]] &&\\ [[ -e ${outdir}/${sra}/${sra}_2.fastq ]] &&\\ [[ -e $outdir/${sra}/${pipeline}/final.fna ]]; then echo \"[---$SN---] ($(date)) EXISTS: $pipeline final.fna for sample ${sra}\" else read -r -d '' cmd2 <<EOF ${pipeline} ${outdir}/${sra}/${sra}_1.fastq\\ ${outdir}/${sra}/${sra}_2.fastq\\ ${outdir}/refs/HIV_B.K03455.HXB2.amplicons.fasta\\ ${sra}\\ ${outdir}/${sra}/${pipeline} EOF echo -e \"[---$SN---] ($(date)) $pipeline command for ${sra}:\\n\\n$cmd2\\n\" eval $cmd2 [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $pipeline for ${sra} samples\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $pipeline\" && exit 1 ) fi # Step 3: PredictHaplo stage=\"predict_haplo\" echo -e \"\\n[---$SN---] ($(date)) Stage: $stage\" if [[ -e ${outdir}/${sra}/${pipeline}/ph_haplotypes_comb.fna ]]; then echo \"[---$SN---] ($(date)) EXISTS: $stage corrected_1.fastq,corrected_2.fastq\" else command -v PredictHaplo-Paired >/dev/null 2>&1 if [[ $? -eq 0 ]] ; then echo -e \"[---$SN---] ($(date)) $stage PredictHaplo present\" read -r -d '' cmd1 <<EOF haphpipe predict_haplo\\ --fq1 ${outdir}/${sra}/${pipeline}/corrected_1.fastq\\ --fq2 ${outdir}/${sra}/${pipeline}/corrected_2.fastq\\ --ref_fa ${outdir}/${sra}/${pipeline}/final.fna\\ --logfile ${outdir}/${sra}/${pipeline}/haphpipe_PH.out\\ --outdir ${outdir}/${sra}/${pipeline} EOF echo -e \"[---$SN---] ($(date)) $stage command:\\n\\n$cmd1\\n\" eval $cmd1 for ph in ${outdir}/${sra}/${pipeline}/PH*; do read -r -d '' cmd2 <<EOF haphpipe ph_parser\\ --haplotypes_fa $ph/PH*.best_*.fas\\ --logfile ${outdir}/${sra}/${pipeline}/haphpipe_PH.out\\ --outdir ${ph}\\ --prefix ${sra}_$(basename $ph) EOF echo -e \"[---$SN---] ($(date)) $stage command:\\n\\n$cmd2\\n\" eval $cmd2 cat ${outdir}/${sra}/${pipeline}/PH*/ph_haplotypes.fna > ${outdir}/${sra}/${pipeline}/ph_haplotypes_comb.fna done fi [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $stage\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $stage\" && exit 1 ) fi ############################################################################### # Step 2d: Sample 4: SRR8525939 ############################################################################### # Step 1: Fastq-dump stage=\"Demo Sample 4: SRR8525939\" sra='SRR8525939' echo -e \"\\n[---$SN---] ($(date)) Stage: $stage\" # checking for the 5 directories and fastq files if [[ -e ${outdir}/${sra}/${sra}_1.fastq ]] &&\\ [[ -e ${outdir}/${sra}/${sra}_2.fastq ]]; then echo \"[---$SN---] ($(date)) EXISTS: Demo Sample ${sra} paired fastq files\" else read -r -d '' cmd1 <<EOF fastq-dump --outdir ${outdir}/${sra} \\ --split-files\\ --origfmt\\ --minSpotId 30000\\ --maxSpotId 40000\\ ${sra} EOF echo -e \"[---$SN---] ($(date)) $stage for ${sra} command:\\n\\n$cmd1\\n\" eval $cmd1 [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $stage\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $stage fastq-dump failed for sample ${sra}\" && exit 1 ) fi if [[ ! -e ${outdir}/${sra}/${sra}_1.fastq ]] &&\\ [[ ! -e ${outdir}/${sra}/${sra}_2.fastq ]]; then echo \"[---$SN---] ($(date)) FAILED: file downloading SRA data for ${sra} did not complete\" && exit 1 fi # Step 2: Haphpipe_assemble_02 pipeline=\"haphpipe_assemble_02\" echo -e \"\\n[---$SN---] ($(date)) Stage: $pipeline\" # checking for both fastq files and final.fna if [[ -e ${outdir}/${sra}/${sra}_1.fastq ]] &&\\ [[ -e ${outdir}/${sra}/${sra}_2.fastq ]] &&\\ [[ -e $outdir/${sra}/${pipeline}/final.fna ]]; then echo \"[---$SN---] ($(date)) EXISTS: $pipeline final.fna for sample ${sra}\" else read -r -d '' cmd2 <<EOF ${pipeline} ${outdir}/${sra}/${sra}_1.fastq\\ ${outdir}/${sra}/${sra}_2.fastq\\ ${outdir}/refs/HIV_B.K03455.HXB2.amplicons.fasta\\ ${sra}\\ ${outdir}/${sra}/${pipeline} EOF echo -e \"[---$SN---] ($(date)) $pipeline command for ${sra}:\\n\\n$cmd2\\n\" eval $cmd2 [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $pipeline for ${sra} samples\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $pipeline\" && exit 1 ) fi # Step 3: PredictHaplo stage=\"predict_haplo\" echo -e \"\\n[---$SN---] ($(date)) Stage: $stage\" if [[ -e ${outdir}/${sra}/${pipeline}/ph_haplotypes_comb.fna ]]; then echo \"[---$SN---] ($(date)) EXISTS: $stage corrected_1.fastq,corrected_2.fastq\" else command -v PredictHaplo-Paired >/dev/null 2>&1 if [[ $? -eq 0 ]] ; then echo -e \"[---$SN---] ($(date)) $stage PredictHaplo present\" read -r -d '' cmd1 <<EOF haphpipe predict_haplo\\ --fq1 ${outdir}/${sra}/${pipeline}/corrected_1.fastq\\ --fq2 ${outdir}/${sra}/${pipeline}/corrected_2.fastq\\ --ref_fa ${outdir}/${sra}/${pipeline}/final.fna\\ --logfile ${outdir}/${sra}/${pipeline}/haphpipe_PH.out\\ --outdir ${outdir}/${sra}/${pipeline} EOF echo -e \"[---$SN---] ($(date)) $stage command:\\n\\n$cmd1\\n\" eval $cmd1 for ph in ${outdir}/${sra}/${pipeline}/PH*; do read -r -d '' cmd2 <<EOF haphpipe ph_parser\\ --haplotypes_fa $ph/PH*.best_*.fas\\ --logfile ${outdir}/${sra}/${pipeline}/haphpipe_PH.out\\ --outdir ${ph}\\ --prefix ${sra}_$(basename $ph) EOF echo -e \"[---$SN---] ($(date)) $stage command:\\n\\n$cmd2\\n\" eval $cmd2 cat ${outdir}/${sra}/${pipeline}/PH*/ph_haplotypes.fna > ${outdir}/${sra}/${pipeline}/ph_haplotypes_comb.fna done fi [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $stage\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $stage\" && exit 1 ) fi ############################################################################### # Step 2e: Sample 5: SRR8525940 ############################################################################### # Step 1: Fastq-dump stage=\"Demo Sample 5: SRR8525940\" sra='SRR8525940' echo -e \"\\n[---$SN---] ($(date)) Stage: $stage\" # checking for the 5 directories and fastq files if [[ -e ${outdir}/${sra}/${sra}_1.fastq ]] &&\\ [[ -e ${outdir}/${sra}/${sra}_2.fastq ]]; then echo \"[---$SN---] ($(date)) EXISTS: Demo Sample ${sra} paired fastq files\" else read -r -d '' cmd1 <<EOF fastq-dump --outdir ${outdir}/${sra} \\ --split-files\\ --origfmt\\ --minSpotId 30000\\ --maxSpotId 40000\\ ${sra} EOF echo -e \"[---$SN---] ($(date)) $stage for ${sra} command:\\n\\n$cmd1\\n\" eval $cmd1 [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $stage\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $stage fastq-dump failed for sample ${sra}\" && exit 1 ) fi if [[ ! -e ${outdir}/${sra}/${sra}_1.fastq ]] &&\\ [[ ! -e ${outdir}/${sra}/${sra}_2.fastq ]]; then echo \"[---$SN---] ($(date)) FAILED: file downloading SRA data for ${sra} did not complete\" && exit 1 fi # Step 2: Haphpipe_assemble_02 pipeline=\"haphpipe_assemble_02\" echo -e \"\\n[---$SN---] ($(date)) Stage: $pipeline\" # checking for both fastq files and final.fna if [[ -e ${outdir}/${sra}/${sra}_1.fastq ]] &&\\ [[ -e ${outdir}/${sra}/${sra}_2.fastq ]] &&\\ [[ -e $outdir/${sra}/${pipeline}/final.fna ]]; then echo \"[---$SN---] ($(date)) EXISTS: $pipeline final.fna for sample ${sra}\" else read -r -d '' cmd2 <<EOF ${pipeline} ${outdir}/${sra}/${sra}_1.fastq\\ ${outdir}/${sra}/${sra}_2.fastq\\ ${outdir}/refs/HIV_B.K03455.HXB2.amplicons.fasta\\ ${sra}\\ ${outdir}/${sra}/${pipeline} EOF echo -e \"[---$SN---] ($(date)) $pipeline command for ${sra}:\\n\\n$cmd2\\n\" eval $cmd2 [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $pipeline for ${sra} samples\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $pipeline\" && exit 1 ) fi # Step 3: PredictHaplo stage=\"predict_haplo\" echo -e \"\\n[---$SN---] ($(date)) Stage: $stage\" if [[ -e ${outdir}/${sra}/${pipeline}/ph_haplotypes_comb.fna ]]; then echo \"[---$SN---] ($(date)) EXISTS: $stage corrected_1.fastq,corrected_2.fastq\" else command -v PredictHaplo-Paired >/dev/null 2>&1 if [[ $? -eq 0 ]] ; then echo -e \"[---$SN---] ($(date)) $stage PredictHaplo present\" read -r -d '' cmd1 <<EOF haphpipe predict_haplo\\ --fq1 ${outdir}/${sra}/${pipeline}/corrected_1.fastq\\ --fq2 ${outdir}/${sra}/${pipeline}/corrected_2.fastq\\ --ref_fa ${outdir}/${sra}/${pipeline}/final.fna\\ --logfile ${outdir}/${sra}/${pipeline}/haphpipe_PH.out\\ --outdir ${outdir}/${sra}/${pipeline} EOF echo -e \"[---$SN---] ($(date)) $stage command:\\n\\n$cmd1\\n\" eval $cmd1 for ph in ${outdir}/${sra}/${pipeline}/PH*; do read -r -d '' cmd2 <<EOF haphpipe ph_parser\\ --haplotypes_fa $ph/PH*.best_*.fas\\ --logfile ${outdir}/${sra}/${pipeline}/haphpipe_PH.out\\ --outdir ${ph}\\ --prefix ${sra}_$(basename $ph) EOF echo -e \"[---$SN---] ($(date)) $stage command:\\n\\n$cmd2\\n\" eval $cmd2 cat ${outdir}/${sra}/${pipeline}/PH*/ph_haplotypes.fna > ${outdir}/${sra}/${pipeline}/ph_haplotypes_comb.fna done fi [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $stage\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $stage\" && exit 1 ) fi ############################################################################### # Step 3: Run MSA with MAFFT ############################################################################### stage=\"multiple_align\" echo -e \"\\n[---$SN---] ($(date)) Stage: $stage\" if [[ -e ${outdir}/hp_alignments/alignment_region00.fasta ]] &&\\ [[ -e ${outdir}/hp_alignments/alignment_region01.fasta ]] &&\\ [[ -e ${outdir}/hp_alignments/alignment_region02.fasta ]]; then echo \"[---$SN---] ($(date)) EXISTS: $stage alignment_region00.fasta,alignment_region01.fasta,alignment_region02.fasta\" else # check for PredictHaplo command -v PredictHaplo-Paired >/dev/null 2>&1 if [[ $? -eq 0 ]] ; then read -r -d '' cmd1 <<EOF ls -d ${outdir}/SRR*/${pipeline} > ${outdir}/dir_list.txt &&\\ ls -d ${outdir}/SRR*/${pipeline}/PH0* >> ${outdir}/dir_list.txt EOF echo -e \"[---$SN---] ($(date)) $stage command:\\n\\n$cmd1\\n\" eval $cmd1 read -r -d '' cmd2 <<EOF haphpipe multiple_align\\ --ncpu $ncpu\\ --dir_list ${outdir}/dir_list.txt\\ --ref_gtf ${outdir}/refs/HIV_B.K03455.HXB2.gtf\\ --logfile haphpipe.out\\ --phylipout\\ --outdir ${outdir} EOF echo -e \"[---$SN---] ($(date)) $stage command:\\n\\n$cmd2\\n\" eval $cmd2 [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $stage\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $stage\" && exit 1 ) # if no PredictHaplo, execute these commands else read -r -d '' cmd1 <<EOF ls -d ${outdir}/SRR*/${pipeline} > ${outdir}/dir_list.txt EOF echo -e \"[---$SN---] ($(date)) $stage command:\\n\\n$cmd1\\n\" eval $cmd1 read -r -d '' cmd2 <<EOF haphpipe multiple_align\\ --ncpu $ncpu\\ --dir_list ${outdir}/dir_list.txt\\ --ref_gtf ${outdir}/refs/HIV_B.K03455.HXB2.gtf\\ --logfile haphpipe.out\\ --phylipout\\ --outdir ${outdir} EOF echo -e \"[---$SN---] ($(date)) $stage command:\\n\\n$cmd2\\n\" eval $cmd2 [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $stage\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $stage\" && exit 1 ) fi fi ############################################################################### # Step 4: ModelTest ############################################################################### stage=\"model_test\" echo -e \"\\n[---$SN---] ($(date)) Stage: $stage\" if [[ -e ${outdir}/alignment_region00_modeltest_results.out ]] &&\\ [[ -e ${outdir}/alignment_region01_modeltest_results.out ]] &&\\ [[ -e ${outdir}/alignment_region02_modeltest_results.out ]]; then echo \"[---$SN---] ($(date)) EXISTS: $stage alignment_region00_modeltest_results.out,alignment_region01_modeltest_results.out,alignment_region02_modeltest_results.out\" else for region in ${outdir}/hp_alignments/alignment_region??.fasta; do reg=${region%.fasta} read -r -d '' cmd <<EOF haphpipe model_test\\ --seqs ${region}\\ --run_id $(basename $reg)\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir}\\ --template raxml\\ --ncpu ${ncpu} EOF echo -e \"[---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $stage for $region\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $stage for $region\" && exit 1 ) done fi ############################################################################### # Step 5: Build tree with RAxML ############################################################################### stage=\"build_tree_NG\" echo -e \"\\n[---$SN---] ($(date)) Stage: $stage\" if [[ -e ${outdir}/hp_tree/alignment_region00.raxml.support ]] &&\\ [[ -e ${outdir}/hp_tree/alignment_region01.raxml.support ]] &&\\ [[ -e ${outdir}/hp_tree/alignment_region02.raxml.support ]]; then echo \"[---$SN---] ($(date)) EXISTS: $stage alignment_region00.raxml.support, alignment_region01.raxml.support, alignment_region02.raxml.support\" else # check for PredictHaplo command -v PredictHaplo-Paired >/dev/null 2>&1 if [[ $? -eq 0 ]] ; then for alignment in ${outdir}/hp_alignments/alignment_region??.phy; do reg=${alignment%.phy} read -r -d '' cmd <<EOF haphpipe build_tree_NG\\ --all\\ --seqs ${alignment}\\ --output_name $(basename $reg)\\ --model GTR+I\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir}\\ --in_type PHYLIP EOF echo -e \"[---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $stage\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $stage\" && exit 1 ) done else for alignment in ${outdir}/hp_alignments/alignment_region00.phy ${outdir}/hp_alignments/alignment_region01.phy; do reg=${alignment%.phy} read -r -d '' cmd <<EOF haphpipe build_tree_NG\\ --all\\ --seqs ${alignment}\\ --output_name $(basename $reg)\\ --model GTR+I\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir}\\ --in_type PHYLIP EOF echo -e \"[---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n\" eval $cmd [[ $? -eq 0 ]] && echo \"[---$SN---] ($(date)) COMPLETED: $stage\" && echo \"NOTE: region02 did not run because there are only 3 sequences in the alignment\" || \\ ( echo \"[---$SN---] ($(date)) FAILED: $stage\" && exit 1 ) done fi fi #---Complete job t2=$(date +\"%s\") diff=$(($t2-$t1)) echo \"[---$SN---] ($(date)) $(($diff / 60)) minutes and $(($diff % 60)) seconds elapsed.\" echo \"[---$SN---] ($(date)) $SN COMPLETE.\" The structure of this interactive demo is such: One sample (SRR8525886) will be used an example, and the single bash command for each step will be shown. Step 0: Get the reference data needed for this interactive demo. hp_demo --refonly Step 1: Pull sample data from SRA with fastq-dump. fastq-dump --outdir haphpipe_demo/SRR8525886 \\ --split-files \\ --origfmt \\ --minSpotId 30000 \\ --maxSpotId 40000 \\ SRR8525886 Reminder that the above code is the same as this command on a single line - the \\ just makes it more visually readable. fastq-dump --outdir haphpipe_demo/SRR8525886 --split-files --origfmt --minSpotId 30000 --maxSpotId 40000 SRR8525886 Step 2: Run haphpipe_assemble_02 haphpipe_assemble_02 \\ haphpipe_demo/SRR8525886/SRR8525886_1.fastq \\ haphpipe_demo/SRR8525886/SRR8525886_2.fastq \\ haphpipe_demo/refs/HIV_B.K03455.HXB2.amplicons.fasta \\ SRR8525886 \\ haphpipe_demo/SRR8525886/haphpipe_assemble_02 (if PredictHaplo is installed) Step 3a: Run PredictHaplo haphpipe predict_haplo\\ --fq1 haphpipe_demo/SRR8525886/haphpipe_assemble_02/corrected_1.fastq \\ --fq2 haphpipe_demo/SRR8525886/haphpipe_assemble_02/corrected_2.fastq \\ --ref_fa haphpipe_demo/SRR8525886/haphpipe_assemble_02/final.fna \\ --logfile haphpipe_demo/SRR8525886/haphpipe_assemble_02/haphpipe_PH.out \\ --outdir haphpipe_demo/SRR8525886/haphpipe_assemble_02 Step 3b: Run PredictHaplo Parser for each amplicon region Individually: # PH01 haphpipe ph_parser \\ --haplotypes_fa haphpipe_demo/SRR8525886/haphpipe_assemble_02/PH01_PRRT/PH*.best_*.fas \\ --logfile haphpipe_demo/SRR8525886/haphpipe_assemble_02/haphpipe_PH.out \\ --outdir haphpipe_demo/SRR8525886/haphpipe_assemble_02/PH01_PRRT \\ --prefix ${sra}_PH01_PRRT # PH02 haphpipe ph_parser \\ --haplotypes_fa haphpipe_demo/SRR8525886/haphpipe_assemble_02/PH02_INT/PH*.best_*.fas \\ --logfile haphpipe_demo/SRR8525886/haphpipe_assemble_02/haphpipe_PH.out \\ --outdir haphpipe_demo/SRR8525886/haphpipe_assemble_02/PH02_INT \\ --prefix SRR8525886_PH02_INT # PH03 haphpipe ph_parser \\ --haplotypes_fa haphpipe_demo/SRR8525886/haphpipe_assemble_02/PH03_gp120/PH*.best_*.fas \\ --logfile haphpipe_demo/SRR8525886/haphpipe_assemble_02/haphpipe_PH.out \\ --outdir haphpipe_demo/SRR8525886/haphpipe_assemble_02/PH03_gp120 \\ --prefix SRR8525886_PH03_gp120 Or you can run ph_parser in a loop for all the haplotype regions: for ph in haphpipe_demo/SRR8525886/haphpipe_assemble_02/PH*; do haphpipe ph_parser \\ --haplotypes_fa ${ph}/PH*.best_*.fas \\ --logfile $(dirname $ph)/haphpipe_PH.out \\ --outdir ${ph} \\ --prefix SRR8525886_$(basename $ph) done Step 4: Rerun steps 1-3 for all samples, make sure to replace the accession number. Step 5: Run multiple sequence alignment using MAFFT. Make a file with a list of directories containing the final.fna files and the ph_haplotype.fna : ls -d haphpipe_demo/SRR*/haphpipe_assemble_02 > haphpipe_demo/dir_list.txt &&\\ ls -d haphpipe_demo/SRR*/haphpipe_assemble_02/PH0* >> haphpipe_demo/dir_list.txt If PredictHaplo was not installed, you only need the first line: ls -d haphpipe_demo/SRR*/haphpipe_assemble_02 > haphpipe_demo/dir_list.txt Now, run multiple_align haphpipe multiple_align \\ --ncpu 1 \\ --dir_list haphpipe_demo/dir_list.txt \\ --ref_gtf haphpipe_demo/refs/HIV_B.K03455.HXB2.gtf \\ --logfile haphpipe.out \\ --phylipout \\ --outdir haphpipe_demo Step 6: Estimate best-fit model of evolution using ModelTest-NG # Region00 - this is amplicon PRRT haphpipe model_test \\ --seqs haphpipe_demo/hp_alignments/alignment_region00.fasta \\ --run_id alignment_region00 \\ --logfile haphpipe_demo/haphpipe.out \\ --outdir haphpipe_demo \\ --template raxml \\ --ncpu 1 # Region01 - this is amplicon INT haphpipe model_test \\ --seqs haphpipe_demo/hp_alignments/alignment_region01.fasta \\ --run_id alignment_region01 \\ --logfile haphpipe_demo/haphpipe.out \\ --outdir haphpipe_demo \\ --template raxml \\ --ncpu 1 # Region02 - this is amplicon gp120 haphpipe model_test \\ --seqs haphpipe_demo/hp_alignments/alignment_region02.fasta \\ --run_id alignment_region02 \\ --logfile haphpipe_demo/haphpipe.out \\ --outdir haphpipe_demo \\ --template raxml \\ --ncpu 1 Step 7: Build a phylogenetic tree for each region using RAXML. The models can be changed according to the output of ModelTest (step 6 - above). Here we just show with model GTRGAMMAX. # Region00 - this is amplicon PRRT haphpipe build_tree_NG \\ --all \\ --seqs haphpipe_demo/hp_alignments/alignment_region00.fasta \\ --output_name alignment_region00 \\ --model GTRGAMMAX\\ --logfile haphpipe_demo/haphpipe.out\\ --outdir haphpipe_demo # Region01 - this is amplicon INT haphpipe build_tree_NG \\ --all \\ --seqs haphpipe_demo/hp_alignments/alignment_region01.fasta \\ --output_name alignment_region01 \\ --model GTRGAMMAX\\ --logfile haphpipe_demo/haphpipe.out\\ --outdir haphpipe_demo If PredictHaplo is not run, there is not enough taxa - only 3 - to build a tree for region02 gp120. # Region02 - this is amplicon gp120 haphpipe build_tree_NG \\ --all \\ --seqs haphpipe_demo/hp_alignments/alignment_region02.fasta \\ --output_name alignment_region02 \\ --model GTRGAMMAX\\ --logfile haphpipe_demo/haphpipe.out\\ --outdir haphpipe_demo","title":"Running Demo Interactively"},{"location":"expipes/","text":"The example pipelines are written in bash scripting language. The reference files used in both examples are included in the demo data. To run in haphpipe, execute one of the following lines: haphpipe_assemble_01 read1.fq.gz read2.fq.gz ../refs/HIV_B.K03455.HXB2.fasta ../refs/HIV_B.K03455.HXB2.gtf sampleID haphpipe_assemble_02 read1.fq.gz read2.fq.gz ../refs/HIV_B.K03455.HXB2.amplicons.fasta sampleID Pipeline 1 implements amplicon assembly using a de novo approach. Reads are error-corrected and used to refine the initial assembly, with up to 5 refinement steps. Pipeline 2 implements amplicon assembly using a reference-based mapping approach. Reads are error-corrected and used to refine the initial assembly, with up to 5 refinement steps. Pipeline 1: haphpipe_assemble_01 This pipeline implements de novo assembly. Reads are first trimmed ( trim_reads ) and used as input for denovo assembly ( assemble_denovo ). The de novo assembly stage automatically performs error correction on the trimmed reads. The assembled contigs are used as input for amplicon assembly ( assemble_amplicons ) along with reference FASTA and GTF files. The assembly is then iteratively refined up to five times ( refine_assembly ) by mapping corrected reads to the assembled FASTA file and lastly finalized ( finalize_assembly ), resulting in a FASTA file with final consensus sequences, final VCF, and aligned BAM file. To see the input information for Pipeline 1, use the -h option again like so: haphpipe_assemble_01 -h , and it will show the output: USAGE: haphpipe_assemble_01 [read1] [read2] [reference_fasta] [reference_gtf] [samp_id] <outdir> ----- HAPHPIPE assembly pipeline 01 ----- This pipeline implements amplicon assembly using a denovo approach. Reads are error-corrected and used to refine the initial assembly, with up to 5 refinement steps. Input: read1: Fastq file for read 1. May be compressed (.gz) read2: Fastq file for read 2. May be compressed (.gz) reference_fasta: Reference sequence (fasta) reference_gtf: Amplicon regions (GTF) samp_id: Sample ID outdir: Output directory (default is [sample_dir]/haphpipe_assemble_01) To use this pipeline with your own data, replace [read1] with your read1 fastq file, [read2] with your read2 fastq file, [reference_fasta] with the reference fasta file of your choice, [reference_gtf] with the reference GTF file corresponding to the reference fasta file, and [samp_id] with the sample ID label. See examples below: General command to execute pipeline 1: haphpipe_assemble_01 samp/read1.fq.gz samp/read2.fq.gz refs/ref.fasta refs/ref.gtf samp Example command to run with demo samples: haphpipe_assemble_01 SRR8525886/SRR8525886_1.fastq SRR8525886/SRR8525886_2.fastq refs/HIV_B.K03455.HXB2.fasta refs/HIV_B.K03455.HXB2.gtf SRR8525886 SRR8525886 Pipeline 2: haphpipe_assemble_02 This pipeline implements reference-based mapping assembly. Reads are first trimmed ( trim_reads ) and error-corrected ( ec_reads ). The corrected reads are used as input for reference-based mapping assembly ( refine_assembly ) for up to five iterations. Lastly, the assembly is finalized ( finalize_assembly ) by mapping reads onto the refined reference sequence. The final output is a FASTA file with final consensus sequences, final VCF, and aligned BAM file. To see the input information for Pipeline 2, use the -h option again like so: haphpipe_assemble_02 -h , and it will show the output: USAGE: haphpipe_assemble_02 [read1] [read2] [amplicons_fasta] [samp_id] <outdir> ----- HAPHPIPE assembly pipeline 02 ----- This pipeline implements amplicon assembly using a reference-based approach. Reads are error-corrected and aligned to provided amplicon reference with up to five refinement steps. Input: read1: Fastq file for read 1. May be compressed (.gz) read2: Fastq file for read 2. May be compressed (.gz) amplicons_fasta: Amplicon reference sequence (fasta) samp_id: Sample ID outdir: Output directory (default is sample_dir/haphpipe_assemble_02) To use this pipeline with your own data, replace [read1] with your read1 fastq file, [read2] with your read2 fastq file, [amplicons_fasta] with the reference fasta file containing as many amplicons as you desire, and [samp_id] with the sample ID label. See examples below: General command to execute pipeline 2: haphpipe_assemble_02 samp/read1.fq.gz samp/read2.fq.gz refs/ref.fasta samp Example command to run with demo samples: haphpipe_assemble_02 SRR8525886/SRR8525886_1.fastq SRR8525886/SRR8525886_2.fastq refs/HIV_B.K03455.HXB2.amplicons fasta SRR8525886 SRR8525886","title":"Example Pipelines"},{"location":"expipes/#pipeline-1-haphpipe_assemble_01","text":"This pipeline implements de novo assembly. Reads are first trimmed ( trim_reads ) and used as input for denovo assembly ( assemble_denovo ). The de novo assembly stage automatically performs error correction on the trimmed reads. The assembled contigs are used as input for amplicon assembly ( assemble_amplicons ) along with reference FASTA and GTF files. The assembly is then iteratively refined up to five times ( refine_assembly ) by mapping corrected reads to the assembled FASTA file and lastly finalized ( finalize_assembly ), resulting in a FASTA file with final consensus sequences, final VCF, and aligned BAM file. To see the input information for Pipeline 1, use the -h option again like so: haphpipe_assemble_01 -h , and it will show the output: USAGE: haphpipe_assemble_01 [read1] [read2] [reference_fasta] [reference_gtf] [samp_id] <outdir> ----- HAPHPIPE assembly pipeline 01 ----- This pipeline implements amplicon assembly using a denovo approach. Reads are error-corrected and used to refine the initial assembly, with up to 5 refinement steps. Input: read1: Fastq file for read 1. May be compressed (.gz) read2: Fastq file for read 2. May be compressed (.gz) reference_fasta: Reference sequence (fasta) reference_gtf: Amplicon regions (GTF) samp_id: Sample ID outdir: Output directory (default is [sample_dir]/haphpipe_assemble_01) To use this pipeline with your own data, replace [read1] with your read1 fastq file, [read2] with your read2 fastq file, [reference_fasta] with the reference fasta file of your choice, [reference_gtf] with the reference GTF file corresponding to the reference fasta file, and [samp_id] with the sample ID label. See examples below: General command to execute pipeline 1: haphpipe_assemble_01 samp/read1.fq.gz samp/read2.fq.gz refs/ref.fasta refs/ref.gtf samp Example command to run with demo samples: haphpipe_assemble_01 SRR8525886/SRR8525886_1.fastq SRR8525886/SRR8525886_2.fastq refs/HIV_B.K03455.HXB2.fasta refs/HIV_B.K03455.HXB2.gtf SRR8525886 SRR8525886","title":"Pipeline 1: haphpipe_assemble_01"},{"location":"expipes/#pipeline-2-haphpipe_assemble_02","text":"This pipeline implements reference-based mapping assembly. Reads are first trimmed ( trim_reads ) and error-corrected ( ec_reads ). The corrected reads are used as input for reference-based mapping assembly ( refine_assembly ) for up to five iterations. Lastly, the assembly is finalized ( finalize_assembly ) by mapping reads onto the refined reference sequence. The final output is a FASTA file with final consensus sequences, final VCF, and aligned BAM file. To see the input information for Pipeline 2, use the -h option again like so: haphpipe_assemble_02 -h , and it will show the output: USAGE: haphpipe_assemble_02 [read1] [read2] [amplicons_fasta] [samp_id] <outdir> ----- HAPHPIPE assembly pipeline 02 ----- This pipeline implements amplicon assembly using a reference-based approach. Reads are error-corrected and aligned to provided amplicon reference with up to five refinement steps. Input: read1: Fastq file for read 1. May be compressed (.gz) read2: Fastq file for read 2. May be compressed (.gz) amplicons_fasta: Amplicon reference sequence (fasta) samp_id: Sample ID outdir: Output directory (default is sample_dir/haphpipe_assemble_02) To use this pipeline with your own data, replace [read1] with your read1 fastq file, [read2] with your read2 fastq file, [amplicons_fasta] with the reference fasta file containing as many amplicons as you desire, and [samp_id] with the sample ID label. See examples below: General command to execute pipeline 2: haphpipe_assemble_02 samp/read1.fq.gz samp/read2.fq.gz refs/ref.fasta samp Example command to run with demo samples: haphpipe_assemble_02 SRR8525886/SRR8525886_1.fastq SRR8525886/SRR8525886_2.fastq refs/HIV_B.K03455.HXB2.amplicons fasta SRR8525886 SRR8525886","title":"Pipeline 2: haphpipe_assemble_02"},{"location":"faq/","text":"HAPHPIPE How does HAPHPIPE compare to existing viral NGS pipelines and how was HAPHPIPE tested? Please refer to our validation study . NGS What is the difference between next-generation sequencing and Sanger sequencing? The critical difference between Sanger sequencing and NGS is sequencing volume. While the Sanger method only sequences a single DNA fragment at a time, NGS is massively parallel, sequencing millions of fragments simultaneously per run. This high-throughput process translates into sequencing hundreds to thousands of genes at one time. NGS also offers greater discovery power to detect novel or rare variants with deep sequencing. I am new to using NGS data. Where should I start? See this link for a beginner's guide to NGS analysis. Using the Command Line I am new to using the command line. Where should I start? See this link for a beginner's guide to the command line. What is a directory structure? Why is this important? In computing, a directory structure is the way an operating system\u2019s file system and its files are displayed to the user. As researchers and software engineers, we should make sure that someone who is unfamiliar with our project is able to look at our computer files and understand in detail what we did and why. We all have the rough experience where after a few months, we may simply not remember what we were up to when we created a particular set of files, or we may be forgetful about what conclusions were drew. We will either have to then spend time reconstructing previous experiments or lose whatever insights gained from those experiments. So maintaining a well-structured directory is essential to both software development and bioinformatics. What is an interactive job and how do I run one? When using HAPHPIPE on an HPC cluster, if stages/pipelines are not being run in a job through SLURM or a similar package manager, HAPHPIPE commands should be run on an interactive node of your HPC. This guide gives instructions for using interactive nodes on GW's ColonialOne. SRA How do I download SRA files manually? See the SRA search and download help site . Conda What is Conda? See the Conda documentation here . Bash I am new to using bash. Where should I start? See this link for a beginner's guide to bash scripting.","title":"FAQ"},{"location":"faq/#haphpipe","text":"How does HAPHPIPE compare to existing viral NGS pipelines and how was HAPHPIPE tested? Please refer to our validation study .","title":"HAPHPIPE"},{"location":"faq/#ngs","text":"What is the difference between next-generation sequencing and Sanger sequencing? The critical difference between Sanger sequencing and NGS is sequencing volume. While the Sanger method only sequences a single DNA fragment at a time, NGS is massively parallel, sequencing millions of fragments simultaneously per run. This high-throughput process translates into sequencing hundreds to thousands of genes at one time. NGS also offers greater discovery power to detect novel or rare variants with deep sequencing. I am new to using NGS data. Where should I start? See this link for a beginner's guide to NGS analysis.","title":"NGS"},{"location":"faq/#using-the-command-line","text":"I am new to using the command line. Where should I start? See this link for a beginner's guide to the command line. What is a directory structure? Why is this important? In computing, a directory structure is the way an operating system\u2019s file system and its files are displayed to the user. As researchers and software engineers, we should make sure that someone who is unfamiliar with our project is able to look at our computer files and understand in detail what we did and why. We all have the rough experience where after a few months, we may simply not remember what we were up to when we created a particular set of files, or we may be forgetful about what conclusions were drew. We will either have to then spend time reconstructing previous experiments or lose whatever insights gained from those experiments. So maintaining a well-structured directory is essential to both software development and bioinformatics. What is an interactive job and how do I run one? When using HAPHPIPE on an HPC cluster, if stages/pipelines are not being run in a job through SLURM or a similar package manager, HAPHPIPE commands should be run on an interactive node of your HPC. This guide gives instructions for using interactive nodes on GW's ColonialOne.","title":"Using the Command Line"},{"location":"faq/#sra","text":"How do I download SRA files manually? See the SRA search and download help site .","title":"SRA"},{"location":"faq/#conda","text":"What is Conda? See the Conda documentation here .","title":"Conda"},{"location":"faq/#bash","text":"I am new to using bash. Where should I start? See this link for a beginner's guide to bash scripting.","title":"Bash"},{"location":"help/","text":"Conda Bioconda Bioconda channels Bash Command line tutorial List of bash commands Bash scripting tutorial Additional scripting help: linuxconfig.org flaviocopesc.com ryanstutorials.net NGS overview: NYU resource Accessing on a PC using a virtual machine Guide to options VirtualBox","title":"Helpful Resources"},{"location":"help/#conda","text":"Bioconda Bioconda channels","title":"Conda"},{"location":"help/#bash","text":"Command line tutorial List of bash commands Bash scripting tutorial Additional scripting help: linuxconfig.org flaviocopesc.com ryanstutorials.net","title":"Bash"},{"location":"help/#ngs-overview","text":"NYU resource","title":"NGS overview:"},{"location":"help/#accessing-on-a-pc-using-a-virtual-machine","text":"Guide to options VirtualBox","title":"Accessing on a PC using a virtual machine"},{"location":"hp_annotate/","text":"The Description stage includes modules to annotate and describe consensus sequence(s) and haplotypes. Use -h after any command for a list of options. pairwise_align Apply correct coordinate system to final sequence(s) to facilitate downstream analyses. Input is the final sequence file in FASTA format, a reference sequence in FASTA format, and a reference GFT file. Output is a JSON file to be used in extract_pairwise . Usage: haphpipe pairwise_align [SETTINGS] --amplicons_fa <FASTA> --ref_fa <FASTA> --ref_gtf <GTF> [--outdir] (or): hp_pairwise_align [SETTINGS] --amplicons_fa <FASTA> --ref_fa <FASTA> --ref_gtf <GTF> [--outdir] Output files: pairwise_aligned.json Input/Output Arguments: Option Description --amplicons_fa Fasta file with assembled amplicons. --ref_fa Reference fasta file. --ref_gtf GTF format file containing amplicon regions. Primary and alternate coding regions should be provided in the attribute field (for amino acid alignment). --outdir Output directory (default: False). Settings: Option Description --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe pairwise_align --amplicons_fa final.fna --ref_fa HIV_B.K03455.HXB2.fasta --ref_gtf HIV_B.K03455.HXB2.gtf extract_pairwise Extract sequence regions from the pairwise alignment produced in pairwise_align . Input is the JSON file from pairwise_align . Output is either an unaligned nucleotide FASTA file, an aligned nucleotide FASTA file, an amino acid FASTA file, an amplicon GTF file, or a tab-separated values (TSV) file (default: nucleotide FASTA with regions of interest from GTF file used in pairwise_align ). Usage: haphpipe extract_pairwise [OPTIONS] [SETTINGS] --align_json <JSON> [--outdir] (or): hp_extract_pairwise [OPTIONS] [SETTINGS] --align_json <JSON> [--outdir] Output files: stdout.fasta Input/Output Arguments: Option Description --align_json JSON file describing alignment (output of pairwise_align module). --outfile Output file (default: stdout). Options: Option Description --outfmt Format for output: nuc_fa, aln_fa, amp_gtf, ost, or prot_fa (default: nuc_fa). --refreg Reference region. String format is ref:start-stop. For example, the region string to extract pol when aligned to HXB2 is HIV_B.K03455.HXB2:2085-5096. Settings: Option Description --debug Print commands but do not run (default: False). Example usage: haphpipe extract_pairwise --align_json pairwise_aligned.json --refreg HIV_B.K03455.HXB2:2085-5096 summary_stats Report summary statistics from an alignment and/or haplotype calling as TXT and TSV files. Input is a list of paths to directories (TXT format, one per line), each of which contain the following files: final_bt2.out , trimmomatic_summary.out , final.bam , final.fna , and final.vcf.gz . If applicable, also input a list of directories containing PredictHaplo summary files ( ph_summary.txt ). If amplicons were used in assembly, use the --amplicons option to report statistics per amplicon. Usage: haphpipe summary_stats [SETTINGS] --dir_list <TXT> [--ph_list <TXT>] [--amplicons] [--outdir] (or): hp_summary_stats [SETTINGS] --dir_list <TXT> [--ph_list <TXT>] [--amplicons] [--outdir] Output files: summary_stats.txt, summary_stats.tsv, PH_summary_stats.tsv Input/Output Arguments: Option Description --dir_list List of directories which include the required files, one on each line. --ph_list List of directories which include haplotype summary files, one on each line. --amplicons Amplicons used in assembly (default: False). Settings: Option Description --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Name for log file. --debug Print commands but do not run (default: False). Example usage: haphpipe summary_stats --dir_list demo_sra_list.txt --ph_list demo_sra_ph_list.txt --amplicons annotate_ from_ref Annotate consensus sequence from reference annotation. Input is JSON file from pairwise_align and reference GTF file. Usage: haphpipe annotate_from_ref [OPTIONS] [SETTINGS] --haplotypes_fa <best.fas> [--outdir] (or): hp_annotate_from_ref [OPTIONS] [SETTINGS] --haplotypes_fa <best.fas> [--outdir] Output files: Input/Output Arguments Settings: Example usage: (add)","title":"Description"},{"location":"hp_annotate/#pairwise_align","text":"Apply correct coordinate system to final sequence(s) to facilitate downstream analyses. Input is the final sequence file in FASTA format, a reference sequence in FASTA format, and a reference GFT file. Output is a JSON file to be used in extract_pairwise . Usage: haphpipe pairwise_align [SETTINGS] --amplicons_fa <FASTA> --ref_fa <FASTA> --ref_gtf <GTF> [--outdir] (or): hp_pairwise_align [SETTINGS] --amplicons_fa <FASTA> --ref_fa <FASTA> --ref_gtf <GTF> [--outdir] Output files: pairwise_aligned.json Input/Output Arguments: Option Description --amplicons_fa Fasta file with assembled amplicons. --ref_fa Reference fasta file. --ref_gtf GTF format file containing amplicon regions. Primary and alternate coding regions should be provided in the attribute field (for amino acid alignment). --outdir Output directory (default: False). Settings: Option Description --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe pairwise_align --amplicons_fa final.fna --ref_fa HIV_B.K03455.HXB2.fasta --ref_gtf HIV_B.K03455.HXB2.gtf","title":"pairwise_align"},{"location":"hp_annotate/#extract_pairwise","text":"Extract sequence regions from the pairwise alignment produced in pairwise_align . Input is the JSON file from pairwise_align . Output is either an unaligned nucleotide FASTA file, an aligned nucleotide FASTA file, an amino acid FASTA file, an amplicon GTF file, or a tab-separated values (TSV) file (default: nucleotide FASTA with regions of interest from GTF file used in pairwise_align ). Usage: haphpipe extract_pairwise [OPTIONS] [SETTINGS] --align_json <JSON> [--outdir] (or): hp_extract_pairwise [OPTIONS] [SETTINGS] --align_json <JSON> [--outdir] Output files: stdout.fasta Input/Output Arguments: Option Description --align_json JSON file describing alignment (output of pairwise_align module). --outfile Output file (default: stdout). Options: Option Description --outfmt Format for output: nuc_fa, aln_fa, amp_gtf, ost, or prot_fa (default: nuc_fa). --refreg Reference region. String format is ref:start-stop. For example, the region string to extract pol when aligned to HXB2 is HIV_B.K03455.HXB2:2085-5096. Settings: Option Description --debug Print commands but do not run (default: False). Example usage: haphpipe extract_pairwise --align_json pairwise_aligned.json --refreg HIV_B.K03455.HXB2:2085-5096","title":"extract_pairwise"},{"location":"hp_annotate/#summary_stats","text":"Report summary statistics from an alignment and/or haplotype calling as TXT and TSV files. Input is a list of paths to directories (TXT format, one per line), each of which contain the following files: final_bt2.out , trimmomatic_summary.out , final.bam , final.fna , and final.vcf.gz . If applicable, also input a list of directories containing PredictHaplo summary files ( ph_summary.txt ). If amplicons were used in assembly, use the --amplicons option to report statistics per amplicon. Usage: haphpipe summary_stats [SETTINGS] --dir_list <TXT> [--ph_list <TXT>] [--amplicons] [--outdir] (or): hp_summary_stats [SETTINGS] --dir_list <TXT> [--ph_list <TXT>] [--amplicons] [--outdir] Output files: summary_stats.txt, summary_stats.tsv, PH_summary_stats.tsv Input/Output Arguments: Option Description --dir_list List of directories which include the required files, one on each line. --ph_list List of directories which include haplotype summary files, one on each line. --amplicons Amplicons used in assembly (default: False). Settings: Option Description --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Name for log file. --debug Print commands but do not run (default: False). Example usage: haphpipe summary_stats --dir_list demo_sra_list.txt --ph_list demo_sra_ph_list.txt --amplicons","title":"summary_stats"},{"location":"hp_annotate/#annotate_-from_ref","text":"Annotate consensus sequence from reference annotation. Input is JSON file from pairwise_align and reference GTF file. Usage: haphpipe annotate_from_ref [OPTIONS] [SETTINGS] --haplotypes_fa <best.fas> [--outdir] (or): hp_annotate_from_ref [OPTIONS] [SETTINGS] --haplotypes_fa <best.fas> [--outdir] Output files: Input/Output Arguments Settings: Example usage: (add)","title":"annotate_ from_ref"},{"location":"hp_assemble/","text":"The assembly stage is designed to construct consensus sequence(s). Input reads (in FASTQ format) are assembled using either denovo assembly or reference-based alignment. Resulting consensus can be further refined. Use -h after any command for a list of options. assemble_denovo Assemble reads via de novo assembly using SPAdes ( documentation ). Input is reads in FASTQ format. Output is contigs in FNA format. Usage: haphpipe assemble_denovo [OPTIONS] [SETTINGS] --fq1 <FASTQ> --fq2 <FASTQ> --fqU <FASTQ> [--outdir] (or): hp_assemble_denovo [OPTIONS] [SETTINGS] --fq1 <FASTQ> --fq2 <FASTQ> --fqU <FASTQ> [--outdir] Output files: denovo_contigs.fna denovo_summary.txt Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --outdir Output directory (default: current directory). Options: Option Description --no_error_correction Do not perform error correction (default: False) --subsample Use a subsample of reads for assembly --seed Seed for random number generator (ignored if not subsampling) Settings: Option Description --ncpu Number of CPU to use (default: 1). --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe assemble_denovo --fq1 corrected_1.fastq --fq2 corrected_2.fastq --outdir denovo_assembly --no_error_correction TRUE assemble_amplicons Assemble contigs from de novo assembly using both a reference sequence and amplicon regions with MUMMER 3+ ( documentation ). Input is contigs and reference sequence in FASTA format and amplicon regions in GTF format. Usage: haphpipe assemble_amplicons [OPTIONS] [SETTINGS] --contigs_fa <FASTA> --ref_fa <FASTA> --ref_gtf <GTF> [--outdir] (or): hp_assemble_amplicons [OPTIONS] [SETTINGS] --contigs_fa <FASTA> --ref_fa <FASTA> --ref_gtf <GTF> [--outdir] Output files: amplicon_assembly.fna Input/Output Arguments: Option Description --contigs_fa Fasta file with assembled contigs. --ref_fa Fasta file with reference genome to scaffold against. --ref_gtf GTF format file containing amplicon regions. --outdir Output directory (default: current directory). Scaffold Options: Option Description --sample_id Sample ID (default: sampleXX). --padding Bases to include outside reference annotation (default: 50). Settings: Option Description --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe assemble_amplicons --contigs_fa denovo_contigs.fa --ref_fa HIV_B.K03455.HXB2.fasta --ref_gtf HIV_B.K03455.HXB2.gtf assemble_scaffold Scaffold contigs against a reference sequence with MUMMER 3+ ( documentation ). Input is contigs in FASTA format and reference sequence in FASTA format. Output is scaffold assembly, alligned scaffold, imputed scaffold, and padded scaffold in FASTA format. Usage: haphpipe assemble_scaffold [OPTIONS] [SETTINGS] --contigs_fa <FASTA> --ref_fa <FASTA> [--outdir] (or): hp_assemble_scaffold [OPTIONS] [SETTINGS] --contigs_fa <FASTA> --ref_fa <FASTA> [--outdir] Output files: scaffold_aligned.fa scaffold_assembly.fa scaffold_imputed.fa scaffold_padded.out Input/Output Arguments: Option Description --contigs_fa Fasta file with assembled contigs. --ref_fa Fasta file with reference genome to scaffold against. --outdir Output directory (default: current directory). Options: Option Description --seqname Name to append to scaffold sequence (default: sample01). Settings: Option Description --keep_tmp Additional options (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe assemble_scaffold --contigs_fa denovo_contigs.fa --ref_fa HIV_B.K03455.HXB2.fasta align_reads Map reads to reference sequence (instead of running de novo assembly) using Bowtie2 ( documentation ) and Picard ( documentation ). Input is reads in FASTQ format and reference sequence in FASTA format. Usage: haphpipe align_reads [OPTIONS] [SETTINGS] --fq1 <FASTQ> --fq2 <FASTQ> --fqU <FASTQ> --ref_fa <FASTA> [--outdir] (or): hp_align_reads [OPTIONS] [SETTINGS] --fq1 <FASTQ> --fq2 <FASTQ> --fqU <FASTQ> --ref_fa <FASTA> [--outdir] Output files: aligned.bam aligned.bt2.out Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --ref_fa Reference fasta file. --outdir Output directory (default: current directory). Options: Option Description --bt2_preset {very-fast, fast, sensitive,very-sensitive,very-fast-local,fast-local,sensitive-local,very-sensitive-local} --sample_id Sample ID. Used as read group ID in BAM (default: sampleXX). --no_realign Do not realign indels (default: False). --remove_duplicates Remove duplicates from final alignment. Otherwise duplicates are marked but not removed (default: False). --encoding {Phred+33,Phred+64} Quality score encoding. Settings: Option Description --ncpu Number of CPUs to use (default: 1). --xmx Maximum heap size for Java VM, in GB (default: 32). --keep_tmp Additional options (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe align_reads --fq1 corrected_1.fastq --fq2 corrected _2.fastq --ref_fa HIV_B.K03455.HXB2.fasta call_variants Variant calling from alignment using GATK ( documentation ). Input is alignment file in BAM format and reference sequence in FASTA format (either reference from reference-based assembly or consensus final sequence from de novo assembly). Output is a Variant Call File (VCF) format file. Usage: haphpipe call_variants [OPTIONS] [SETTINGS] --aln_bam <BAM> --ref_fa <FASTA> [--outdir] (or): hp_call_variants [OPTIONS] [SETTINGS] --aln_bam <BAM> --ref_fa <FASTA> [--outdir] Output files: variants.vcf.gz Input/Output Arguments: Option Description --aln_bam Alignment file. --ref_fa Reference fasta file. --outdir Output directory (default: False). Options: Option Description --emit_all Output calls for all site (default: False). --min_base_qual Minimum base quality required to consider a base for calling (default: 15). Settings: Option Description --ncpu Number of CPUs to use (default: 1). --xmx Maximum heap size for Java VM, in GB (default: 32). --keep_tmp Additional options (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe call_variants --aln_bam alignment.bam --ref_fa HIV_B.K03455.HXB2.fasta vcf_to_consensus Generate a consensus sequence from a VCF file. Input is a VCF file. Output is the consensus sequence in FASTA format. Usage: haphpipe vcf_to_consensus [OPTIONS] [SETTINGS] --vcf <FASTQ> [--outdir] [--sampidx] (or): hp_vcf_to_consensus [OPTIONS] [SETTINGS] --vcf <FASTQ> [--outdir] [--sampidx] Output files: consensus.fna Input/Output Arguments: Option Description --vcf VCF file (created with all sites). --outdir Output directory (default: False). --sampidx Index for sample if multi-sample VCF (default: 0). Options: Option Description --min_DP Minimum depth to call site (default: 1). --major Allele fraction to make unambiguous call (default: 0.5). --minor Allele fraction to make ambiguous call (default: 0.2). Settings: Option Description --keep_tmp Additional options (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. Example usage: haphpipe vcf_to_consensus --vcf variants.vcf refine_assembly Map reads to a denovo assembly or reference alignment. Assembly or alignment is iteratively updated. Input is reads in FASTQ format and reference sequence (assembly or reference alignment) in FASTA format. Output is refined assembly in FASTA format. Usage: haphpipe refine_assembly [OPTIONS] [SETTINGS] --fq1 <FASTQ> --fq2 <FASTQ> --fqU <FASTQ> --ref_fa <FASTA> [--outdir] (or): hp_refine_assembly [OPTIONS] [SETTINGS] --fq1 <FASTQ> --fq2 <FASTQ> --fqU <FASTQ> --ref_fa <FASTA> [--outdir] Output files: refined.fna Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --ref_fa Reference fasta file. --outdir Output directory (default: False). Options: Option Description --max_step Maximum number of refinement steps (default: 1). --subsample Use a subsample of reads for refinement. --seed Seed for random number generator (ignored if not subsampling). --sample_id Sample ID. Used as read group ID in BAM (default: sampleXX). Settings: Option Description --ncpu Number of CPUs to use (default: 1). --xmx Maximum heap size for Java VM, in GB (default: 32). --keep_tmp Additional options (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe refine_assembly --fq_1 corrected_1.fastq --fq2 corrected_2.fastq --ref_fa HIV_B.K03455.HXB2.fasta finalize_assembly Finalize consensus, map reads to consensus, and call variants. Input is reads in FASTQ format and reference sequence in FASTA format. Output is finalized reference sequence, alignment, and variants (in FASTA, BAM, and VCF formats, respectively). Usage: haphpipe finalize_assembly [OPTIONS] [SETTINGS] --fq1 <FASTQ> --fq2 <FASTQ> --fqU <FASTQ> --ref_fa <FASTA> [--outdir] (or): hp_finalize_assembly [OPTIONS] [SETTINGS] --fq1 <FASTQ> --fq2 <FASTQ> --fqU <FASTQ> --ref_fa <FASTA> [--outdir] Output files: final.fna final.ban final.vcf.gz Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --ref_fa Consensus fasta file. --outdir Output directory (default: current directory). Options: Option Description --bt2_preset {very-fast,fast,sensitive,very-sensitive,very-fast-local,fast-local,sensitive-local,very-sensitive-local} Bowtie2 preset to use (default: very-sensitive). --sample_id Sample ID (default: sampleXX). Settings: Option Description --ncpu Number of CPU to use (default: 1). --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe finalize_assembly --fq_1 corrected_1.fastq --fq2 corrected_2.fastq --ref_fa refined.fna","title":"Assemble"},{"location":"hp_assemble/#assemble_denovo","text":"Assemble reads via de novo assembly using SPAdes ( documentation ). Input is reads in FASTQ format. Output is contigs in FNA format. Usage: haphpipe assemble_denovo [OPTIONS] [SETTINGS] --fq1 <FASTQ> --fq2 <FASTQ> --fqU <FASTQ> [--outdir] (or): hp_assemble_denovo [OPTIONS] [SETTINGS] --fq1 <FASTQ> --fq2 <FASTQ> --fqU <FASTQ> [--outdir] Output files: denovo_contigs.fna denovo_summary.txt Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --outdir Output directory (default: current directory). Options: Option Description --no_error_correction Do not perform error correction (default: False) --subsample Use a subsample of reads for assembly --seed Seed for random number generator (ignored if not subsampling) Settings: Option Description --ncpu Number of CPU to use (default: 1). --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe assemble_denovo --fq1 corrected_1.fastq --fq2 corrected_2.fastq --outdir denovo_assembly --no_error_correction TRUE","title":"assemble_denovo"},{"location":"hp_assemble/#assemble_amplicons","text":"Assemble contigs from de novo assembly using both a reference sequence and amplicon regions with MUMMER 3+ ( documentation ). Input is contigs and reference sequence in FASTA format and amplicon regions in GTF format. Usage: haphpipe assemble_amplicons [OPTIONS] [SETTINGS] --contigs_fa <FASTA> --ref_fa <FASTA> --ref_gtf <GTF> [--outdir] (or): hp_assemble_amplicons [OPTIONS] [SETTINGS] --contigs_fa <FASTA> --ref_fa <FASTA> --ref_gtf <GTF> [--outdir] Output files: amplicon_assembly.fna Input/Output Arguments: Option Description --contigs_fa Fasta file with assembled contigs. --ref_fa Fasta file with reference genome to scaffold against. --ref_gtf GTF format file containing amplicon regions. --outdir Output directory (default: current directory). Scaffold Options: Option Description --sample_id Sample ID (default: sampleXX). --padding Bases to include outside reference annotation (default: 50). Settings: Option Description --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe assemble_amplicons --contigs_fa denovo_contigs.fa --ref_fa HIV_B.K03455.HXB2.fasta --ref_gtf HIV_B.K03455.HXB2.gtf","title":"assemble_amplicons"},{"location":"hp_assemble/#assemble_scaffold","text":"Scaffold contigs against a reference sequence with MUMMER 3+ ( documentation ). Input is contigs in FASTA format and reference sequence in FASTA format. Output is scaffold assembly, alligned scaffold, imputed scaffold, and padded scaffold in FASTA format. Usage: haphpipe assemble_scaffold [OPTIONS] [SETTINGS] --contigs_fa <FASTA> --ref_fa <FASTA> [--outdir] (or): hp_assemble_scaffold [OPTIONS] [SETTINGS] --contigs_fa <FASTA> --ref_fa <FASTA> [--outdir] Output files: scaffold_aligned.fa scaffold_assembly.fa scaffold_imputed.fa scaffold_padded.out Input/Output Arguments: Option Description --contigs_fa Fasta file with assembled contigs. --ref_fa Fasta file with reference genome to scaffold against. --outdir Output directory (default: current directory). Options: Option Description --seqname Name to append to scaffold sequence (default: sample01). Settings: Option Description --keep_tmp Additional options (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe assemble_scaffold --contigs_fa denovo_contigs.fa --ref_fa HIV_B.K03455.HXB2.fasta","title":"assemble_scaffold"},{"location":"hp_assemble/#align_reads","text":"Map reads to reference sequence (instead of running de novo assembly) using Bowtie2 ( documentation ) and Picard ( documentation ). Input is reads in FASTQ format and reference sequence in FASTA format. Usage: haphpipe align_reads [OPTIONS] [SETTINGS] --fq1 <FASTQ> --fq2 <FASTQ> --fqU <FASTQ> --ref_fa <FASTA> [--outdir] (or): hp_align_reads [OPTIONS] [SETTINGS] --fq1 <FASTQ> --fq2 <FASTQ> --fqU <FASTQ> --ref_fa <FASTA> [--outdir] Output files: aligned.bam aligned.bt2.out Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --ref_fa Reference fasta file. --outdir Output directory (default: current directory). Options: Option Description --bt2_preset {very-fast, fast, sensitive,very-sensitive,very-fast-local,fast-local,sensitive-local,very-sensitive-local} --sample_id Sample ID. Used as read group ID in BAM (default: sampleXX). --no_realign Do not realign indels (default: False). --remove_duplicates Remove duplicates from final alignment. Otherwise duplicates are marked but not removed (default: False). --encoding {Phred+33,Phred+64} Quality score encoding. Settings: Option Description --ncpu Number of CPUs to use (default: 1). --xmx Maximum heap size for Java VM, in GB (default: 32). --keep_tmp Additional options (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe align_reads --fq1 corrected_1.fastq --fq2 corrected _2.fastq --ref_fa HIV_B.K03455.HXB2.fasta","title":"align_reads"},{"location":"hp_assemble/#call_variants","text":"Variant calling from alignment using GATK ( documentation ). Input is alignment file in BAM format and reference sequence in FASTA format (either reference from reference-based assembly or consensus final sequence from de novo assembly). Output is a Variant Call File (VCF) format file. Usage: haphpipe call_variants [OPTIONS] [SETTINGS] --aln_bam <BAM> --ref_fa <FASTA> [--outdir] (or): hp_call_variants [OPTIONS] [SETTINGS] --aln_bam <BAM> --ref_fa <FASTA> [--outdir] Output files: variants.vcf.gz Input/Output Arguments: Option Description --aln_bam Alignment file. --ref_fa Reference fasta file. --outdir Output directory (default: False). Options: Option Description --emit_all Output calls for all site (default: False). --min_base_qual Minimum base quality required to consider a base for calling (default: 15). Settings: Option Description --ncpu Number of CPUs to use (default: 1). --xmx Maximum heap size for Java VM, in GB (default: 32). --keep_tmp Additional options (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe call_variants --aln_bam alignment.bam --ref_fa HIV_B.K03455.HXB2.fasta","title":"call_variants"},{"location":"hp_assemble/#vcf_to_consensus","text":"Generate a consensus sequence from a VCF file. Input is a VCF file. Output is the consensus sequence in FASTA format. Usage: haphpipe vcf_to_consensus [OPTIONS] [SETTINGS] --vcf <FASTQ> [--outdir] [--sampidx] (or): hp_vcf_to_consensus [OPTIONS] [SETTINGS] --vcf <FASTQ> [--outdir] [--sampidx] Output files: consensus.fna Input/Output Arguments: Option Description --vcf VCF file (created with all sites). --outdir Output directory (default: False). --sampidx Index for sample if multi-sample VCF (default: 0). Options: Option Description --min_DP Minimum depth to call site (default: 1). --major Allele fraction to make unambiguous call (default: 0.5). --minor Allele fraction to make ambiguous call (default: 0.2). Settings: Option Description --keep_tmp Additional options (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. Example usage: haphpipe vcf_to_consensus --vcf variants.vcf","title":"vcf_to_consensus"},{"location":"hp_assemble/#refine_assembly","text":"Map reads to a denovo assembly or reference alignment. Assembly or alignment is iteratively updated. Input is reads in FASTQ format and reference sequence (assembly or reference alignment) in FASTA format. Output is refined assembly in FASTA format. Usage: haphpipe refine_assembly [OPTIONS] [SETTINGS] --fq1 <FASTQ> --fq2 <FASTQ> --fqU <FASTQ> --ref_fa <FASTA> [--outdir] (or): hp_refine_assembly [OPTIONS] [SETTINGS] --fq1 <FASTQ> --fq2 <FASTQ> --fqU <FASTQ> --ref_fa <FASTA> [--outdir] Output files: refined.fna Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --ref_fa Reference fasta file. --outdir Output directory (default: False). Options: Option Description --max_step Maximum number of refinement steps (default: 1). --subsample Use a subsample of reads for refinement. --seed Seed for random number generator (ignored if not subsampling). --sample_id Sample ID. Used as read group ID in BAM (default: sampleXX). Settings: Option Description --ncpu Number of CPUs to use (default: 1). --xmx Maximum heap size for Java VM, in GB (default: 32). --keep_tmp Additional options (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe refine_assembly --fq_1 corrected_1.fastq --fq2 corrected_2.fastq --ref_fa HIV_B.K03455.HXB2.fasta","title":"refine_assembly"},{"location":"hp_assemble/#finalize_assembly","text":"Finalize consensus, map reads to consensus, and call variants. Input is reads in FASTQ format and reference sequence in FASTA format. Output is finalized reference sequence, alignment, and variants (in FASTA, BAM, and VCF formats, respectively). Usage: haphpipe finalize_assembly [OPTIONS] [SETTINGS] --fq1 <FASTQ> --fq2 <FASTQ> --fqU <FASTQ> --ref_fa <FASTA> [--outdir] (or): hp_finalize_assembly [OPTIONS] [SETTINGS] --fq1 <FASTQ> --fq2 <FASTQ> --fqU <FASTQ> --ref_fa <FASTA> [--outdir] Output files: final.fna final.ban final.vcf.gz Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --ref_fa Consensus fasta file. --outdir Output directory (default: current directory). Options: Option Description --bt2_preset {very-fast,fast,sensitive,very-sensitive,very-fast-local,fast-local,sensitive-local,very-sensitive-local} Bowtie2 preset to use (default: very-sensitive). --sample_id Sample ID (default: sampleXX). Settings: Option Description --ncpu Number of CPU to use (default: 1). --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe finalize_assembly --fq_1 corrected_1.fastq --fq2 corrected_2.fastq --ref_fa refined.fna","title":"finalize_assembly"},{"location":"hp_haplotype/","text":"The haplotype stage in HAPHPIPE implements PredictHaplo ( paper ), although other haplotype reconstruction programs can be utilized outside of HAPHPIPE using the final output of HAPHPIPE, typically with the final consensus sequence (FASTA) file, reads (raw, trimmed, and/or corrected), and/or final alignment (BAM) file as input. Use -h after any command for a list of options. Note: PredictHaplo is not compatable with read files including read IDs (.1 and .2 appended at the end of read names for read 1 and read 2, respectively). If your file has read IDs, use the following commands to create new files with the read IDs taken out before running PredictHaplo: cat corrected_1.fastq | sed 's/\\.1 / /' > corrected_1_fixed.fastq cat corrected_2.fastq | sed 's/\\.1 / /' > corrected_2_fixed.fastq predict_haplo Haplotype identification with PredictHaplo. Input is reads in FASTQ format and and reference sequence in FASTA format. Output is the longest global haplotype file and corresponding HTML file. Note: PredictHaplo must be installed separately before running this stage. Usage: haphpipe predict_haplo [OPTIONS] [SETTINGS] --fq1 <FASTQ> --fq2 <FASTQ> --ref_fa <FASTA> --interval_txt [TXT] [--outdir] (or): hp_predict_haplo [OPTIONS] [SETTINGS] --fq1 <FASTQ> --fq2 <FASTQ> --ref_fa <FASTA> --interval_txt [TXT] [--outdir] Output files: best.fa Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --ref_fa Reference sequence used to align reads (Fasta). --interval_txt File with intervals to perform haplotype reconstruction. --outdir Output directory (default: current directory). Options: Option Description --min_readlength Minimum read length passed to PredictHaplo (default: 36). Settings: Option Description --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe predict_haplo corrected_1.fastq --fq2 corrected_2.fastq --ref_fa final.fna ph_parser Returns PredictHaplo output as a correctly formatted FASTA file. Input is the output file from predict_haplo (longest global .fas file). Output is a correctly formatted FASTA file. Usage: haphpipe ph_parser [OPTIONS] [SETTINGS] --haplotypes_fa <best.fas> [--outdir] (or): hp_ph_parser [OPTIONS] [SETTINGS] --haplotypes_fa <best.fas> [--outdir] Output files: ph_summary.txt ph_haplotypes.fna Input/Output Arguments: Option Description --haplotypes_fa Haplotype file created by PredictHaplo. --outdir Output directory (default: current directory). Options: Option Description --prefix Prefix to add to sequence names. --keep_gaps Do not remove gaps from alignment (default: False). Settings: Option Description --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. Example usage: haphpipe ph_parser PH01.best_1_864.fas Example: By default, PredictHaplo outputs their own unique version of a fasta file. It includes the frequency, some information regarding their unqiue overlapping scores, and their unique confidence scores. This file is always named PH#.best_#_#.fas , where the first number is the reconstructed haplotype number and the next numbers are the start and end of the longest haplotype reconstructed by PredictHaplo. PredictHaplo Output $ cat PH01.best_1_1208.fas >reconstructed_0 ;Freq:0.190638 ;Overlap quality scores (5,10):1,1 ;Confidence scores ;~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~n7~~~y~~~~~t~~~i~jjk~~zz ;~~~~{~~~~~N{~{~Sx~~~~~~z~K~F~~~~~~~y~~~~~~~|~~F~wx|~~{~~|~~s<|]~~~~~~ ;~m~kj|{|v~{|_`~~~z~~~~~~~jy{y~~~~~a~__~~|~~~~~{wXZ~}~~~qm~xV~~~~~~~}~ ;Q~}~y||~~~}}~~~z~~~~~~{}A~}b|~~u~~|}}|~}}z~}bx~~n||~~||{~}~~d}~bz~~~} ;|~}}~~~}~~~{|}}g{~~~}~r~}}~~~~u~~{kx{~}~}~|~}~}{}~}~}||~~~~[~}~}}~~~~ ;~~}~~~U||U}~|}}~~}}~~~~}u~b|}~w~~~~~{}|wv~}Dxzp{|}~@~~P~}~}~V~~z}~|ry ;q~}|}~}~~}t~o~}~f}~~}{~~~}~{~~~~~~~}~~}~~|~~~M~~}~}x~}c~}^v~~~yzA~}~} ;y}~z}~~~~~~~~~{z~~}~~~}~{}~~~~~}~~~~~~~|~~v~~}~~|y~]|{~||~~~~~~~~||~~ ;Y||~~|Q~|~~~|~~~~~|~~~z~~z~{{{y~~~~~~~~~~w{{~wz|~Z|~z|~~}p|~~|}}~~x}} ;z}~}}|a|}}}}{}|~~}}~}}~}~{~|~}}~}{}|}|}}}~|}}}}{}}}|}}|}|}}}|}}}}~}}} ;||}}}}{}}~}~}}}}}}}}}}}~}}}}}}}}}}}}}}}}}y}~}}}}}}}}}}~|}}}}|}}}}|}}} ;}}}}}}}}|}}}}}}}}~}}}}}}|}}}}}}}}|}}}}}|}}}}|~|}}}}{}}}}}}}}}}}}}}}}} ;}}=}}}{}}}}}}}}}}}}}}}}}}}}}}}}~||}}|~}}}}}{}}}}}|}}|}}}}}}}}}}}}}|}} ;|}~}}}}|}}}}}}}}|}|~J}}}}}}~}}}}}}}}}}}}}}}}}}}}|}}`~}}}}|}}}}}}}}}}~ ;|}}}}}}}}}}}~}}}|}}}}}}}}}}|}}}}}}}}}}}t}}{}`}}}}}}|}~}~}|~}}}|k}}}}} ;|}|~}}|}}~}}~}|}}z}}}}}}}}}}}}}}~}}~}}}~~}|}}}~}}}}}}}}~}~}}||~~}~z}} ;~~}~}||~|~}|{}||~|z~~||}~|~}}|}~~}|}}~}}|}z~~~~{~}{}~y~~~~~{|}}~~|y~~ ;~|~~||~~~~~~~|n~~{~~~~~~~~~~~~~~~ ;EndOfComments ACCAAATGAAAGATTGTACTGAGAGACAGGCTAATTTTTTAGGGAATATCTGGCCTTCCCGCAAGGGAAG GCCAGGGAACTTTCCTCAGAGCAGGCTAGAGCCAACAGCCCCACCAGAAGAGAGCTTCAGGCTTGGGGAA GCAACAACAAAGCCCTCTCAGAAACAGGAGCCAATAGACAAGGAAATGTATCCTTTAACTTCCCTCAGAT CACTCTTTGGCAACGACCCCTTGTCACAATAAAGATAGGGGAGCAACTAAAGGAAGCCCTGTTAGATACA GGAGCAGATGATACAGTATTAGAAGAAATGAATTTGCCAGGAAGATGGAAACCAAGAATGATAGGGGGAA TCGGGGGTTTTATCAAAGTAAGACAGTATGATCAGATAGCCATAGACATCTGTGGCCATAAAGCTATAGG TACAGTATTAGTAGGACCTACACCTGTCAACATAATTGGAAGAAATCTGTTGACTCAGCTTGGTTGCACT TTAAATTTTCCCATTAGTCCTATTGAAACTGTACCAGTAAAATTGAAGCCAGGAATGGATGGCCCAAAGG TTAAACAATGGCCATTGACAGAAGAAAAAATAAAAGCATTAGTAGAAATTTGTACAGAAATGGAAAAAGA AGGGAAAATTTCAAAAATTGGGCCTGAAAATCCATACAATACTCCAGTATTTGCCATAAGGAAAAAAGAC AGTACTAAATGGAGAAAATTAGTAGATTTCAGAGAACTTAATAAGAGAACTCAGGACTTCTGGGAAGTCC AATTAGGAATACCACATCCCTCAGGGTTAAAAAAGAAAAAATCAGTAACAGTACTGGATGCGGGTGATGC ATATTTTTCAGTTCCCTTAGATGAAGACTTCAGAAAGTATACTGCATTTACCATACCTAGTGTAAACAAT GAGACACTAGGGATTAGGTATCAGTACAATGTGCTTCCACAAGGATGGAAAGGATCACCAGCAATATTCC AAGCTAGCATGACAAAAATCTTAGAGCCTTTTAGAAAACAAAATCCAGACATAGTTATCTATCAATACAT GGATGATCTGTATGTAGGATCTGACCTAGAAATAGGGCAGCATAGAACAAAAATAGAGGAACTGAGAGAA CATCTGTTGAGGTGGGGGTTTTGCACACCAGACAAGAAACATCAGAAGGAACCTCCATTCCTTTGGATGG GTTATGAACTCCATCCTT >reconstructed_1 ;Freq:0.294104 ;Overlap quality scores (5,10):1,1 ;Confidence scores ;~~~~~~~~z~~~~~~~~~|~y~~~~u}|~~~~~}}~~~~~~~ya~|~~}}~~~y~~~j~YXH~~s~ ;~~~~z~~|~~b|}^}xZ}}~}z~t~r~{}~}x}}yb}~~~}~}u~~}~gu|}}{~|}}~ozsw}|}}h~ ;|l}v|^][t}zz}vw}}s}}}}~v}|~|~~}}}~~}}}}}~}}}}}}zo\\}}}~}vn}iv}}}}}}}}} ;v}}~|u}}}}}}}}}|}}}}}}|}^}}[}~}r}}}{|}}|t|}}{z}}pz}}{}}}}}}}v~}y|~}}} ;}}}}~}}~}}}tz}}`}}}~}}q}}}}}~}y}}|r||}}}}~}}~}}{}}}}}}}}}}}y~}}}}t}}} ;}}}}}}{|}|}}{r}}}}tv~}}}t}m|~}h}}}}}v}}qt}|y|{|u~}}|}~_}~|~~|}~|}~}|e ;Y}~}}~}}~}v}t}}}j}}~}}}~}}~|}}}}|}}|}}}~}}}}};}}~}}x}}|}}yu}}}wy`}|~~ ;{}}x}}}}}~}}}~{v}}}}}{}~}}~}~}}|}}}}}}}}}}u}}y~~{g}L}}}}}|~}{~|}|}|}} ;l}}}}{_}}}}~z~}|}~~|~~x|~}~z~~|~}~~~}~~~}y}~~|k}}Pf}w~~~~T}}}F~~}~z}{ ;{~|}|zZ~s~|}r~}}~~|}}}}}~|}}}}n}}}~}}}}~}}}}}}}}}}}|~}}}}}}~~}}~~~~~} ;N}}}}}|~}}~}}}}{}}}~}}|~~|}|}}}~|}~}}}}}}D}~}}}}}}}}}}}}}}}}|}}z}|}}} ;}k}}}}}}}}}}}}}}}}}}}|}}}}}|}}}}}|}}}}}|}}}}|~{~}}}G}~}}}}}~}}}}}}}}} ;}}h}}~|}|}}}}}}}}}}}}}}}}}}}}}}}}{}}|}}}}|~|}}}}}}}M}}}}}}}~}}}}}}}}} ;}}~|}}}}}~~~}}}~}}|}g}}}}}}}}~|}}}}}}}}}}}}}}}}}|}~t~}|}}}}}}}||~}}}} ;|}}}}~}}}}}}~}}}gi}}~}}}}}}}}}}}}|}}~}}g}~}}v}}}}}|{}}}}}}~}}~|e}}}}} ;}}|}}~}~}~}}~}|~}|}~|||}}}}|~}}~}}}}}}}}|}}~}}~}|~}|y~}~}}}}}|}}~~}~} ;}}~}~}|~}}}~}}}}~}{}}~}}~}}}||~~}||~}x|}{~|}|~~|~}}}|{~}}~}{}}||~}}}~ ;|~}~}}}~|}}~~|,~~~~~~~~~~~~~~m~~~ ;EndOfComments ACCAAATGAAAGATTGTACTGAGAGACAGGCTAATTTTTTAGGGAAGATCTGGCCTTCCCGCGGCGGAAG GCCAGGGAATTTTCTTCAGAGCAGACCAGAGCCAACAGCCCCACCAGAAGAGAGCTTCAGGTTTGGGGAG GAGACAACAACTCCCTCTCAGAAGCAGGAGCCGAAGGACAAGGAAACATATCCTTTAGCTTCCCTCAAAT CACTCTTTGGCAACGACCCCTTGTTACAATAAAGATAGGGGGGCAGCTAAAGGAAGCTCTATTAGATACA GGAGCAGATGACACAGTATTAGAAGAAATGGATTTGCCAGGAAGATGGAAACCAAAAATGATAGGGGGAA TTGGAGGTTTTATCAAAGTAAAACAGTATGATCAGATACCCATAGAAATTTGTGGACATAAAGTGATAGG TACAGTATTAGTAGGACCTACACCTGTCAACATAATTGGAAGAAATCTGTTGACTCAGCTTGGTTGCACT TTAAATTTTCCCATTAGTCCTATTGAAACTGTACCAGTAAAATTGAAGCCAGGAATGGATGGCCCAAAGG TTAAACAATGGCCATTGACAGAAGAAAAAATAAAAGCATTAATAGAAATCTGTGCAGAAATGGAAAAGGA AGGGAAAATTTCAAAAATTGGGCCTGAAAATCCATACAATACTCCAGTATTTGCCATAAGAAAAAAAGAC AGTACTAAATGGAGAAAATTAGTAGATTTCAAAGAACTTAATAAGAGAACTCAGGACTTCTGGGAAGTCC AATTAGGAATACCACATCCCTCAGGGTTAAAAAAGAAAAAGTCAGTAACAGTACTGGATGTGGGTGATGC ATATTTTTCAGTTCCCTTAGATGAAGACTTCAGAAAGTACACTGCATTTACCATACCTAGTGTAAACAAT GAGACACCAGGGATTAGGTATCAGTACAATGTGCTTCCACAAGGATGGAAAGGATCACCAGCAATATTCC AATGTAGCATGACAAAAATCTTAGAACCTTTTAGAAAACAAAATCCAGATATAGTTATCTATCAATACAT GGATGATCTGTATGTAGGATCTGACCTAGAAATAGGGCAGCATAGAACAAAAATAGAGGAACTGAGAGAA CATCTGTTGAGGTGGGGGTTTTGCACACCAGACAAGAAACATCAGAAGGAACCTCCATTCCTTTGGATGG GTTATGAACTCCATCCTT >reconstructed_2 ;Freq:0.515259 ;Overlap quality scores (5,10):4,4 ;Confidence scores ;~~~~~~~w~~zz~~~~z~~w~|wy~|~~{|~~|~~~~~~~~}~{~hD~~~}|~|}}t~~_;~tue}}S} ;~}~~p}}l}~P}}ZlvL~z}}h~g~A~x}}}s}}{S~}~}}}}l}~|}SOq}}q}w}}~i\\pz~w~~z~ ;}7~|vzUu4|smtgm}~W}}~~}r}n`f`}{PZLSI:Vs_V_}}}}_<db}}}}}CQ}YJ}t}}}~}}} ;T}}~|}}}}}}}~}}k}}}}}}q}L}}]}}}L}}}tt|~}{q}}{M}lAIu}}}s{}}}~y~}st}|}} ;z}}}}|}}}}}{q|}Z}}}}}}P~}|}|}}E}}cLbi}vu}}|}}|}g}}}}}}|}~}~{}~}|}{}}} ;}~}|}}}{{|~}pp}~~~{|}}~}[yEx}}\\}}}{}N}uoK}}pauSy}}}v~~J}}q}}v~}c}~}\\i ;]|}}~}|}~}r}Y}}}B}}~||~}}}}{}}}|}|~|}~~~~}}~{w}{}~|R|}s|}k{~}}}ra}q}} ;o}}d}~}}}}}~}}|X~}}~}e}}}}}}}|}y|}}}}~}|~}c~~b~~nL~a}}}}}~}}m}}~{}}}} ;g}}}}uLm~l}}`~|n}}~}}|d~~y~u}|tz~~~~|}}~~t}}}tx}|Tu~v}}~}^|~~w~|~}m{} ;u}}}|w;}{}{}r}||}}z|}|~|}z}||}y}}}~}}}|}}~}}}|}y}}}}}}}~}||}}}}}}~}}| ;v{}|}}{}}}|~|}}|}}}}}}}}}}|}}}}}|}~}}}}}}p}}||}}}}}~}}}{}}}}}}}z}}}}} ;}|}}~}}}|}}}}}}}}}}}}}}}}}}}}}}|}||}}|}|}}}}|}|}}}}u}}}}}}}}}}}}}}}}} ;}}g}}}{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}y}}}}}|}m}}}}}}}}}}}}}}}}} ;}}}}}}}}}}}}}}}}}}|}i}}}}}}}}}}|}}}}}}}}}}}}}}}}}}}H}}}}}}}}}|}{}}}}} ;|}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}y}}|}H}}}}}}}}}}}}}}}}}}u}}}}} ;}}}}}}}}}}}}}}}}}z}}}}}}}}}}}}}}}}|}}}|}}}}}}}}}|}}}}}}}}}}}}|}}}}|}} ;}}}}}}}}}}}}||}|}}|}}}}}}|}}|}}}~}|}|}}}|}{}}}}|}}}}|{|}}}}x||}}}|{}} ;}}}}}}}~}}~}~|n}~}}}}}~}}~~|~~~y~, ;EndOfComments ACCAAATGAAAGATTGTACTGAGAGACAGGCTAATTTTTTAGGGAAAATCTGGCCTTCCCACAAGGGAAG GCCAGGGAATTTTCTTCAGAGCAGACCAGAGCCAACAGCCCCACCAGAAGAGAGCTTCAGGTTTGGGGAA GAGACAACAACTCCCTCTCAGAAGCAGGAGCCGATAGACAAGGAAATGTATCCTTTAGCTTCCCTCAAAT CACTCTTTGGCAACGACCCCTCGTCACAATAAAGATAGGGGGGCAACTAAAGGAAGCTCTATTAGATACA GGAGCAGATGATACAGTATTAGAAGAAATGAATTTGCCAGGAAGATGGAAACCAAAAATGATAGGGGGAA TTGGAGGTTTTATCAAAGTAAAACAGTATGATCAGATACCCATAGAAATCTGTGGACATAAAGCTATAGG TACAGTATTAGTAGGACCTACACCTGTCAACATAATTGGAAGAAATCTGTTGACTCAGATTGGTTGCACT TTAAATTTTCCCATTAGTCCTATTGAAACTGTACCAGTAAAATTAAAGCCAGGAATGGATGGCCCAAAAG TTAAACAATGGCCATTGACAGAAGAAAAAATAAAAGCATTAGTAGAAATTTGTACAGAAATGGAAAAGGA AGGGAAAATTTCAAAAATTGGGCCTGAAAATCCATACAATACTCCAGTATTTGCCATAAGGAAAAAAGAC AGTACTAAATGGAGAAAATTAGTAGATTTCAGAGAACTTAATAAGAGAACTCAAGACTTCCGGGAAGTCC AATTAGGAATACCACATCCCTCAGGGTTAAAAAAGAAAAAATCAGTAACAGTACTGGATGTGGGTGATGC ATATTTTTCAGTTCCCTTAGATGAAGACTTCAGAAAGTATACTGCATTTACCATACCTAGTGTAAACAAT GAGACACCAGGGATTAGGTATCAGTACAATGTGCTTCCACAAGGATGGAAAGGATCACCAGCAATATTCC AAGCTAGCATGACAAAAATCTTAGAGCCTTTTAGAAAACAAAATCCAGACATAGTTATCTATCAATACAT GGATGATCTGTATGTAGGATCTGACCTAGAAATAGGGCAGCATAGAACAAAAATAGAGGAACTGAGAGAA CATCTGTTGAGGTGGGGGTTTTGCACACCAGACAAGAAACATCAGAAGGAACCTCCATTCCTTTGGATGG GTTATGAACTCCATCCCC ph_parser takes this output and creates a proper fasta file with each resontructed haplotype and a text file that has the hpalotype diversity estimate. PH Parser Output $ cat ph_summary.txt PH_num_hap 3 PH_hap_diversity 0.611668153059 PH_seq_len 1208 $ cat ph_haplotypes.fna >reconstructed_0 Freq=0.190638 ACCAAATGAAAGATTGTACTGAGAGACAGGCTAATTTTTTAGGGAATATCTGGCCTTCCCGCAAGGGAAG GCCAGGGAACTTTCCTCAGAGCAGGCTAGAGCCAACAGCCCCACCAGAAGAGAGCTTCAGGCTTGGGGAA GCAACAACAAAGCCCTCTCAGAAACAGGAGCCAATAGACAAGGAAATGTATCCTTTAACTTCCCTCAGAT CACTCTTTGGCAACGACCCCTTGTCACAATAAAGATAGGGGAGCAACTAAAGGAAGCCCTGTTAGATACA GGAGCAGATGATACAGTATTAGAAGAAATGAATTTGCCAGGAAGATGGAAACCAAGAATGATAGGGGGAA TCGGGGGTTTTATCAAAGTAAGACAGTATGATCAGATAGCCATAGACATCTGTGGCCATAAAGCTATAGG TACAGTATTAGTAGGACCTACACCTGTCAACATAATTGGAAGAAATCTGTTGACTCAGCTTGGTTGCACT TTAAATTTTCCCATTAGTCCTATTGAAACTGTACCAGTAAAATTGAAGCCAGGAATGGATGGCCCAAAGG TTAAACAATGGCCATTGACAGAAGAAAAAATAAAAGCATTAGTAGAAATTTGTACAGAAATGGAAAAAGA AGGGAAAATTTCAAAAATTGGGCCTGAAAATCCATACAATACTCCAGTATTTGCCATAAGGAAAAAAGAC AGTACTAAATGGAGAAAATTAGTAGATTTCAGAGAACTTAATAAGAGAACTCAGGACTTCTGGGAAGTCC AATTAGGAATACCACATCCCTCAGGGTTAAAAAAGAAAAAATCAGTAACAGTACTGGATGCGGGTGATGC ATATTTTTCAGTTCCCTTAGATGAAGACTTCAGAAAGTATACTGCATTTACCATACCTAGTGTAAACAAT GAGACACTAGGGATTAGGTATCAGTACAATGTGCTTCCACAAGGATGGAAAGGATCACCAGCAATATTCC AAGCTAGCATGACAAAAATCTTAGAGCCTTTTAGAAAACAAAATCCAGACATAGTTATCTATCAATACAT GGATGATCTGTATGTAGGATCTGACCTAGAAATAGGGCAGCATAGAACAAAAATAGAGGAACTGAGAGAA CATCTGTTGAGGTGGGGGTTTTGCACACCAGACAAGAAACATCAGAAGGAACCTCCATTCCTTTGGATGG GTTATGAACTCCATCCTT >reconstructed_1 Freq=0.294104 ACCAAATGAAAGATTGTACTGAGAGACAGGCTAATTTTTTAGGGAAGATCTGGCCTTCCCGCGGCGGAAG GCCAGGGAATTTTCTTCAGAGCAGACCAGAGCCAACAGCCCCACCAGAAGAGAGCTTCAGGTTTGGGGAG GAGACAACAACTCCCTCTCAGAAGCAGGAGCCGAAGGACAAGGAAACATATCCTTTAGCTTCCCTCAAAT CACTCTTTGGCAACGACCCCTTGTTACAATAAAGATAGGGGGGCAGCTAAAGGAAGCTCTATTAGATACA GGAGCAGATGACACAGTATTAGAAGAAATGGATTTGCCAGGAAGATGGAAACCAAAAATGATAGGGGGAA TTGGAGGTTTTATCAAAGTAAAACAGTATGATCAGATACCCATAGAAATTTGTGGACATAAAGTGATAGG TACAGTATTAGTAGGACCTACACCTGTCAACATAATTGGAAGAAATCTGTTGACTCAGCTTGGTTGCACT TTAAATTTTCCCATTAGTCCTATTGAAACTGTACCAGTAAAATTGAAGCCAGGAATGGATGGCCCAAAGG TTAAACAATGGCCATTGACAGAAGAAAAAATAAAAGCATTAATAGAAATCTGTGCAGAAATGGAAAAGGA AGGGAAAATTTCAAAAATTGGGCCTGAAAATCCATACAATACTCCAGTATTTGCCATAAGAAAAAAAGAC AGTACTAAATGGAGAAAATTAGTAGATTTCAAAGAACTTAATAAGAGAACTCAGGACTTCTGGGAAGTCC AATTAGGAATACCACATCCCTCAGGGTTAAAAAAGAAAAAGTCAGTAACAGTACTGGATGTGGGTGATGC ATATTTTTCAGTTCCCTTAGATGAAGACTTCAGAAAGTACACTGCATTTACCATACCTAGTGTAAACAAT GAGACACCAGGGATTAGGTATCAGTACAATGTGCTTCCACAAGGATGGAAAGGATCACCAGCAATATTCC AATGTAGCATGACAAAAATCTTAGAACCTTTTAGAAAACAAAATCCAGATATAGTTATCTATCAATACAT GGATGATCTGTATGTAGGATCTGACCTAGAAATAGGGCAGCATAGAACAAAAATAGAGGAACTGAGAGAA CATCTGTTGAGGTGGGGGTTTTGCACACCAGACAAGAAACATCAGAAGGAACCTCCATTCCTTTGGATGG GTTATGAACTCCATCCTT >reconstructed_2 Freq=0.515259 ACCAAATGAAAGATTGTACTGAGAGACAGGCTAATTTTTTAGGGAAAATCTGGCCTTCCCACAAGGGAAG GCCAGGGAATTTTCTTCAGAGCAGACCAGAGCCAACAGCCCCACCAGAAGAGAGCTTCAGGTTTGGGGAA GAGACAACAACTCCCTCTCAGAAGCAGGAGCCGATAGACAAGGAAATGTATCCTTTAGCTTCCCTCAAAT CACTCTTTGGCAACGACCCCTCGTCACAATAAAGATAGGGGGGCAACTAAAGGAAGCTCTATTAGATACA GGAGCAGATGATACAGTATTAGAAGAAATGAATTTGCCAGGAAGATGGAAACCAAAAATGATAGGGGGAA TTGGAGGTTTTATCAAAGTAAAACAGTATGATCAGATACCCATAGAAATCTGTGGACATAAAGCTATAGG TACAGTATTAGTAGGACCTACACCTGTCAACATAATTGGAAGAAATCTGTTGACTCAGATTGGTTGCACT TTAAATTTTCCCATTAGTCCTATTGAAACTGTACCAGTAAAATTAAAGCCAGGAATGGATGGCCCAAAAG TTAAACAATGGCCATTGACAGAAGAAAAAATAAAAGCATTAGTAGAAATTTGTACAGAAATGGAAAAGGA AGGGAAAATTTCAAAAATTGGGCCTGAAAATCCATACAATACTCCAGTATTTGCCATAAGGAAAAAAGAC AGTACTAAATGGAGAAAATTAGTAGATTTCAGAGAACTTAATAAGAGAACTCAAGACTTCCGGGAAGTCC AATTAGGAATACCACATCCCTCAGGGTTAAAAAAGAAAAAATCAGTAACAGTACTGGATGTGGGTGATGC ATATTTTTCAGTTCCCTTAGATGAAGACTTCAGAAAGTATACTGCATTTACCATACCTAGTGTAAACAAT GAGACACCAGGGATTAGGTATCAGTACAATGTGCTTCCACAAGGATGGAAAGGATCACCAGCAATATTCC AAGCTAGCATGACAAAAATCTTAGAGCCTTTTAGAAAACAAAATCCAGACATAGTTATCTATCAATACAT GGATGATCTGTATGTAGGATCTGACCTAGAAATAGGGCAGCATAGAACAAAAATAGAGGAACTGAGAGAA CATCTGTTGAGGTGGGGGTTTTGCACACCAGACAAGAAACATCAGAAGGAACCTCCATTCCTTTGGATGG GTTATGAACTCCATCCCC cliquesnv A reference-based reconstruction of viral variants from NGS data( documentation ). Input is read files in Fastq format and reference Fasta file. Input reads are aligned and outputted to SAM format using BWA and SAMtools prior to running CliqueSNV. Output is inferred viral variants with respective frequencies and diversity. Please see the CliqueSNV documentation for a full description of CliqueSNV options. Usage: haphpipe cliquesnv [CliqueSNV OPTIONS] [HAPHPIPE OPTIONS] --fq1 <FASTQ> --fq2 <FASTQ> (or) --fqU <FASTQ> [--outdir] Output files: File Description cs[NUM]_[REGION].fasta Reconstructed haplotypes in FASTA format. cs[NUM]_[REGION].txt CliqueSNV output summary file. cs[NUM]_[REGION]_summary.txt HAPHPIPE-generated output summary file with sequence length and diversity. Input/Output Arguments: Option CliqueSNV Equivalent Description --fq1 SEQS NA Input reads in FASTQ format (read 1) --fq2 SEQS NA Input reads in FASTQ format (read 2) --fqU SEQS NA Input reads in FASTQ format (unpaired reads) --ref_fa REF_FA NA Reference FASTA --outdir OUTDIR --prefix Output directory (default: .) CliqueSNV Options: Option CliqueSNV Equivalent Description --jardir JARDIR -jar Path to clique-snv.jar --O22min O22MIN -t Minimum threshold for O22 value --O22minfreq --O22MINFREQ -tf Minimum threshold for O22 frequency relative to read coverage --printlog -log Print log data to console --merging MERGING -cm Cliques merging algorithm: accurate or fast --outputstart OUTPUTSTART -os Output start position --outputend OUTPUTEND -oe Output end position --fasta_format FASTA_FORMAT -fdf Fasta defline format: short or extended, add number at end to adjust precision of frequency Options: Option Description --keep_tmp Keep temporary directory --quiet Do not write output to console (silence stdout and stderr) (default: False) --logfile LOGFILE Name for log file (output) --debug Print commands but do not run (default: False) --ncpu NCPU Number of CPU to use (default: 1) Example usage: haphpipe cliquesnv --fq1 corrected_1.fastq --fq2 corrected_2.fastq --ref_fa final.fna","title":"Haplotype"},{"location":"hp_haplotype/#predict_haplo","text":"Haplotype identification with PredictHaplo. Input is reads in FASTQ format and and reference sequence in FASTA format. Output is the longest global haplotype file and corresponding HTML file. Note: PredictHaplo must be installed separately before running this stage. Usage: haphpipe predict_haplo [OPTIONS] [SETTINGS] --fq1 <FASTQ> --fq2 <FASTQ> --ref_fa <FASTA> --interval_txt [TXT] [--outdir] (or): hp_predict_haplo [OPTIONS] [SETTINGS] --fq1 <FASTQ> --fq2 <FASTQ> --ref_fa <FASTA> --interval_txt [TXT] [--outdir] Output files: best.fa Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --ref_fa Reference sequence used to align reads (Fasta). --interval_txt File with intervals to perform haplotype reconstruction. --outdir Output directory (default: current directory). Options: Option Description --min_readlength Minimum read length passed to PredictHaplo (default: 36). Settings: Option Description --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe predict_haplo corrected_1.fastq --fq2 corrected_2.fastq --ref_fa final.fna","title":"predict_haplo"},{"location":"hp_haplotype/#ph_parser","text":"Returns PredictHaplo output as a correctly formatted FASTA file. Input is the output file from predict_haplo (longest global .fas file). Output is a correctly formatted FASTA file. Usage: haphpipe ph_parser [OPTIONS] [SETTINGS] --haplotypes_fa <best.fas> [--outdir] (or): hp_ph_parser [OPTIONS] [SETTINGS] --haplotypes_fa <best.fas> [--outdir] Output files: ph_summary.txt ph_haplotypes.fna Input/Output Arguments: Option Description --haplotypes_fa Haplotype file created by PredictHaplo. --outdir Output directory (default: current directory). Options: Option Description --prefix Prefix to add to sequence names. --keep_gaps Do not remove gaps from alignment (default: False). Settings: Option Description --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. Example usage: haphpipe ph_parser PH01.best_1_864.fas Example: By default, PredictHaplo outputs their own unique version of a fasta file. It includes the frequency, some information regarding their unqiue overlapping scores, and their unique confidence scores. This file is always named PH#.best_#_#.fas , where the first number is the reconstructed haplotype number and the next numbers are the start and end of the longest haplotype reconstructed by PredictHaplo. PredictHaplo Output $ cat PH01.best_1_1208.fas >reconstructed_0 ;Freq:0.190638 ;Overlap quality scores (5,10):1,1 ;Confidence scores ;~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~n7~~~y~~~~~t~~~i~jjk~~zz ;~~~~{~~~~~N{~{~Sx~~~~~~z~K~F~~~~~~~y~~~~~~~|~~F~wx|~~{~~|~~s<|]~~~~~~ ;~m~kj|{|v~{|_`~~~z~~~~~~~jy{y~~~~~a~__~~|~~~~~{wXZ~}~~~qm~xV~~~~~~~}~ ;Q~}~y||~~~}}~~~z~~~~~~{}A~}b|~~u~~|}}|~}}z~}bx~~n||~~||{~}~~d}~bz~~~} ;|~}}~~~}~~~{|}}g{~~~}~r~}}~~~~u~~{kx{~}~}~|~}~}{}~}~}||~~~~[~}~}}~~~~ ;~~}~~~U||U}~|}}~~}}~~~~}u~b|}~w~~~~~{}|wv~}Dxzp{|}~@~~P~}~}~V~~z}~|ry ;q~}|}~}~~}t~o~}~f}~~}{~~~}~{~~~~~~~}~~}~~|~~~M~~}~}x~}c~}^v~~~yzA~}~} ;y}~z}~~~~~~~~~{z~~}~~~}~{}~~~~~}~~~~~~~|~~v~~}~~|y~]|{~||~~~~~~~~||~~ ;Y||~~|Q~|~~~|~~~~~|~~~z~~z~{{{y~~~~~~~~~~w{{~wz|~Z|~z|~~}p|~~|}}~~x}} ;z}~}}|a|}}}}{}|~~}}~}}~}~{~|~}}~}{}|}|}}}~|}}}}{}}}|}}|}|}}}|}}}}~}}} ;||}}}}{}}~}~}}}}}}}}}}}~}}}}}}}}}}}}}}}}}y}~}}}}}}}}}}~|}}}}|}}}}|}}} ;}}}}}}}}|}}}}}}}}~}}}}}}|}}}}}}}}|}}}}}|}}}}|~|}}}}{}}}}}}}}}}}}}}}}} ;}}=}}}{}}}}}}}}}}}}}}}}}}}}}}}}~||}}|~}}}}}{}}}}}|}}|}}}}}}}}}}}}}|}} ;|}~}}}}|}}}}}}}}|}|~J}}}}}}~}}}}}}}}}}}}}}}}}}}}|}}`~}}}}|}}}}}}}}}}~ ;|}}}}}}}}}}}~}}}|}}}}}}}}}}|}}}}}}}}}}}t}}{}`}}}}}}|}~}~}|~}}}|k}}}}} ;|}|~}}|}}~}}~}|}}z}}}}}}}}}}}}}}~}}~}}}~~}|}}}~}}}}}}}}~}~}}||~~}~z}} ;~~}~}||~|~}|{}||~|z~~||}~|~}}|}~~}|}}~}}|}z~~~~{~}{}~y~~~~~{|}}~~|y~~ ;~|~~||~~~~~~~|n~~{~~~~~~~~~~~~~~~ ;EndOfComments ACCAAATGAAAGATTGTACTGAGAGACAGGCTAATTTTTTAGGGAATATCTGGCCTTCCCGCAAGGGAAG GCCAGGGAACTTTCCTCAGAGCAGGCTAGAGCCAACAGCCCCACCAGAAGAGAGCTTCAGGCTTGGGGAA GCAACAACAAAGCCCTCTCAGAAACAGGAGCCAATAGACAAGGAAATGTATCCTTTAACTTCCCTCAGAT CACTCTTTGGCAACGACCCCTTGTCACAATAAAGATAGGGGAGCAACTAAAGGAAGCCCTGTTAGATACA GGAGCAGATGATACAGTATTAGAAGAAATGAATTTGCCAGGAAGATGGAAACCAAGAATGATAGGGGGAA TCGGGGGTTTTATCAAAGTAAGACAGTATGATCAGATAGCCATAGACATCTGTGGCCATAAAGCTATAGG TACAGTATTAGTAGGACCTACACCTGTCAACATAATTGGAAGAAATCTGTTGACTCAGCTTGGTTGCACT TTAAATTTTCCCATTAGTCCTATTGAAACTGTACCAGTAAAATTGAAGCCAGGAATGGATGGCCCAAAGG TTAAACAATGGCCATTGACAGAAGAAAAAATAAAAGCATTAGTAGAAATTTGTACAGAAATGGAAAAAGA AGGGAAAATTTCAAAAATTGGGCCTGAAAATCCATACAATACTCCAGTATTTGCCATAAGGAAAAAAGAC AGTACTAAATGGAGAAAATTAGTAGATTTCAGAGAACTTAATAAGAGAACTCAGGACTTCTGGGAAGTCC AATTAGGAATACCACATCCCTCAGGGTTAAAAAAGAAAAAATCAGTAACAGTACTGGATGCGGGTGATGC ATATTTTTCAGTTCCCTTAGATGAAGACTTCAGAAAGTATACTGCATTTACCATACCTAGTGTAAACAAT GAGACACTAGGGATTAGGTATCAGTACAATGTGCTTCCACAAGGATGGAAAGGATCACCAGCAATATTCC AAGCTAGCATGACAAAAATCTTAGAGCCTTTTAGAAAACAAAATCCAGACATAGTTATCTATCAATACAT GGATGATCTGTATGTAGGATCTGACCTAGAAATAGGGCAGCATAGAACAAAAATAGAGGAACTGAGAGAA CATCTGTTGAGGTGGGGGTTTTGCACACCAGACAAGAAACATCAGAAGGAACCTCCATTCCTTTGGATGG GTTATGAACTCCATCCTT >reconstructed_1 ;Freq:0.294104 ;Overlap quality scores (5,10):1,1 ;Confidence scores ;~~~~~~~~z~~~~~~~~~|~y~~~~u}|~~~~~}}~~~~~~~ya~|~~}}~~~y~~~j~YXH~~s~ ;~~~~z~~|~~b|}^}xZ}}~}z~t~r~{}~}x}}yb}~~~}~}u~~}~gu|}}{~|}}~ozsw}|}}h~ ;|l}v|^][t}zz}vw}}s}}}}~v}|~|~~}}}~~}}}}}~}}}}}}zo\\}}}~}vn}iv}}}}}}}}} ;v}}~|u}}}}}}}}}|}}}}}}|}^}}[}~}r}}}{|}}|t|}}{z}}pz}}{}}}}}}}v~}y|~}}} ;}}}}~}}~}}}tz}}`}}}~}}q}}}}}~}y}}|r||}}}}~}}~}}{}}}}}}}}}}}y~}}}}t}}} ;}}}}}}{|}|}}{r}}}}tv~}}}t}m|~}h}}}}}v}}qt}|y|{|u~}}|}~_}~|~~|}~|}~}|e ;Y}~}}~}}~}v}t}}}j}}~}}}~}}~|}}}}|}}|}}}~}}}}};}}~}}x}}|}}yu}}}wy`}|~~ ;{}}x}}}}}~}}}~{v}}}}}{}~}}~}~}}|}}}}}}}}}}u}}y~~{g}L}}}}}|~}{~|}|}|}} ;l}}}}{_}}}}~z~}|}~~|~~x|~}~z~~|~}~~~}~~~}y}~~|k}}Pf}w~~~~T}}}F~~}~z}{ ;{~|}|zZ~s~|}r~}}~~|}}}}}~|}}}}n}}}~}}}}~}}}}}}}}}}}|~}}}}}}~~}}~~~~~} ;N}}}}}|~}}~}}}}{}}}~}}|~~|}|}}}~|}~}}}}}}D}~}}}}}}}}}}}}}}}}|}}z}|}}} ;}k}}}}}}}}}}}}}}}}}}}|}}}}}|}}}}}|}}}}}|}}}}|~{~}}}G}~}}}}}~}}}}}}}}} ;}}h}}~|}|}}}}}}}}}}}}}}}}}}}}}}}}{}}|}}}}|~|}}}}}}}M}}}}}}}~}}}}}}}}} ;}}~|}}}}}~~~}}}~}}|}g}}}}}}}}~|}}}}}}}}}}}}}}}}}|}~t~}|}}}}}}}||~}}}} ;|}}}}~}}}}}}~}}}gi}}~}}}}}}}}}}}}|}}~}}g}~}}v}}}}}|{}}}}}}~}}~|e}}}}} ;}}|}}~}~}~}}~}|~}|}~|||}}}}|~}}~}}}}}}}}|}}~}}~}|~}|y~}~}}}}}|}}~~}~} ;}}~}~}|~}}}~}}}}~}{}}~}}~}}}||~~}||~}x|}{~|}|~~|~}}}|{~}}~}{}}||~}}}~ ;|~}~}}}~|}}~~|,~~~~~~~~~~~~~~m~~~ ;EndOfComments ACCAAATGAAAGATTGTACTGAGAGACAGGCTAATTTTTTAGGGAAGATCTGGCCTTCCCGCGGCGGAAG GCCAGGGAATTTTCTTCAGAGCAGACCAGAGCCAACAGCCCCACCAGAAGAGAGCTTCAGGTTTGGGGAG GAGACAACAACTCCCTCTCAGAAGCAGGAGCCGAAGGACAAGGAAACATATCCTTTAGCTTCCCTCAAAT CACTCTTTGGCAACGACCCCTTGTTACAATAAAGATAGGGGGGCAGCTAAAGGAAGCTCTATTAGATACA GGAGCAGATGACACAGTATTAGAAGAAATGGATTTGCCAGGAAGATGGAAACCAAAAATGATAGGGGGAA TTGGAGGTTTTATCAAAGTAAAACAGTATGATCAGATACCCATAGAAATTTGTGGACATAAAGTGATAGG TACAGTATTAGTAGGACCTACACCTGTCAACATAATTGGAAGAAATCTGTTGACTCAGCTTGGTTGCACT TTAAATTTTCCCATTAGTCCTATTGAAACTGTACCAGTAAAATTGAAGCCAGGAATGGATGGCCCAAAGG TTAAACAATGGCCATTGACAGAAGAAAAAATAAAAGCATTAATAGAAATCTGTGCAGAAATGGAAAAGGA AGGGAAAATTTCAAAAATTGGGCCTGAAAATCCATACAATACTCCAGTATTTGCCATAAGAAAAAAAGAC AGTACTAAATGGAGAAAATTAGTAGATTTCAAAGAACTTAATAAGAGAACTCAGGACTTCTGGGAAGTCC AATTAGGAATACCACATCCCTCAGGGTTAAAAAAGAAAAAGTCAGTAACAGTACTGGATGTGGGTGATGC ATATTTTTCAGTTCCCTTAGATGAAGACTTCAGAAAGTACACTGCATTTACCATACCTAGTGTAAACAAT GAGACACCAGGGATTAGGTATCAGTACAATGTGCTTCCACAAGGATGGAAAGGATCACCAGCAATATTCC AATGTAGCATGACAAAAATCTTAGAACCTTTTAGAAAACAAAATCCAGATATAGTTATCTATCAATACAT GGATGATCTGTATGTAGGATCTGACCTAGAAATAGGGCAGCATAGAACAAAAATAGAGGAACTGAGAGAA CATCTGTTGAGGTGGGGGTTTTGCACACCAGACAAGAAACATCAGAAGGAACCTCCATTCCTTTGGATGG GTTATGAACTCCATCCTT >reconstructed_2 ;Freq:0.515259 ;Overlap quality scores (5,10):4,4 ;Confidence scores ;~~~~~~~w~~zz~~~~z~~w~|wy~|~~{|~~|~~~~~~~~}~{~hD~~~}|~|}}t~~_;~tue}}S} ;~}~~p}}l}~P}}ZlvL~z}}h~g~A~x}}}s}}{S~}~}}}}l}~|}SOq}}q}w}}~i\\pz~w~~z~ ;}7~|vzUu4|smtgm}~W}}~~}r}n`f`}{PZLSI:Vs_V_}}}}_<db}}}}}CQ}YJ}t}}}~}}} ;T}}~|}}}}}}}~}}k}}}}}}q}L}}]}}}L}}}tt|~}{q}}{M}lAIu}}}s{}}}~y~}st}|}} ;z}}}}|}}}}}{q|}Z}}}}}}P~}|}|}}E}}cLbi}vu}}|}}|}g}}}}}}|}~}~{}~}|}{}}} ;}~}|}}}{{|~}pp}~~~{|}}~}[yEx}}\\}}}{}N}uoK}}pauSy}}}v~~J}}q}}v~}c}~}\\i ;]|}}~}|}~}r}Y}}}B}}~||~}}}}{}}}|}|~|}~~~~}}~{w}{}~|R|}s|}k{~}}}ra}q}} ;o}}d}~}}}}}~}}|X~}}~}e}}}}}}}|}y|}}}}~}|~}c~~b~~nL~a}}}}}~}}m}}~{}}}} ;g}}}}uLm~l}}`~|n}}~}}|d~~y~u}|tz~~~~|}}~~t}}}tx}|Tu~v}}~}^|~~w~|~}m{} ;u}}}|w;}{}{}r}||}}z|}|~|}z}||}y}}}~}}}|}}~}}}|}y}}}}}}}~}||}}}}}}~}}| ;v{}|}}{}}}|~|}}|}}}}}}}}}}|}}}}}|}~}}}}}}p}}||}}}}}~}}}{}}}}}}}z}}}}} ;}|}}~}}}|}}}}}}}}}}}}}}}}}}}}}}|}||}}|}|}}}}|}|}}}}u}}}}}}}}}}}}}}}}} ;}}g}}}{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}y}}}}}|}m}}}}}}}}}}}}}}}}} ;}}}}}}}}}}}}}}}}}}|}i}}}}}}}}}}|}}}}}}}}}}}}}}}}}}}H}}}}}}}}}|}{}}}}} ;|}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}y}}|}H}}}}}}}}}}}}}}}}}}u}}}}} ;}}}}}}}}}}}}}}}}}z}}}}}}}}}}}}}}}}|}}}|}}}}}}}}}|}}}}}}}}}}}}|}}}}|}} ;}}}}}}}}}}}}||}|}}|}}}}}}|}}|}}}~}|}|}}}|}{}}}}|}}}}|{|}}}}x||}}}|{}} ;}}}}}}}~}}~}~|n}~}}}}}~}}~~|~~~y~, ;EndOfComments ACCAAATGAAAGATTGTACTGAGAGACAGGCTAATTTTTTAGGGAAAATCTGGCCTTCCCACAAGGGAAG GCCAGGGAATTTTCTTCAGAGCAGACCAGAGCCAACAGCCCCACCAGAAGAGAGCTTCAGGTTTGGGGAA GAGACAACAACTCCCTCTCAGAAGCAGGAGCCGATAGACAAGGAAATGTATCCTTTAGCTTCCCTCAAAT CACTCTTTGGCAACGACCCCTCGTCACAATAAAGATAGGGGGGCAACTAAAGGAAGCTCTATTAGATACA GGAGCAGATGATACAGTATTAGAAGAAATGAATTTGCCAGGAAGATGGAAACCAAAAATGATAGGGGGAA TTGGAGGTTTTATCAAAGTAAAACAGTATGATCAGATACCCATAGAAATCTGTGGACATAAAGCTATAGG TACAGTATTAGTAGGACCTACACCTGTCAACATAATTGGAAGAAATCTGTTGACTCAGATTGGTTGCACT TTAAATTTTCCCATTAGTCCTATTGAAACTGTACCAGTAAAATTAAAGCCAGGAATGGATGGCCCAAAAG TTAAACAATGGCCATTGACAGAAGAAAAAATAAAAGCATTAGTAGAAATTTGTACAGAAATGGAAAAGGA AGGGAAAATTTCAAAAATTGGGCCTGAAAATCCATACAATACTCCAGTATTTGCCATAAGGAAAAAAGAC AGTACTAAATGGAGAAAATTAGTAGATTTCAGAGAACTTAATAAGAGAACTCAAGACTTCCGGGAAGTCC AATTAGGAATACCACATCCCTCAGGGTTAAAAAAGAAAAAATCAGTAACAGTACTGGATGTGGGTGATGC ATATTTTTCAGTTCCCTTAGATGAAGACTTCAGAAAGTATACTGCATTTACCATACCTAGTGTAAACAAT GAGACACCAGGGATTAGGTATCAGTACAATGTGCTTCCACAAGGATGGAAAGGATCACCAGCAATATTCC AAGCTAGCATGACAAAAATCTTAGAGCCTTTTAGAAAACAAAATCCAGACATAGTTATCTATCAATACAT GGATGATCTGTATGTAGGATCTGACCTAGAAATAGGGCAGCATAGAACAAAAATAGAGGAACTGAGAGAA CATCTGTTGAGGTGGGGGTTTTGCACACCAGACAAGAAACATCAGAAGGAACCTCCATTCCTTTGGATGG GTTATGAACTCCATCCCC ph_parser takes this output and creates a proper fasta file with each resontructed haplotype and a text file that has the hpalotype diversity estimate. PH Parser Output $ cat ph_summary.txt PH_num_hap 3 PH_hap_diversity 0.611668153059 PH_seq_len 1208 $ cat ph_haplotypes.fna >reconstructed_0 Freq=0.190638 ACCAAATGAAAGATTGTACTGAGAGACAGGCTAATTTTTTAGGGAATATCTGGCCTTCCCGCAAGGGAAG GCCAGGGAACTTTCCTCAGAGCAGGCTAGAGCCAACAGCCCCACCAGAAGAGAGCTTCAGGCTTGGGGAA GCAACAACAAAGCCCTCTCAGAAACAGGAGCCAATAGACAAGGAAATGTATCCTTTAACTTCCCTCAGAT CACTCTTTGGCAACGACCCCTTGTCACAATAAAGATAGGGGAGCAACTAAAGGAAGCCCTGTTAGATACA GGAGCAGATGATACAGTATTAGAAGAAATGAATTTGCCAGGAAGATGGAAACCAAGAATGATAGGGGGAA TCGGGGGTTTTATCAAAGTAAGACAGTATGATCAGATAGCCATAGACATCTGTGGCCATAAAGCTATAGG TACAGTATTAGTAGGACCTACACCTGTCAACATAATTGGAAGAAATCTGTTGACTCAGCTTGGTTGCACT TTAAATTTTCCCATTAGTCCTATTGAAACTGTACCAGTAAAATTGAAGCCAGGAATGGATGGCCCAAAGG TTAAACAATGGCCATTGACAGAAGAAAAAATAAAAGCATTAGTAGAAATTTGTACAGAAATGGAAAAAGA AGGGAAAATTTCAAAAATTGGGCCTGAAAATCCATACAATACTCCAGTATTTGCCATAAGGAAAAAAGAC AGTACTAAATGGAGAAAATTAGTAGATTTCAGAGAACTTAATAAGAGAACTCAGGACTTCTGGGAAGTCC AATTAGGAATACCACATCCCTCAGGGTTAAAAAAGAAAAAATCAGTAACAGTACTGGATGCGGGTGATGC ATATTTTTCAGTTCCCTTAGATGAAGACTTCAGAAAGTATACTGCATTTACCATACCTAGTGTAAACAAT GAGACACTAGGGATTAGGTATCAGTACAATGTGCTTCCACAAGGATGGAAAGGATCACCAGCAATATTCC AAGCTAGCATGACAAAAATCTTAGAGCCTTTTAGAAAACAAAATCCAGACATAGTTATCTATCAATACAT GGATGATCTGTATGTAGGATCTGACCTAGAAATAGGGCAGCATAGAACAAAAATAGAGGAACTGAGAGAA CATCTGTTGAGGTGGGGGTTTTGCACACCAGACAAGAAACATCAGAAGGAACCTCCATTCCTTTGGATGG GTTATGAACTCCATCCTT >reconstructed_1 Freq=0.294104 ACCAAATGAAAGATTGTACTGAGAGACAGGCTAATTTTTTAGGGAAGATCTGGCCTTCCCGCGGCGGAAG GCCAGGGAATTTTCTTCAGAGCAGACCAGAGCCAACAGCCCCACCAGAAGAGAGCTTCAGGTTTGGGGAG GAGACAACAACTCCCTCTCAGAAGCAGGAGCCGAAGGACAAGGAAACATATCCTTTAGCTTCCCTCAAAT CACTCTTTGGCAACGACCCCTTGTTACAATAAAGATAGGGGGGCAGCTAAAGGAAGCTCTATTAGATACA GGAGCAGATGACACAGTATTAGAAGAAATGGATTTGCCAGGAAGATGGAAACCAAAAATGATAGGGGGAA TTGGAGGTTTTATCAAAGTAAAACAGTATGATCAGATACCCATAGAAATTTGTGGACATAAAGTGATAGG TACAGTATTAGTAGGACCTACACCTGTCAACATAATTGGAAGAAATCTGTTGACTCAGCTTGGTTGCACT TTAAATTTTCCCATTAGTCCTATTGAAACTGTACCAGTAAAATTGAAGCCAGGAATGGATGGCCCAAAGG TTAAACAATGGCCATTGACAGAAGAAAAAATAAAAGCATTAATAGAAATCTGTGCAGAAATGGAAAAGGA AGGGAAAATTTCAAAAATTGGGCCTGAAAATCCATACAATACTCCAGTATTTGCCATAAGAAAAAAAGAC AGTACTAAATGGAGAAAATTAGTAGATTTCAAAGAACTTAATAAGAGAACTCAGGACTTCTGGGAAGTCC AATTAGGAATACCACATCCCTCAGGGTTAAAAAAGAAAAAGTCAGTAACAGTACTGGATGTGGGTGATGC ATATTTTTCAGTTCCCTTAGATGAAGACTTCAGAAAGTACACTGCATTTACCATACCTAGTGTAAACAAT GAGACACCAGGGATTAGGTATCAGTACAATGTGCTTCCACAAGGATGGAAAGGATCACCAGCAATATTCC AATGTAGCATGACAAAAATCTTAGAACCTTTTAGAAAACAAAATCCAGATATAGTTATCTATCAATACAT GGATGATCTGTATGTAGGATCTGACCTAGAAATAGGGCAGCATAGAACAAAAATAGAGGAACTGAGAGAA CATCTGTTGAGGTGGGGGTTTTGCACACCAGACAAGAAACATCAGAAGGAACCTCCATTCCTTTGGATGG GTTATGAACTCCATCCTT >reconstructed_2 Freq=0.515259 ACCAAATGAAAGATTGTACTGAGAGACAGGCTAATTTTTTAGGGAAAATCTGGCCTTCCCACAAGGGAAG GCCAGGGAATTTTCTTCAGAGCAGACCAGAGCCAACAGCCCCACCAGAAGAGAGCTTCAGGTTTGGGGAA GAGACAACAACTCCCTCTCAGAAGCAGGAGCCGATAGACAAGGAAATGTATCCTTTAGCTTCCCTCAAAT CACTCTTTGGCAACGACCCCTCGTCACAATAAAGATAGGGGGGCAACTAAAGGAAGCTCTATTAGATACA GGAGCAGATGATACAGTATTAGAAGAAATGAATTTGCCAGGAAGATGGAAACCAAAAATGATAGGGGGAA TTGGAGGTTTTATCAAAGTAAAACAGTATGATCAGATACCCATAGAAATCTGTGGACATAAAGCTATAGG TACAGTATTAGTAGGACCTACACCTGTCAACATAATTGGAAGAAATCTGTTGACTCAGATTGGTTGCACT TTAAATTTTCCCATTAGTCCTATTGAAACTGTACCAGTAAAATTAAAGCCAGGAATGGATGGCCCAAAAG TTAAACAATGGCCATTGACAGAAGAAAAAATAAAAGCATTAGTAGAAATTTGTACAGAAATGGAAAAGGA AGGGAAAATTTCAAAAATTGGGCCTGAAAATCCATACAATACTCCAGTATTTGCCATAAGGAAAAAAGAC AGTACTAAATGGAGAAAATTAGTAGATTTCAGAGAACTTAATAAGAGAACTCAAGACTTCCGGGAAGTCC AATTAGGAATACCACATCCCTCAGGGTTAAAAAAGAAAAAATCAGTAACAGTACTGGATGTGGGTGATGC ATATTTTTCAGTTCCCTTAGATGAAGACTTCAGAAAGTATACTGCATTTACCATACCTAGTGTAAACAAT GAGACACCAGGGATTAGGTATCAGTACAATGTGCTTCCACAAGGATGGAAAGGATCACCAGCAATATTCC AAGCTAGCATGACAAAAATCTTAGAGCCTTTTAGAAAACAAAATCCAGACATAGTTATCTATCAATACAT GGATGATCTGTATGTAGGATCTGACCTAGAAATAGGGCAGCATAGAACAAAAATAGAGGAACTGAGAGAA CATCTGTTGAGGTGGGGGTTTTGCACACCAGACAAGAAACATCAGAAGGAACCTCCATTCCTTTGGATGG GTTATGAACTCCATCCCC","title":"ph_parser"},{"location":"hp_haplotype/#cliquesnv","text":"A reference-based reconstruction of viral variants from NGS data( documentation ). Input is read files in Fastq format and reference Fasta file. Input reads are aligned and outputted to SAM format using BWA and SAMtools prior to running CliqueSNV. Output is inferred viral variants with respective frequencies and diversity. Please see the CliqueSNV documentation for a full description of CliqueSNV options. Usage: haphpipe cliquesnv [CliqueSNV OPTIONS] [HAPHPIPE OPTIONS] --fq1 <FASTQ> --fq2 <FASTQ> (or) --fqU <FASTQ> [--outdir] Output files: File Description cs[NUM]_[REGION].fasta Reconstructed haplotypes in FASTA format. cs[NUM]_[REGION].txt CliqueSNV output summary file. cs[NUM]_[REGION]_summary.txt HAPHPIPE-generated output summary file with sequence length and diversity. Input/Output Arguments: Option CliqueSNV Equivalent Description --fq1 SEQS NA Input reads in FASTQ format (read 1) --fq2 SEQS NA Input reads in FASTQ format (read 2) --fqU SEQS NA Input reads in FASTQ format (unpaired reads) --ref_fa REF_FA NA Reference FASTA --outdir OUTDIR --prefix Output directory (default: .) CliqueSNV Options: Option CliqueSNV Equivalent Description --jardir JARDIR -jar Path to clique-snv.jar --O22min O22MIN -t Minimum threshold for O22 value --O22minfreq --O22MINFREQ -tf Minimum threshold for O22 frequency relative to read coverage --printlog -log Print log data to console --merging MERGING -cm Cliques merging algorithm: accurate or fast --outputstart OUTPUTSTART -os Output start position --outputend OUTPUTEND -oe Output end position --fasta_format FASTA_FORMAT -fdf Fasta defline format: short or extended, add number at end to adjust precision of frequency Options: Option Description --keep_tmp Keep temporary directory --quiet Do not write output to console (silence stdout and stderr) (default: False) --logfile LOGFILE Name for log file (output) --debug Print commands but do not run (default: False) --ncpu NCPU Number of CPU to use (default: 1) Example usage: haphpipe cliquesnv --fq1 corrected_1.fastq --fq2 corrected_2.fastq --ref_fa final.fna","title":"cliquesnv"},{"location":"hp_reads/","text":"The Reads stage involves cleaning up the raw read sequences, as well as other processing steps. Modules to manipulate reads. Use -h after any command for a list of options. sample_reads Subsample reads using seqtk ( documentation ). Input is reads in FASTQ format. Output is sampled reads in FASTQ format. You do not have to have all read options (i.e., read1, read2 AND unpaired reads). You can have a combination of any of those. Usage: haphpipe sample_reads [OPTIONS] [SETTINGS] --fq1 <FASTQ> --fq2 <FASTQ> --fqU <FASTQ> [--outdir] (or): hp_sample_reads [OPTIONS] [SETTINGS] --fq1 <FASTQ> --fq2 <FASTQ> --fqU <FASTQ> [--outdir] Output files: sample_1.fastq sample_2.fastq Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --outdir Output directory (default: current directory). Options: Option Description --nreads Number of reads to sample. If greater than the number of reads in file, all reads will be sampled. --frac Fraction of reads to sample, between 0 and 1. Each read has [frac] --seed Seed for random number generator. Settings: Option Description --quiet Do not write output to console (silence stdout and stderr), default is False. --logfile Append console output to this file. --debug Print commands but do not run, default is False. Example usage: This pulls 1000 reads from these paired end files with a starting seed of 1234. haphpipe sample_reads --fq1 read_1.fastq --fq2 read_2.fastq --nreads 1000 --seed 1234 -- trim_reads Trim reads using Trimmomatic ( documentation ). Input is reads in FASTQ format. Output is trimmed reads in FASTQ format. Usage: haphpipe trim_reads [OPTIONS] [SETTINGS] --fq1 <FASTQ> --fq2 <FASTQ> --fqU <FASTQ> [--outdir] (or): hp_trim_reads [OPTIONS] [SETTINGS] --fq1 <FASTQ> --fq2 <FASTQ> --fqU <FASTQ> [--outdir] Output files: trimmed_1.fastq trimmed_2.fastq trimmed_U.fastq trimmomatic_summary.out Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --outdir Output directory (default: current directory). Options: Option Description --adapter_file Adapter file. --trimmers Trim commands for trimmomatic (default: ['LEADING:3', 'TRAILING:3', 'SLIDINGWINDOW:4:15', 'MINLEN:36']). --encoding Quality score encoding. Settings: Option Description --ncpu Number of CPU to use (default: 1). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: This trims paired end read files 1 and 2. haphpipe trim_reads --fq1 read_1.fastq --fq2 read_2.fastq -- join_reads Join reads using FLASH ( paper ). Input is reads in FASTQ format. Output is joined reads in FASTQ format. Usage: haphpipe join_reads [OPTIONS] [SETTINGS] --fq1 <FASTQ> --fq2 <FASTQ> [--outdir] (or): hp_join_reads [OPTIONS] [SETTINGS] --fq1 <FASTQ> --fq2 <FASTQ> [--outdir] Output files: joined.fastq notjoined_1.fastq notjoined_2.fastq Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --outdir Output directory (default: current directory). Settings: Option Description --min_overlap The minimum required overlap length between two reads to provide a confident overlap (default: 10). --max_overlap Maximum overlap length expected in approximately 90% of read pairs, longer overlaps are penalized. --allow_outies Also try combining read pairs in the \"outie\" orientation (default: False). --encoding Quality score encoding. Settings: Option Description --ncpu Number of CPU to use (default: 1). --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe join_reads --fq1 trimmed_1.fastq --fq2 trimmed_2.fastq ec_reads Error correction using SPAdes ( documentation ). Input is reads in FASTQ format. Output is error-corrected reads in FASTQ format. Remember that HAPHPIPE is intended for Illumina reads, therefore the error correction is based on Illumina sequencing errors. Usage: haphpipe ec_reads [SETTINGS] --fq1 <FASTQ> --fq2 <FASTQ> --fqU <FASTQ> [--outdir] (or): hp_ec_reads [SETTINGS] --fq1 <FASTQ> --fq2 <FASTQ> --fqU <FASTQ> [--outdir] Output files: corrected_1.fastq corrected_2.fastq corrected_U.fastq Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --outdir Output directory (default: current directory). Settings: Option Description --ncpu Number of CPU to use (default: 1). --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run, default is False. Example usage: haphpipe ec_reads --fq1 trimmed_1.fastq --fq2 trimmed_2.fastq","title":"Reads"},{"location":"hp_reads/#sample_reads","text":"Subsample reads using seqtk ( documentation ). Input is reads in FASTQ format. Output is sampled reads in FASTQ format. You do not have to have all read options (i.e., read1, read2 AND unpaired reads). You can have a combination of any of those. Usage: haphpipe sample_reads [OPTIONS] [SETTINGS] --fq1 <FASTQ> --fq2 <FASTQ> --fqU <FASTQ> [--outdir] (or): hp_sample_reads [OPTIONS] [SETTINGS] --fq1 <FASTQ> --fq2 <FASTQ> --fqU <FASTQ> [--outdir] Output files: sample_1.fastq sample_2.fastq Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --outdir Output directory (default: current directory). Options: Option Description --nreads Number of reads to sample. If greater than the number of reads in file, all reads will be sampled. --frac Fraction of reads to sample, between 0 and 1. Each read has [frac] --seed Seed for random number generator. Settings: Option Description --quiet Do not write output to console (silence stdout and stderr), default is False. --logfile Append console output to this file. --debug Print commands but do not run, default is False. Example usage: This pulls 1000 reads from these paired end files with a starting seed of 1234. haphpipe sample_reads --fq1 read_1.fastq --fq2 read_2.fastq --nreads 1000 --seed 1234 --","title":"sample_reads"},{"location":"hp_reads/#trim_reads","text":"Trim reads using Trimmomatic ( documentation ). Input is reads in FASTQ format. Output is trimmed reads in FASTQ format. Usage: haphpipe trim_reads [OPTIONS] [SETTINGS] --fq1 <FASTQ> --fq2 <FASTQ> --fqU <FASTQ> [--outdir] (or): hp_trim_reads [OPTIONS] [SETTINGS] --fq1 <FASTQ> --fq2 <FASTQ> --fqU <FASTQ> [--outdir] Output files: trimmed_1.fastq trimmed_2.fastq trimmed_U.fastq trimmomatic_summary.out Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --outdir Output directory (default: current directory). Options: Option Description --adapter_file Adapter file. --trimmers Trim commands for trimmomatic (default: ['LEADING:3', 'TRAILING:3', 'SLIDINGWINDOW:4:15', 'MINLEN:36']). --encoding Quality score encoding. Settings: Option Description --ncpu Number of CPU to use (default: 1). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: This trims paired end read files 1 and 2. haphpipe trim_reads --fq1 read_1.fastq --fq2 read_2.fastq --","title":"trim_reads"},{"location":"hp_reads/#join_reads","text":"Join reads using FLASH ( paper ). Input is reads in FASTQ format. Output is joined reads in FASTQ format. Usage: haphpipe join_reads [OPTIONS] [SETTINGS] --fq1 <FASTQ> --fq2 <FASTQ> [--outdir] (or): hp_join_reads [OPTIONS] [SETTINGS] --fq1 <FASTQ> --fq2 <FASTQ> [--outdir] Output files: joined.fastq notjoined_1.fastq notjoined_2.fastq Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --outdir Output directory (default: current directory). Settings: Option Description --min_overlap The minimum required overlap length between two reads to provide a confident overlap (default: 10). --max_overlap Maximum overlap length expected in approximately 90% of read pairs, longer overlaps are penalized. --allow_outies Also try combining read pairs in the \"outie\" orientation (default: False). --encoding Quality score encoding. Settings: Option Description --ncpu Number of CPU to use (default: 1). --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe join_reads --fq1 trimmed_1.fastq --fq2 trimmed_2.fastq","title":"join_reads"},{"location":"hp_reads/#ec_reads","text":"Error correction using SPAdes ( documentation ). Input is reads in FASTQ format. Output is error-corrected reads in FASTQ format. Remember that HAPHPIPE is intended for Illumina reads, therefore the error correction is based on Illumina sequencing errors. Usage: haphpipe ec_reads [SETTINGS] --fq1 <FASTQ> --fq2 <FASTQ> --fqU <FASTQ> [--outdir] (or): hp_ec_reads [SETTINGS] --fq1 <FASTQ> --fq2 <FASTQ> --fqU <FASTQ> [--outdir] Output files: corrected_1.fastq corrected_2.fastq corrected_U.fastq Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --outdir Output directory (default: current directory). Settings: Option Description --ncpu Number of CPU to use (default: 1). --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run, default is False. Example usage: haphpipe ec_reads --fq1 trimmed_1.fastq --fq2 trimmed_2.fastq","title":"ec_reads"},{"location":"inout/","text":"Module Input Input Format Output Output File Names sample_reads FASTQ file(s) .fastq.gz or .fastq or .fq Subampled FASTQ file(s) sample_1.fastq sample_2.fastq trim_reads FASTQ file(s) .fastq.gz or .fastq or .fq Trimmed FASTQ file(s) and summary from Trimmomatic text file trimmed_1.fastq trimmed_2.fastq trimmed_U.fastq trimmomatic_summary.out join_reads FASTQ file(s) .fastq.gz or .fastq or .fq FASTQ files for joined reads and unjoined reads joined.fastq notjoined_1.fastq notjoined_2.fastq ec_reads FASTQ file(s) .fastq.gz or .fastq or .fq Error corrected FASTQ file(s) corrected_1.fastq corrected_2.fastq corrected_U.fastq assemble_denovo FASTQ file(s) .fastq.gz or .fastq or .fq De novo assembled contigs FASTA file and de novo summary text file denovo_contigs.fna denovo_summary.txt assemble_amplicons FASTA files and GTF file .fna or .fasta or .fa and .gtf FASTA file with assembled amplicons amplicon_assembly.fna assemble_scaffold FASTA files .fna or .fasta or .fa FASTA files with scaffolded and aligned sequences and FASTA file with assembled amplicons scaffold_aligned.fa scaffold_assembly.fa scaffold_imputed.fa scaffold_padded.out align_reads FASTQ files and FASTA file .fastq.gz or .fastq or .fq and .fna or .fasta or .fa Aligned BAM file and Bowtie2 alignment output summary text file aligned.bam aligned.bt2.out call_variants BAM and FASTA file .bam and .fna or .fasta or .fa VCF file with variants variants.vcf.gz vcf_to_consensus VCF file .vcf FASTA file with consensus sequence consensus.fna refine_assembly FASTQ files and FASTA file .fastq.gz or .fastq or .fq and .fna or .fasta or .fa FASTA file with refined consensus sequence refined.fna finalize_assembly FASTQ files and FASTA file .fastq.gz or .fastq or .fq and .fna or .fasta or .fa FASTA file with final refined consensus sequence and final BAM file with reads aligned to final FASTA file and VCF file with variants relative to final.fna final.fna and final.bam and final.vcf.gz predict_haplo FASTQ files and FASTA file .fastq.gz or .fastq or .fq and .fna or .fasta or .fa PredictHaplo's fasta-like output file best.fa ph_parser PredictHaplo's FAS output file .fas Summary text file with haplotype diversity statistics and FASTA file with haplotype sequences ph_summary.txt and ph_haplotypes.fna clique_snv FASTQ files and FASTA file .fastq.gz or .fastq or .fq and .fna or .fasta or .fa FASTA file with haplotype sequences and TXT files with summary information cs0X_REGION.fasta cs0X_REGION.txt cs0X_REGION_summary.txt pairwise_align FASTA files and GTF file .fna or .fasta or .fa and .gtf JSON file pairwise_aligned.json extract_pairwise JSON file from pairwise_align output .json FASTA output with region extracted to standard out stdout.fasta summary_stats Direcoty lists for assembly and haplotype files (TXT) .txt Summary statistics (TXT and TSV) summary_stats.txt summary_stats.tsv PH_summary_stats.tsv multiple_align FASTA files and/or directory list (TXT) and GTF file .fna or .fasta or .fa and .txt and .gtf FASTA alignment and sequence files separated by amplicons in GTF alignment_regionX.fasta all_sequences_regionX.fasta model_test FASTA or PHYLIP alignment file .fna or .fasta or .fa or .phy Text file containing best-fit models modeltest_results.out build_tree FASTA or PHYLIP alignment file .fna or .fasta or .fa RAxML output files RaxML_info.build_tree.tre and RAxML tree files","title":"File Input/Output"},{"location":"install/","text":"HAPHPIPE Installation Instructions HAPHIPE depends on more than a dozen different programs, each of which may itself depend on other programs and libraries. Installing everything separately would be a nightmare, so you should use the package manager \"Bioconda\" to install the HAPHPIPE package. Bioconda is a popular package manager in the bioinformatics world. See the Helpful Resources section for more information and resources for Bioconda. Here, we will describe where to get Bioconda and how to install it, then how to use Bioconda to install HAPHPIPE. We will also detail the acquisition and installation of one program, GATK, that is not handled by Bioconda. Here, we describe the procedure for installing HAPHPIPE using the package manager Bioconda (Gr\u00fcning et al. 2018) on the command line. If you are unfamiliar with Bioconda, please see for installation and channel setup. The source code for HAPHPIPE is available here and is written in Python 3.7.2. The installation process begins with the creation of a conda environment that installs HAPHPIPE and its dependencies together with one short command. Once the conda environment has been created, it can be activated for use with the command conda activate haphpipe. Due to license restrictions, Bioconda cannot distribute and install GATK (McKenna et al. 2010; Van der Auwera et al. 2013; Poplin et al. 2018) directly. To fully install GATK, you must download a licensed copy of GATK (version 3.8-0) from the Broad Institute (link) . 1. Install conda Download the conda package: wget https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-x86_64.sh sh Miniconda3-latest-MacOSX-x86_64.sh 2. Set up conda channels These need to be put in as a command in the order they come, as this sets the priority for where packages are pulled from. Therefore, in this order, conda-forge is top priority and explored first when downloading a program. conda config --add channels R conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge 3. Create a conda environment for HAPHPIPE conda create -n haphpipe haphpipe This will install HAPHPIPE and all dependencies in Bioconda.Note: on some HPC systems (including GW's Pegasus), certain dependencies must be installed as a 'User Install'. If this is the case (which will be apparent if you receive availability errors for any of the above packages after installing them through conda) use the following command to install the package: pip install --user <package name> 4. Activate the environment conda activate haphpipe You can now test your HAPHPIPE install by running haphpipe -h . 5. Install GATK Due to license restrictions, bioconda cannot distribute and install GATK directly. To fully install GATK, you must download a licensed copy of GATK (version 3.8-0) from the Broad Institute and register the package using gatk3-register. gsutil has been included as a dependency in HAPHPIPE, so you do not need to worry about installing it for the GATK acquisition. This will copy GATK into your conda environment. GATK needs to be installed directly on the command line with the following commands: mkdir -p /path/to/gatk_dir gsutil cp gs://gatk-software/package-archive/gatk/GenomeAnalysisTK-3.8-0-ge9d806836.tar.bz2 path/to/gatk_dir/ gatk3-register /path/to/gatk_dir/GenomeAnalysisTK.jar NOTE: HAPHPIPE was developed and tested using GATK 3.8. PredictHaplo Installation Instructions Users are required to download PredictHaplo on their own computing system prior to running any of the haplotype modules ( predict_haplo and ph_parser ). Here is how the GW CBI team installed PredictHaplo onto our HPC, which has a slurm scheduling system and uses Lua module files. We cannot help with the installation of this software, but have provided the code that we used here to install PredictHaplo onto our system. Please see their website for contact information if needed. This module loads predicthaplo onto GWU's HPC - Colonial One. See https://bmda.dmi.unibas.ch/software.html cd /path/to/modules/predicthaplo # use gcc 4.9.4, add blas and lapack to library path module load gcc/4.9.4 module load blas/gcc/64 module load lapack/gcc/64 # download source cd archive wget https://bmda.dmi.unibas.ch/software/PredictHaplo-Paired-0.4.tgz cd .. # unzip source and change directory name tar xfvz archive/PredictHaplo-Paired-0.4.tgz mv PredictHaplo-Paired-0.4 0.4 cd 0.4 # install scythestat tar xfvz scythestat-1.0.3.tar.gz cd scythestat-1.0.3 ./configure --prefix=/path/to/modules/predicthaplo/0.4/NEWSCYTHE make install cd .. # compile predicthaplo make If a segfault error occurs during the hp_predict_haplo module, this is not a characteristic of HAPHPIPE but rather that of PredictHaplo. Sometimes, we have luck if we just rerun the code again or move to an interactive CPU node. We are unsure what causes this error, and we only see it between the local and global reconstruction phases in PredictHaplo. Quick-Start 1. Activate haphpipe Make sure you have conda running. For students at GW using Colonial One, you need to load the miniconda3 module like such prior to activating the haphpipe conda environemnt: module load miniconda3 . conda activate haphpipe 2. Test that it is loaded correctly haphpipe -h should produce: Program: haphpipe (haplotype and phylodynamics pipeline) Version: 1.0.2 Commands: -- Reads sample_reads subsample reads using seqtk trim_reads trim reads using Trimmomatic join_reads join reads using FLASh ec_reads error correct reads using SPAdes -- Assemble assemble_denovo assemble reads denovo assemble_amplicons assemble contigs to amplicon regions assemble_scaffold assemble contigs to genome align_reads align reads to reference call_variants call variants vcf_to_consensus create consensus sequence from VCF refine_assembly iterative refinement: align - variants - consensus finalize_assembly finalize consensus sequence -- Haplotype predict_haplo assemble haplotypes with PredictHaplo ph_parser parse output from PredictHaplo cliquesnv assemble haplotypes with CliqueSNV -- Description pairwise_align align consensus to an annotated reference extract_pairwise extract sequence regions from pairwise alignment summary_stats generates summary statistics for samples -- Phylo multiple_align multiple sequence alignment model_test tests for model of evolution using ModelTest build_tree_NG builds phylogenetic tree with RAxML-NG -- Miscellaneous demo setup demo directory and test data Directory Structure Below is the recommended directory structure for using HAPHPIPE: We recommend creating a separate directory for each sample, as well as a directory for reference files. Windows Users HAPHPIPE is only available for Mac OSX or Linux platforms. We suggest the following options for running HAPHPIPE on a Windows machine: Run HAPHPIPE on your institution's HPC cluster, if available. Utilize the Windows Subsystem for Linux . Run Linux in a virtual machine via VirtualBox Reference Files Several modules in HAPHPIPE use reference files: either a FASTA file containing a reference sequence or a GTF file denoting genome regions for amplicon assembly. With HIV data, we use the file HIV_B.K03455.HXB2.fasta as a reference whole-genome file, HIV_B.K03455.HXB2.amplicons.fasta for amplicon assembly, and HIV_B.K03455.HXB2.gtf as a GTF file. All three files are downloaded in the demo module . The contents of each file are below. HIV_B.K03455.HXB2.fasta >HIV_B.K03455.HXB2 TGGAAGGGCTAATTCACTCCCAACGAAGACAAGATATCCTTGATCTGTGGATCTACCACACACAAGGCTACTTCCCTGATTAGCAGAACTACACACCAGG GCCAGGGATCAGATATCCACTGACCTTTGGATGGTGCTACAAGCTAGTACCAGTTGAGCCAGAGAAGTTAGAAGAAGCCAACAAAGGAGAGAACACCAGC TTGTTACACCCTGTGAGCCTGCATGGAATGGATGACCCGGAGAGAGAAGTGTTAGAGTGGAGGTTTGACAGCCGCCTAGCATTTCATCACATGGCCCGAG AGCTGCATCCGGAGTACTTCAAGAACTGCTGACATCGAGCTTGCTACAAGGGACTTTCCGCTGGGGACTTTCCAGGGAGGCGTGGCCTGGGCGGGACTGG GGAGTGGCGAGCCCTCAGATCCTGCATATAAGCAGCTGCTTTTTGCCTGTACTGGGTCTCTCTGGTTAGACCAGATCTGAGCCTGGGAGCTCTCTGGCTA ACTAGGGAACCCACTGCTTAAGCCTCAATAAAGCTTGCCTTGAGTGCTTCAAGTAGTGTGTGCCCGTCTGTTGTGTGACTCTGGTAACTAGAGATCCCTC AGACCCTTTTAGTCAGTGTGGAAAATCTCTAGCAGTGGCGCCCGAACAGGGACCTGAAAGCGAAAGGGAAACCAGAGGAGCTCTCTCGACGCAGGACTCG GCTTGCTGAAGCGCGCACGGCAAGAGGCGAGGGGCGGCGACTGGTGAGTACGCCAAAAATTTTGACTAGCGGAGGCTAGAAGGAGAGAGATGGGTGCGAG AGCGTCAGTATTAAGCGGGGGAGAATTAGATCGATGGGAAAAAATTCGGTTAAGGCCAGGGGGAAAGAAAAAATATAAATTAAAACATATAGTATGGGCA AGCAGGGAGCTAGAACGATTCGCAGTTAATCCTGGCCTGTTAGAAACATCAGAAGGCTGTAGACAAATACTGGGACAGCTACAACCATCCCTTCAGACAG GATCAGAAGAACTTAGATCATTATATAATACAGTAGCAACCCTCTATTGTGTGCATCAAAGGATAGAGATAAAAGACACCAAGGAAGCTTTAGACAAGAT AGAGGAAGAGCAAAACAAAAGTAAGAAAAAAGCACAGCAAGCAGCAGCTGACACAGGACACAGCAATCAGGTCAGCCAAAATTACCCTATAGTGCAGAAC ATCCAGGGGCAAATGGTACATCAGGCCATATCACCTAGAACTTTAAATGCATGGGTAAAAGTAGTAGAAGAGAAGGCTTTCAGCCCAGAAGTGATACCCA TGTTTTCAGCATTATCAGAAGGAGCCACCCCACAAGATTTAAACACCATGCTAAACACAGTGGGGGGACATCAAGCAGCCATGCAAATGTTAAAAGAGAC CATCAATGAGGAAGCTGCAGAATGGGATAGAGTGCATCCAGTGCATGCAGGGCCTATTGCACCAGGCCAGATGAGAGAACCAAGGGGAAGTGACATAGCA GGAACTACTAGTACCCTTCAGGAACAAATAGGATGGATGACAAATAATCCACCTATCCCAGTAGGAGAAATTTATAAAAGATGGATAATCCTGGGATTAA ATAAAATAGTAAGAATGTATAGCCCTACCAGCATTCTGGACATAAGACAAGGACCAAAGGAACCCTTTAGAGACTATGTAGACCGGTTCTATAAAACTCT AAGAGCCGAGCAAGCTTCACAGGAGGTAAAAAATTGGATGACAGAAACCTTGTTGGTCCAAAATGCGAACCCAGATTGTAAGACTATTTTAAAAGCATTG GGACCAGCGGCTACACTAGAAGAAATGATGACAGCATGTCAGGGAGTAGGAGGACCCGGCCATAAGGCAAGAGTTTTGGCTGAAGCAATGAGCCAAGTAA CAAATTCAGCTACCATAATGATGCAGAGAGGCAATTTTAGGAACCAAAGAAAGATTGTTAAGTGTTTCAATTGTGGCAAAGAAGGGCACACAGCCAGAAA TTGCAGGGCCCCTAGGAAAAAGGGCTGTTGGAAATGTGGAAAGGAAGGACACCAAATGAAAGATTGTACTGAGAGACAGGCTAATTTTTTAGGGAAGATC TGGCCTTCCTACAAGGGAAGGCCAGGGAATTTTCTTCAGAGCAGACCAGAGCCAACAGCCCCACCAGAAGAGAGCTTCAGGTCTGGGGTAGAGACAACAA CTCCCCCTCAGAAGCAGGAGCCGATAGACAAGGAACTGTATCCTTTAACTTCCCTCAGGTCACTCTTTGGCAACGACCCCTCGTCACAATAAAGATAGGG GGGCAACTAAAGGAAGCTCTATTAGATACAGGAGCAGATGATACAGTATTAGAAGAAATGAGTTTGCCAGGAAGATGGAAACCAAAAATGATAGGGGGAA TTGGAGGTTTTATCAAAGTAAGACAGTATGATCAGATACTCATAGAAATCTGTGGACATAAAGCTATAGGTACAGTATTAGTAGGACCTACACCTGTCAA CATAATTGGAAGAAATCTGTTGACTCAGATTGGTTGCACTTTAAATTTTCCCATTAGCCCTATTGAGACTGTACCAGTAAAATTAAAGCCAGGAATGGAT GGCCCAAAAGTTAAACAATGGCCATTGACAGAAGAAAAAATAAAAGCATTAGTAGAAATTTGTACAGAGATGGAAAAGGAAGGGAAAATTTCAAAAATTG GGCCTGAAAATCCATACAATACTCCAGTATTTGCCATAAAGAAAAAAGACAGTACTAAATGGAGAAAATTAGTAGATTTCAGAGAACTTAATAAGAGAAC TCAAGACTTCTGGGAAGTTCAATTAGGAATACCACATCCCGCAGGGTTAAAAAAGAAAAAATCAGTAACAGTACTGGATGTGGGTGATGCATATTTTTCA GTTCCCTTAGATGAAGACTTCAGGAAGTATACTGCATTTACCATACCTAGTATAAACAATGAGACACCAGGGATTAGATATCAGTACAATGTGCTTCCAC AGGGATGGAAAGGATCACCAGCAATATTCCAAAGTAGCATGACAAAAATCTTAGAGCCTTTTAGAAAACAAAATCCAGACATAGTTATCTATCAATACAT GGATGATTTGTATGTAGGATCTGACTTAGAAATAGGGCAGCATAGAACAAAAATAGAGGAGCTGAGACAACATCTGTTGAGGTGGGGACTTACCACACCA GACAAAAAACATCAGAAAGAACCTCCATTCCTTTGGATGGGTTATGAACTCCATCCTGATAAATGGACAGTACAGCCTATAGTGCTGCCAGAAAAAGACA GCTGGACTGTCAATGACATACAGAAGTTAGTGGGGAAATTGAATTGGGCAAGTCAGATTTACCCAGGGATTAAAGTAAGGCAATTATGTAAACTCCTTAG AGGAACCAAAGCACTAACAGAAGTAATACCACTAACAGAAGAAGCAGAGCTAGAACTGGCAGAAAACAGAGAGATTCTAAAAGAACCAGTACATGGAGTG TATTATGACCCATCAAAAGACTTAATAGCAGAAATACAGAAGCAGGGGCAAGGCCAATGGACATATCAAATTTATCAAGAGCCATTTAAAAATCTGAAAA CAGGAAAATATGCAAGAATGAGGGGTGCCCACACTAATGATGTAAAACAATTAACAGAGGCAGTGCAAAAAATAACCACAGAAAGCATAGTAATATGGGG AAAGACTCCTAAATTTAAACTGCCCATACAAAAGGAAACATGGGAAACATGGTGGACAGAGTATTGGCAAGCCACCTGGATTCCTGAGTGGGAGTTTGTT AATACCCCTCCCTTAGTGAAATTATGGTACCAGTTAGAGAAAGAACCCATAGTAGGAGCAGAAACCTTCTATGTAGATGGGGCAGCTAACAGGGAGACTA AATTAGGAAAAGCAGGATATGTTACTAATAGAGGAAGACAAAAAGTTGTCACCCTAACTGACACAACAAATCAGAAGACTGAGTTACAAGCAATTTATCT AGCTTTGCAGGATTCGGGATTAGAAGTAAACATAGTAACAGACTCACAATATGCATTAGGAATCATTCAAGCACAACCAGATCAAAGTGAATCAGAGTTA GTCAATCAAATAATAGAGCAGTTAATAAAAAAGGAAAAGGTCTATCTGGCATGGGTACCAGCACACAAAGGAATTGGAGGAAATGAACAAGTAGATAAAT TAGTCAGTGCTGGAATCAGGAAAGTACTATTTTTAGATGGAATAGATAAGGCCCAAGATGAACATGAGAAATATCACAGTAATTGGAGAGCAATGGCTAG TGATTTTAACCTGCCACCTGTAGTAGCAAAAGAAATAGTAGCCAGCTGTGATAAATGTCAGCTAAAAGGAGAAGCCATGCATGGACAAGTAGACTGTAGT CCAGGAATATGGCAACTAGATTGTACACATTTAGAAGGAAAAGTTATCCTGGTAGCAGTTCATGTAGCCAGTGGATATATAGAAGCAGAAGTTATTCCAG CAGAAACAGGGCAGGAAACAGCATATTTTCTTTTAAAATTAGCAGGAAGATGGCCAGTAAAAACAATACATACTGACAATGGCAGCAATTTCACCGGTGC TACGGTTAGGGCCGCCTGTTGGTGGGCGGGAATCAAGCAGGAATTTGGAATTCCCTACAATCCCCAAAGTCAAGGAGTAGTAGAATCTATGAATAAAGAA TTAAAGAAAATTATAGGACAGGTAAGAGATCAGGCTGAACATCTTAAGACAGCAGTACAAATGGCAGTATTCATCCACAATTTTAAAAGAAAAGGGGGGA TTGGGGGGTACAGTGCAGGGGAAAGAATAGTAGACATAATAGCAACAGACATACAAACTAAAGAATTACAAAAACAAATTACAAAAATTCAAAATTTTCG GGTTTATTACAGGGACAGCAGAAATCCACTTTGGAAAGGACCAGCAAAGCTCCTCTGGAAAGGTGAAGGGGCAGTAGTAATACAAGATAATAGTGACATA AAAGTAGTGCCAAGAAGAAAAGCAAAGATCATTAGGGATTATGGAAAACAGATGGCAGGTGATGATTGTGTGGCAAGTAGACAGGATGAGGATTAGAACA TGGAAAAGTTTAGTAAAACACCATATGTATGTTTCAGGGAAAGCTAGGGGATGGTTTTATAGACATCACTATGAAAGCCCTCATCCAAGAATAAGTTCAG AAGTACACATCCCACTAGGGGATGCTAGATTGGTAATAACAACATATTGGGGTCTGCATACAGGAGAAAGAGACTGGCATTTGGGTCAGGGAGTCTCCAT AGAATGGAGGAAAAAGAGATATAGCACACAAGTAGACCCTGAACTAGCAGACCAACTAATTCATCTGTATTACTTTGACTGTTTTTCAGACTCTGCTATA AGAAAGGCCTTATTAGGACACATAGTTAGCCCTAGGTGTGAATATCAAGCAGGACATAACAAGGTAGGATCTCTACAATACTTGGCACTAGCAGCATTAA TAACACCAAAAAAGATAAAGCCACCTTTGCCTAGTGTTACGAAACTGACAGAGGATAGATGGAACAAGCCCCAGAAGACCAAGGGCCACAGAGGGAGCCA CACAATGAATGGACACTAGAGCTTTTAGAGGAGCTTAAGAATGAAGCTGTTAGACATTTTCCTAGGATTTGGCTCCATGGCTTAGGGCAACATATCTATG AAACTTATGGGGATACTTGGGCAGGAGTGGAAGCCATAATAAGAATTCTGCAACAACTGCTGTTTATCCATTTTCAGAATTGGGTGTCGACATAGCAGAA TAGGCGTTACTCGACAGAGGAGAGCAAGAAATGGAGCCAGTAGATCCTAGACTAGAGCCCTGGAAGCATCCAGGAAGTCAGCCTAAAACTGCTTGTACCA ATTGCTATTGTAAAAAGTGTTGCTTTCATTGCCAAGTTTGTTTCATAACAAAAGCCTTAGGCATCTCCTATGGCAGGAAGAAGCGGAGACAGCGACGAAG AGCTCATCAGAACAGTCAGACTCATCAAGCTTCTCTATCAAAGCAGTAAGTAGTACATGTAACGCAACCTATACCAATAGTAGCAATAGTAGCATTAGTA GTAGCAATAATAATAGCAATAGTTGTGTGGTCCATAGTAATCATAGAATATAGGAAAATATTAAGACAAAGAAAAATAGACAGGTTAATTGATAGACTAA TAGAAAGAGCAGAAGACAGTGGCAATGAGAGTGAAGGAGAAATATCAGCACTTGTGGAGATGGGGGTGGAGATGGGGCACCATGCTCCTTGGGATGTTGA TGATCTGTAGTGCTACAGAAAAATTGTGGGTCACAGTCTATTATGGGGTACCTGTGTGGAAGGAAGCAACCACCACTCTATTTTGTGCATCAGATGCTAA AGCATATGATACAGAGGTACATAATGTTTGGGCCACACATGCCTGTGTACCCACAGACCCCAACCCACAAGAAGTAGTATTGGTAAATGTGACAGAAAAT TTTAACATGTGGAAAAATGACATGGTAGAACAGATGCATGAGGATATAATCAGTTTATGGGATCAAAGCCTAAAGCCATGTGTAAAATTAACCCCACTCT GTGTTAGTTTAAAGTGCACTGATTTGAAGAATGATACTAATACCAATAGTAGTAGCGGGAGAATGATAATGGAGAAAGGAGAGATAAAAAACTGCTCTTT CAATATCAGCACAAGCATAAGAGGTAAGGTGCAGAAAGAATATGCATTTTTTTATAAACTTGATATAATACCAATAGATAATGATACTACCAGCTATAAG TTGACAAGTTGTAACACCTCAGTCATTACACAGGCCTGTCCAAAGGTATCCTTTGAGCCAATTCCCATACATTATTGTGCCCCGGCTGGTTTTGCGATTC TAAAATGTAATAATAAGACGTTCAATGGAACAGGACCATGTACAAATGTCAGCACAGTACAATGTACACATGGAATTAGGCCAGTAGTATCAACTCAACT GCTGTTAAATGGCAGTCTAGCAGAAGAAGAGGTAGTAATTAGATCTGTCAATTTCACGGACAATGCTAAAACCATAATAGTACAGCTGAACACATCTGTA GAAATTAATTGTACAAGACCCAACAACAATACAAGAAAAAGAATCCGTATCCAGAGAGGACCAGGGAGAGCATTTGTTACAATAGGAAAAATAGGAAATA TGAGACAAGCACATTGTAACATTAGTAGAGCAAAATGGAATAACACTTTAAAACAGATAGCTAGCAAATTAAGAGAACAATTTGGAAATAATAAAACAAT AATCTTTAAGCAATCCTCAGGAGGGGACCCAGAAATTGTAACGCACAGTTTTAATTGTGGAGGGGAATTTTTCTACTGTAATTCAACACAACTGTTTAAT AGTACTTGGTTTAATAGTACTTGGAGTACTGAAGGGTCAAATAACACTGAAGGAAGTGACACAATCACCCTCCCATGCAGAATAAAACAAATTATAAACA TGTGGCAGAAAGTAGGAAAAGCAATGTATGCCCCTCCCATCAGTGGACAAATTAGATGTTCATCAAATATTACAGGGCTGCTATTAACAAGAGATGGTGG TAATAGCAACAATGAGTCCGAGATCTTCAGACCTGGAGGAGGAGATATGAGGGACAATTGGAGAAGTGAATTATATAAATATAAAGTAGTAAAAATTGAA CCATTAGGAGTAGCACCCACCAAGGCAAAGAGAAGAGTGGTGCAGAGAGAAAAAAGAGCAGTGGGAATAGGAGCTTTGTTCCTTGGGTTCTTGGGAGCAG CAGGAAGCACTATGGGCGCAGCCTCAATGACGCTGACGGTACAGGCCAGACAATTATTGTCTGGTATAGTGCAGCAGCAGAACAATTTGCTGAGGGCTAT TGAGGCGCAACAGCATCTGTTGCAACTCACAGTCTGGGGCATCAAGCAGCTCCAGGCAAGAATCCTGGCTGTGGAAAGATACCTAAAGGATCAACAGCTC CTGGGGATTTGGGGTTGCTCTGGAAAACTCATTTGCACCACTGCTGTGCCTTGGAATGCTAGTTGGAGTAATAAATCTCTGGAACAGATTTGGAATCACA CGACCTGGATGGAGTGGGACAGAGAAATTAACAATTACACAAGCTTAATACACTCCTTAATTGAAGAATCGCAAAACCAGCAAGAAAAGAATGAACAAGA ATTATTGGAATTAGATAAATGGGCAAGTTTGTGGAATTGGTTTAACATAACAAATTGGCTGTGGTATATAAAATTATTCATAATGATAGTAGGAGGCTTG GTAGGTTTAAGAATAGTTTTTGCTGTACTTTCTATAGTGAATAGAGTTAGGCAGGGATATTCACCATTATCGTTTCAGACCCACCTCCCAACCCCGAGGG GACCCGACAGGCCCGAAGGAATAGAAGAAGAAGGTGGAGAGAGAGACAGAGACAGATCCATTCGATTAGTGAACGGATCCTTGGCACTTATCTGGGACGA TCTGCGGAGCCTGTGCCTCTTCAGCTACCACCGCTTGAGAGACTTACTCTTGATTGTAACGAGGATTGTGGAACTTCTGGGACGCAGGGGGTGGGAAGCC CTCAAATATTGGTGGAATCTCCTACAGTATTGGAGTCAGGAACTAAAGAATAGTGCTGTTAGCTTGCTCAATGCCACAGCCATAGCAGTAGCTGAGGGGA CAGATAGGGTTATAGAAGTAGTACAAGGAGCTTGTAGAGCTATTCGCCACATACCTAGAAGAATAAGACAGGGCTTGGAAAGGATTTTGCTATAAGATGG GTGGCAAGTGGTCAAAAAGTAGTGTGATTGGATGGCCTACTGTAAGGGAAAGAATGAGACGAGCTGAGCCAGCAGCAGATAGGGTGGGAGCAGCATCTCG AGACCTGGAAAAACATGGAGCAATCACAAGTAGCAATACAGCAGCTACCAATGCTGCTTGTGCCTGGCTAGAAGCACAAGAGGAGGAGGAGGTGGGTTTT CCAGTCACACCTCAGGTACCTTTAAGACCAATGACTTACAAGGCAGCTGTAGATCTTAGCCACTTTTTAAAAGAAAAGGGGGGACTGGAAGGGCTAATTC ACTCCCAAAGAAGACAAGATATCCTTGATCTGTGGATCTACCACACACAAGGCTACTTCCCTGATTAGCAGAACTACACACCAGGGCCAGGGGTCAGATA TCCACTGACCTTTGGATGGTGCTACAAGCTAGTACCAGTTGAGCCAGATAAGATAGAAGAGGCCAATAAAGGAGAGAACACCAGCTTGTTACACCCTGTG AGCCTGCATGGGATGGATGACCCGGAGAGAGAAGTGTTAGAGTGGAGGTTTGACAGCCGCCTAGCATTTCATCACGTGGCCCGAGAGCTGCATCCGGAGT ACTTCAAGAACTGCTGACATCGAGCTTGCTACAAGGGACTTTCCGCTGGGGACTTTCCAGGGAGGCGTGGCCTGGGCGGGACTGGGGAGTGGCGAGCCCT CAGATCCTGCATATAAGCAGCTGCTTTTTGCCTGTACTGGGTCTCTCTGGTTAGACCAGATCTGAGCCTGGGAGCTCTCTGGCTAACTAGGGAACCCACT GCTTAAGCCTCAATAAAGCTTGCCTTGAGTGCTTCAAGTAGTGTGTGCCCGTCTGTTGTGTGACTCTGGTAACTAGAGATCCCTCAGACCCTTTTAGTCA GTGTGGAAAATCTCTAGCA HIV_B.K03455.HXB2.amplicons.fasta >ref|HIV_B.K03455.HXB2|reg|PRRT| CCCTCAGGTCACTCTTTGGCAACGACCCCTCGTCACAATAAAGATAGGGGGGCAACTAAAGGAAGCTCTATTAGATACAG GAGCAGATGATACAGTATTAGAAGAAATGAGTTTGCCAGGAAGATGGAAACCAAAAATGATAGGGGGAATTGGAGGTTTT ATCAAAGTAAGACAGTATGATCAGATACTCATAGAAATCTGTGGACATAAAGCTATAGGTACAGTATTAGTAGGACCTAC ACCTGTCAACATAATTGGAAGAAATCTGTTGACTCAGATTGGTTGCACTTTAAATTTTCCCATTAGCCCTATTGAGACTG TACCAGTAAAATTAAAGCCAGGAATGGATGGCCCAAAAGTTAAACAATGGCCATTGACAGAAGAAAAAATAAAAGCATTA GTAGAAATTTGTACAGAGATGGAAAAGGAAGGGAAAATTTCAAAAATTGGGCCTGAAAATCCATACAATACTCCAGTATT TGCCATAAAGAAAAAAGACAGTACTAAATGGAGAAAATTAGTAGATTTCAGAGAACTTAATAAGAGAACTCAAGACTTCT GGGAAGTTCAATTAGGAATACCACATCCCGCAGGGTTAAAAAAGAAAAAATCAGTAACAGTACTGGATGTGGGTGATGCA TATTTTTCAGTTCCCTTAGATGAAGACTTCAGGAAGTATACTGCATTTACCATACCTAGTATAAACAATGAGACACCAGG GATTAGATATCAGTACAATGTGCTTCCACAGGGATGGAAAGGATCACCAGCAATATTCCAAAGTAGCATGACAAAAATCT TAGAGCCTTTTAGAAAACAAAATCCAGACATAGTTATCTATCAATACATGGATGATTTGTATGTAGGATCTGACTTAGAA ATAGGGCAGCATAGAACAAAAATAGAGGAGCTGAGACAACATCTGTTGAGGTGGGGACTTACCACACCAGACAAAAAACA TCAGAAAGAACCTCCATTCCTTTGGATGGGTTATGAACTCCATCCTGATAAATGGACAGTACAGCCTATAGTGCTGCCAG AAAAAGACAGCTGGACTGTCAATGACATACAGAAGTTAGTGGGGAAATTGAATTGGGCAAGTCAGATTTACCCAGGGATT AAAGTAAGGCAATTATGTAAACTCCTTAGAGGAACCAAAGCACTAACAGAAGTAATACCACTAACAGAAGAAGCAGAGCT AGAACTGGCAGAAAACAGAGAGATTCTAAAAGAACCAGTACATGGAGTGTATTATGACCCATCAAAAGACTTAATAGCAG AAATACAGAAGCAGGGGCAAGGCCAATGGACATATCAAATTTATCAAGAGCCATTTAAAAATCTGAAAACAGGAAAATAT GCAAGAATGAGGGGTGCCCACACTAATGATGTAAAACAATTAACAGAGGCAGTGCAAAAAATAACCACAGAAAGCATAGT AATATGGGGAAAGACTCCTAAATTTAAACTGCCCATACAAAAGGAAACATGGGAAACATGGTGGACAGAGTATTGGCAAG CCACCTGGATTCCTGAGTGGGAGTTTGTTAATACCCCTCCCTTAGTGAAATTATGGTACCAGTTAGAGAAAGAACCCATA GTAGGAGCAGAAACCTTC >ref|HIV_B.K03455.HXB2|reg|INT| TTTTTAGATGGAATAGATAAGGCCCAAGATGAACATGAGAAATATCACAGTAATTGGAGAGCAATGGCTAGTGATTTTAA CCTGCCACCTGTAGTAGCAAAAGAAATAGTAGCCAGCTGTGATAAATGTCAGCTAAAAGGAGAAGCCATGCATGGACAAG TAGACTGTAGTCCAGGAATATGGCAACTAGATTGTACACATTTAGAAGGAAAAGTTATCCTGGTAGCAGTTCATGTAGCC AGTGGATATATAGAAGCAGAAGTTATTCCAGCAGAAACAGGGCAGGAAACAGCATATTTTCTTTTAAAATTAGCAGGAAG ATGGCCAGTAAAAACAATACATACTGACAATGGCAGCAATTTCACCGGTGCTACGGTTAGGGCCGCCTGTTGGTGGGCGG GAATCAAGCAGGAATTTGGAATTCCCTACAATCCCCAAAGTCAAGGAGTAGTAGAATCTATGAATAAAGAATTAAAGAAA ATTATAGGACAGGTAAGAGATCAGGCTGAACATCTTAAGACAGCAGTACAAATGGCAGTATTCATCCACAATTTTAAAAG AAAAGGGGGGATTGGGGGGTACAGTGCAGGGGAAAGAATAGTAGACATAATAGCAACAGACATACAAACTAAAGAATTAC AAAAACAAATTACAAAAATTCAAAATTTTCGGGTTTATTACAGGGACAGCAGAAATCCACTTTGGAAAGGACCAGCAAAG CTCCTCTGGAAAGGTGAAGGGGCAGTAGTAATACAAGATAATAGTGACATAAAAGTAGTGCCAAGAAGAAAAGCAAAGAT CATTAGGGATTATGGAAAACAGATGGCAGGTGATGATTGTGTGGCAAGTAGACAGGATGAGGAT >ref|HIV_B.K03455.HXB2|reg|gp120| CAGTAGATCCTAGACTAGAGCCCTGGAAGCATCCAGGAAGTCAGCCTAAAACTGCTTGTACCAATTGCTATTGTAAAAAG TGTTGCTTTCATTGCCAAGTTTGTTTCATAACAAAAGCCTTAGGCATCTCCTATGGCAGGAAGAAGCGGAGACAGCGACG AAGAGCTCATCAGAACAGTCAGACTCATCAAGCTTCTCTATCAAAGCAGTAAGTAGTACATGTAACGCAACCTATACCAA TAGTAGCAATAGTAGCATTAGTAGTAGCAATAATAATAGCAATAGTTGTGTGGTCCATAGTAATCATAGAATATAGGAAA ATATTAAGACAAAGAAAAATAGACAGGTTAATTGATAGACTAATAGAAAGAGCAGAAGACAGTGGCAATGAGAGTGAAGG AGAAATATCAGCACTTGTGGAGATGGGGGTGGAGATGGGGCACCATGCTCCTTGGGATGTTGATGATCTGTAGTGCTACA GAAAAATTGTGGGTCACAGTCTATTATGGGGTACCTGTGTGGAAGGAAGCAACCACCACTCTATTTTGTGCATCAGATGC TAAAGCATATGATACAGAGGTACATAATGTTTGGGCCACACATGCCTGTGTACCCACAGACCCCAACCCACAAGAAGTAG TATTGGTAAATGTGACAGAAAATTTTAACATGTGGAAAAATGACATGGTAGAACAGATGCATGAGGATATAATCAGTTTA TGGGATCAAAGCCTAAAGCCATGTGTAAAATTAACCCCACTCTGTGTTAGTTTAAAGTGCACTGATTTGAAGAATGATAC TAATACCAATAGTAGTAGCGGGAGAATGATAATGGAGAAAGGAGAGATAAAAAACTGCTCTTTCAATATCAGCACAAGCA TAAGAGGTAAGGTGCAGAAAGAATATGCATTTTTTTATAAACTTGATATAATACCAATAGATAATGATACTACCAGCTAT AAGTTGACAAGTTGTAACACCTCAGTCATTACACAGGCCTGTCCAAAGGTATCCTTTGAGCCAATTCCCATACATTATTG TGCCCCGGCTGGTTTTGCGATTCTAAAATGTAATAATAAGACGTTCAATGGAACAGGACCATGTACAAATGTCAGCACAG TACAATGTACACATGGAATTAGGCCAGTAGTATCAACTCAACTGCTGTTAAATGGCAGTCTAGCAGAAGAAGAGGTAGTA ATTAGATCTGTCAATTTCACGGACAATGCTAAAACCATAATAGTACAGCTGAACACATCTGTAGAAATTAATTGTACAAG ACCCAACAACAATACAAGAAAAAGAATCCGTATCCAGAGAGGACCAGGGAGAGCATTTGTTACAATAGGAAAAATAGGAA ATATGAGACAAGCACATTGTAACATTAGTAGAGCAAAATGGAATAACACTTTAAAACAGATAGCTAGCAAATTAAGAGAA CAATTTGGAAATAATAAAACAATAATCTTTAAGCAATCCTCAGGAGGGGACCCAGAAATTGTAACGCACAGTTTTAATTG TGGAGGGGAATTTTTCTACTGTAATTCAACACAACTGTTTAATAGTACTTGGTTTAATAGTACTTGGAGTACTGAAGGGT CAAATAACACTGAAGGAAGTGACACAATCACCCTCCCATGCAGAATAAAACAAATTATAAACATGTGGCAGAAAGTAGGA AAAGCAATGTATGCCCCTCCCATCAGTGGACAAATTAGATGTTCATCAAATATTACAGGGCTGCTATTAACAAGAGATGG TGGTAATAGCAACAATGAGTCCGAGATCTTCAGACCTGGAGGAGGAGATATGAGGGACAATTGGAGAAGTGAATTATATA AATATAAAGTAGTAAAAATTGAACCATTAGGAGTAGCACCCACCAAGGCAAAGAGAAGAGTGGTGCAGAGAGAAAAAAGA HIV_B.K03455.HXB2.gtf HIV_B.K03455.HXB2 LANL amplicon 2252 3869 . + 2 name \"PRRT\"; primary_cds \"2252-2549\"; alt_cds \"2550-3869\"; HIV_B.K03455.HXB2 LANL amplicon 4230 5093 . + 0 name \"INT\"; primary_cds \"2085-5096\"; alt_cds \"5098-5619\"; HIV_B.K03455.HXB2 LANL amplicon 6225 7757 . + 1 name \"gp120\"; primary_cds \"6225-8795\"; alt_cds \"5831-6223\";","title":"Installation"},{"location":"install/#haphpipe-installation-instructions","text":"HAPHIPE depends on more than a dozen different programs, each of which may itself depend on other programs and libraries. Installing everything separately would be a nightmare, so you should use the package manager \"Bioconda\" to install the HAPHPIPE package. Bioconda is a popular package manager in the bioinformatics world. See the Helpful Resources section for more information and resources for Bioconda. Here, we will describe where to get Bioconda and how to install it, then how to use Bioconda to install HAPHPIPE. We will also detail the acquisition and installation of one program, GATK, that is not handled by Bioconda. Here, we describe the procedure for installing HAPHPIPE using the package manager Bioconda (Gr\u00fcning et al. 2018) on the command line. If you are unfamiliar with Bioconda, please see for installation and channel setup. The source code for HAPHPIPE is available here and is written in Python 3.7.2. The installation process begins with the creation of a conda environment that installs HAPHPIPE and its dependencies together with one short command. Once the conda environment has been created, it can be activated for use with the command conda activate haphpipe. Due to license restrictions, Bioconda cannot distribute and install GATK (McKenna et al. 2010; Van der Auwera et al. 2013; Poplin et al. 2018) directly. To fully install GATK, you must download a licensed copy of GATK (version 3.8-0) from the Broad Institute (link) . 1. Install conda Download the conda package: wget https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-x86_64.sh sh Miniconda3-latest-MacOSX-x86_64.sh 2. Set up conda channels These need to be put in as a command in the order they come, as this sets the priority for where packages are pulled from. Therefore, in this order, conda-forge is top priority and explored first when downloading a program. conda config --add channels R conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge 3. Create a conda environment for HAPHPIPE conda create -n haphpipe haphpipe This will install HAPHPIPE and all dependencies in Bioconda.Note: on some HPC systems (including GW's Pegasus), certain dependencies must be installed as a 'User Install'. If this is the case (which will be apparent if you receive availability errors for any of the above packages after installing them through conda) use the following command to install the package: pip install --user <package name> 4. Activate the environment conda activate haphpipe You can now test your HAPHPIPE install by running haphpipe -h . 5. Install GATK Due to license restrictions, bioconda cannot distribute and install GATK directly. To fully install GATK, you must download a licensed copy of GATK (version 3.8-0) from the Broad Institute and register the package using gatk3-register. gsutil has been included as a dependency in HAPHPIPE, so you do not need to worry about installing it for the GATK acquisition. This will copy GATK into your conda environment. GATK needs to be installed directly on the command line with the following commands: mkdir -p /path/to/gatk_dir gsutil cp gs://gatk-software/package-archive/gatk/GenomeAnalysisTK-3.8-0-ge9d806836.tar.bz2 path/to/gatk_dir/ gatk3-register /path/to/gatk_dir/GenomeAnalysisTK.jar NOTE: HAPHPIPE was developed and tested using GATK 3.8.","title":"HAPHPIPE Installation Instructions"},{"location":"install/#predicthaplo-installation-instructions","text":"Users are required to download PredictHaplo on their own computing system prior to running any of the haplotype modules ( predict_haplo and ph_parser ). Here is how the GW CBI team installed PredictHaplo onto our HPC, which has a slurm scheduling system and uses Lua module files. We cannot help with the installation of this software, but have provided the code that we used here to install PredictHaplo onto our system. Please see their website for contact information if needed. This module loads predicthaplo onto GWU's HPC - Colonial One. See https://bmda.dmi.unibas.ch/software.html cd /path/to/modules/predicthaplo # use gcc 4.9.4, add blas and lapack to library path module load gcc/4.9.4 module load blas/gcc/64 module load lapack/gcc/64 # download source cd archive wget https://bmda.dmi.unibas.ch/software/PredictHaplo-Paired-0.4.tgz cd .. # unzip source and change directory name tar xfvz archive/PredictHaplo-Paired-0.4.tgz mv PredictHaplo-Paired-0.4 0.4 cd 0.4 # install scythestat tar xfvz scythestat-1.0.3.tar.gz cd scythestat-1.0.3 ./configure --prefix=/path/to/modules/predicthaplo/0.4/NEWSCYTHE make install cd .. # compile predicthaplo make If a segfault error occurs during the hp_predict_haplo module, this is not a characteristic of HAPHPIPE but rather that of PredictHaplo. Sometimes, we have luck if we just rerun the code again or move to an interactive CPU node. We are unsure what causes this error, and we only see it between the local and global reconstruction phases in PredictHaplo.","title":"PredictHaplo Installation Instructions"},{"location":"install/#quick-start","text":"1. Activate haphpipe Make sure you have conda running. For students at GW using Colonial One, you need to load the miniconda3 module like such prior to activating the haphpipe conda environemnt: module load miniconda3 . conda activate haphpipe 2. Test that it is loaded correctly haphpipe -h should produce: Program: haphpipe (haplotype and phylodynamics pipeline) Version: 1.0.2 Commands: -- Reads sample_reads subsample reads using seqtk trim_reads trim reads using Trimmomatic join_reads join reads using FLASh ec_reads error correct reads using SPAdes -- Assemble assemble_denovo assemble reads denovo assemble_amplicons assemble contigs to amplicon regions assemble_scaffold assemble contigs to genome align_reads align reads to reference call_variants call variants vcf_to_consensus create consensus sequence from VCF refine_assembly iterative refinement: align - variants - consensus finalize_assembly finalize consensus sequence -- Haplotype predict_haplo assemble haplotypes with PredictHaplo ph_parser parse output from PredictHaplo cliquesnv assemble haplotypes with CliqueSNV -- Description pairwise_align align consensus to an annotated reference extract_pairwise extract sequence regions from pairwise alignment summary_stats generates summary statistics for samples -- Phylo multiple_align multiple sequence alignment model_test tests for model of evolution using ModelTest build_tree_NG builds phylogenetic tree with RAxML-NG -- Miscellaneous demo setup demo directory and test data","title":"Quick-Start"},{"location":"install/#directory-structure","text":"Below is the recommended directory structure for using HAPHPIPE: We recommend creating a separate directory for each sample, as well as a directory for reference files.","title":"Directory Structure"},{"location":"install/#windows-users","text":"HAPHPIPE is only available for Mac OSX or Linux platforms. We suggest the following options for running HAPHPIPE on a Windows machine: Run HAPHPIPE on your institution's HPC cluster, if available. Utilize the Windows Subsystem for Linux . Run Linux in a virtual machine via VirtualBox","title":"Windows Users"},{"location":"install/#reference-files","text":"Several modules in HAPHPIPE use reference files: either a FASTA file containing a reference sequence or a GTF file denoting genome regions for amplicon assembly. With HIV data, we use the file HIV_B.K03455.HXB2.fasta as a reference whole-genome file, HIV_B.K03455.HXB2.amplicons.fasta for amplicon assembly, and HIV_B.K03455.HXB2.gtf as a GTF file. All three files are downloaded in the demo module . The contents of each file are below. HIV_B.K03455.HXB2.fasta >HIV_B.K03455.HXB2 TGGAAGGGCTAATTCACTCCCAACGAAGACAAGATATCCTTGATCTGTGGATCTACCACACACAAGGCTACTTCCCTGATTAGCAGAACTACACACCAGG GCCAGGGATCAGATATCCACTGACCTTTGGATGGTGCTACAAGCTAGTACCAGTTGAGCCAGAGAAGTTAGAAGAAGCCAACAAAGGAGAGAACACCAGC TTGTTACACCCTGTGAGCCTGCATGGAATGGATGACCCGGAGAGAGAAGTGTTAGAGTGGAGGTTTGACAGCCGCCTAGCATTTCATCACATGGCCCGAG AGCTGCATCCGGAGTACTTCAAGAACTGCTGACATCGAGCTTGCTACAAGGGACTTTCCGCTGGGGACTTTCCAGGGAGGCGTGGCCTGGGCGGGACTGG GGAGTGGCGAGCCCTCAGATCCTGCATATAAGCAGCTGCTTTTTGCCTGTACTGGGTCTCTCTGGTTAGACCAGATCTGAGCCTGGGAGCTCTCTGGCTA ACTAGGGAACCCACTGCTTAAGCCTCAATAAAGCTTGCCTTGAGTGCTTCAAGTAGTGTGTGCCCGTCTGTTGTGTGACTCTGGTAACTAGAGATCCCTC AGACCCTTTTAGTCAGTGTGGAAAATCTCTAGCAGTGGCGCCCGAACAGGGACCTGAAAGCGAAAGGGAAACCAGAGGAGCTCTCTCGACGCAGGACTCG GCTTGCTGAAGCGCGCACGGCAAGAGGCGAGGGGCGGCGACTGGTGAGTACGCCAAAAATTTTGACTAGCGGAGGCTAGAAGGAGAGAGATGGGTGCGAG AGCGTCAGTATTAAGCGGGGGAGAATTAGATCGATGGGAAAAAATTCGGTTAAGGCCAGGGGGAAAGAAAAAATATAAATTAAAACATATAGTATGGGCA AGCAGGGAGCTAGAACGATTCGCAGTTAATCCTGGCCTGTTAGAAACATCAGAAGGCTGTAGACAAATACTGGGACAGCTACAACCATCCCTTCAGACAG GATCAGAAGAACTTAGATCATTATATAATACAGTAGCAACCCTCTATTGTGTGCATCAAAGGATAGAGATAAAAGACACCAAGGAAGCTTTAGACAAGAT AGAGGAAGAGCAAAACAAAAGTAAGAAAAAAGCACAGCAAGCAGCAGCTGACACAGGACACAGCAATCAGGTCAGCCAAAATTACCCTATAGTGCAGAAC ATCCAGGGGCAAATGGTACATCAGGCCATATCACCTAGAACTTTAAATGCATGGGTAAAAGTAGTAGAAGAGAAGGCTTTCAGCCCAGAAGTGATACCCA TGTTTTCAGCATTATCAGAAGGAGCCACCCCACAAGATTTAAACACCATGCTAAACACAGTGGGGGGACATCAAGCAGCCATGCAAATGTTAAAAGAGAC CATCAATGAGGAAGCTGCAGAATGGGATAGAGTGCATCCAGTGCATGCAGGGCCTATTGCACCAGGCCAGATGAGAGAACCAAGGGGAAGTGACATAGCA GGAACTACTAGTACCCTTCAGGAACAAATAGGATGGATGACAAATAATCCACCTATCCCAGTAGGAGAAATTTATAAAAGATGGATAATCCTGGGATTAA ATAAAATAGTAAGAATGTATAGCCCTACCAGCATTCTGGACATAAGACAAGGACCAAAGGAACCCTTTAGAGACTATGTAGACCGGTTCTATAAAACTCT AAGAGCCGAGCAAGCTTCACAGGAGGTAAAAAATTGGATGACAGAAACCTTGTTGGTCCAAAATGCGAACCCAGATTGTAAGACTATTTTAAAAGCATTG GGACCAGCGGCTACACTAGAAGAAATGATGACAGCATGTCAGGGAGTAGGAGGACCCGGCCATAAGGCAAGAGTTTTGGCTGAAGCAATGAGCCAAGTAA CAAATTCAGCTACCATAATGATGCAGAGAGGCAATTTTAGGAACCAAAGAAAGATTGTTAAGTGTTTCAATTGTGGCAAAGAAGGGCACACAGCCAGAAA TTGCAGGGCCCCTAGGAAAAAGGGCTGTTGGAAATGTGGAAAGGAAGGACACCAAATGAAAGATTGTACTGAGAGACAGGCTAATTTTTTAGGGAAGATC TGGCCTTCCTACAAGGGAAGGCCAGGGAATTTTCTTCAGAGCAGACCAGAGCCAACAGCCCCACCAGAAGAGAGCTTCAGGTCTGGGGTAGAGACAACAA CTCCCCCTCAGAAGCAGGAGCCGATAGACAAGGAACTGTATCCTTTAACTTCCCTCAGGTCACTCTTTGGCAACGACCCCTCGTCACAATAAAGATAGGG GGGCAACTAAAGGAAGCTCTATTAGATACAGGAGCAGATGATACAGTATTAGAAGAAATGAGTTTGCCAGGAAGATGGAAACCAAAAATGATAGGGGGAA TTGGAGGTTTTATCAAAGTAAGACAGTATGATCAGATACTCATAGAAATCTGTGGACATAAAGCTATAGGTACAGTATTAGTAGGACCTACACCTGTCAA CATAATTGGAAGAAATCTGTTGACTCAGATTGGTTGCACTTTAAATTTTCCCATTAGCCCTATTGAGACTGTACCAGTAAAATTAAAGCCAGGAATGGAT GGCCCAAAAGTTAAACAATGGCCATTGACAGAAGAAAAAATAAAAGCATTAGTAGAAATTTGTACAGAGATGGAAAAGGAAGGGAAAATTTCAAAAATTG GGCCTGAAAATCCATACAATACTCCAGTATTTGCCATAAAGAAAAAAGACAGTACTAAATGGAGAAAATTAGTAGATTTCAGAGAACTTAATAAGAGAAC TCAAGACTTCTGGGAAGTTCAATTAGGAATACCACATCCCGCAGGGTTAAAAAAGAAAAAATCAGTAACAGTACTGGATGTGGGTGATGCATATTTTTCA GTTCCCTTAGATGAAGACTTCAGGAAGTATACTGCATTTACCATACCTAGTATAAACAATGAGACACCAGGGATTAGATATCAGTACAATGTGCTTCCAC AGGGATGGAAAGGATCACCAGCAATATTCCAAAGTAGCATGACAAAAATCTTAGAGCCTTTTAGAAAACAAAATCCAGACATAGTTATCTATCAATACAT GGATGATTTGTATGTAGGATCTGACTTAGAAATAGGGCAGCATAGAACAAAAATAGAGGAGCTGAGACAACATCTGTTGAGGTGGGGACTTACCACACCA GACAAAAAACATCAGAAAGAACCTCCATTCCTTTGGATGGGTTATGAACTCCATCCTGATAAATGGACAGTACAGCCTATAGTGCTGCCAGAAAAAGACA GCTGGACTGTCAATGACATACAGAAGTTAGTGGGGAAATTGAATTGGGCAAGTCAGATTTACCCAGGGATTAAAGTAAGGCAATTATGTAAACTCCTTAG AGGAACCAAAGCACTAACAGAAGTAATACCACTAACAGAAGAAGCAGAGCTAGAACTGGCAGAAAACAGAGAGATTCTAAAAGAACCAGTACATGGAGTG TATTATGACCCATCAAAAGACTTAATAGCAGAAATACAGAAGCAGGGGCAAGGCCAATGGACATATCAAATTTATCAAGAGCCATTTAAAAATCTGAAAA CAGGAAAATATGCAAGAATGAGGGGTGCCCACACTAATGATGTAAAACAATTAACAGAGGCAGTGCAAAAAATAACCACAGAAAGCATAGTAATATGGGG AAAGACTCCTAAATTTAAACTGCCCATACAAAAGGAAACATGGGAAACATGGTGGACAGAGTATTGGCAAGCCACCTGGATTCCTGAGTGGGAGTTTGTT AATACCCCTCCCTTAGTGAAATTATGGTACCAGTTAGAGAAAGAACCCATAGTAGGAGCAGAAACCTTCTATGTAGATGGGGCAGCTAACAGGGAGACTA AATTAGGAAAAGCAGGATATGTTACTAATAGAGGAAGACAAAAAGTTGTCACCCTAACTGACACAACAAATCAGAAGACTGAGTTACAAGCAATTTATCT AGCTTTGCAGGATTCGGGATTAGAAGTAAACATAGTAACAGACTCACAATATGCATTAGGAATCATTCAAGCACAACCAGATCAAAGTGAATCAGAGTTA GTCAATCAAATAATAGAGCAGTTAATAAAAAAGGAAAAGGTCTATCTGGCATGGGTACCAGCACACAAAGGAATTGGAGGAAATGAACAAGTAGATAAAT TAGTCAGTGCTGGAATCAGGAAAGTACTATTTTTAGATGGAATAGATAAGGCCCAAGATGAACATGAGAAATATCACAGTAATTGGAGAGCAATGGCTAG TGATTTTAACCTGCCACCTGTAGTAGCAAAAGAAATAGTAGCCAGCTGTGATAAATGTCAGCTAAAAGGAGAAGCCATGCATGGACAAGTAGACTGTAGT CCAGGAATATGGCAACTAGATTGTACACATTTAGAAGGAAAAGTTATCCTGGTAGCAGTTCATGTAGCCAGTGGATATATAGAAGCAGAAGTTATTCCAG CAGAAACAGGGCAGGAAACAGCATATTTTCTTTTAAAATTAGCAGGAAGATGGCCAGTAAAAACAATACATACTGACAATGGCAGCAATTTCACCGGTGC TACGGTTAGGGCCGCCTGTTGGTGGGCGGGAATCAAGCAGGAATTTGGAATTCCCTACAATCCCCAAAGTCAAGGAGTAGTAGAATCTATGAATAAAGAA TTAAAGAAAATTATAGGACAGGTAAGAGATCAGGCTGAACATCTTAAGACAGCAGTACAAATGGCAGTATTCATCCACAATTTTAAAAGAAAAGGGGGGA TTGGGGGGTACAGTGCAGGGGAAAGAATAGTAGACATAATAGCAACAGACATACAAACTAAAGAATTACAAAAACAAATTACAAAAATTCAAAATTTTCG GGTTTATTACAGGGACAGCAGAAATCCACTTTGGAAAGGACCAGCAAAGCTCCTCTGGAAAGGTGAAGGGGCAGTAGTAATACAAGATAATAGTGACATA AAAGTAGTGCCAAGAAGAAAAGCAAAGATCATTAGGGATTATGGAAAACAGATGGCAGGTGATGATTGTGTGGCAAGTAGACAGGATGAGGATTAGAACA TGGAAAAGTTTAGTAAAACACCATATGTATGTTTCAGGGAAAGCTAGGGGATGGTTTTATAGACATCACTATGAAAGCCCTCATCCAAGAATAAGTTCAG AAGTACACATCCCACTAGGGGATGCTAGATTGGTAATAACAACATATTGGGGTCTGCATACAGGAGAAAGAGACTGGCATTTGGGTCAGGGAGTCTCCAT AGAATGGAGGAAAAAGAGATATAGCACACAAGTAGACCCTGAACTAGCAGACCAACTAATTCATCTGTATTACTTTGACTGTTTTTCAGACTCTGCTATA AGAAAGGCCTTATTAGGACACATAGTTAGCCCTAGGTGTGAATATCAAGCAGGACATAACAAGGTAGGATCTCTACAATACTTGGCACTAGCAGCATTAA TAACACCAAAAAAGATAAAGCCACCTTTGCCTAGTGTTACGAAACTGACAGAGGATAGATGGAACAAGCCCCAGAAGACCAAGGGCCACAGAGGGAGCCA CACAATGAATGGACACTAGAGCTTTTAGAGGAGCTTAAGAATGAAGCTGTTAGACATTTTCCTAGGATTTGGCTCCATGGCTTAGGGCAACATATCTATG AAACTTATGGGGATACTTGGGCAGGAGTGGAAGCCATAATAAGAATTCTGCAACAACTGCTGTTTATCCATTTTCAGAATTGGGTGTCGACATAGCAGAA TAGGCGTTACTCGACAGAGGAGAGCAAGAAATGGAGCCAGTAGATCCTAGACTAGAGCCCTGGAAGCATCCAGGAAGTCAGCCTAAAACTGCTTGTACCA ATTGCTATTGTAAAAAGTGTTGCTTTCATTGCCAAGTTTGTTTCATAACAAAAGCCTTAGGCATCTCCTATGGCAGGAAGAAGCGGAGACAGCGACGAAG AGCTCATCAGAACAGTCAGACTCATCAAGCTTCTCTATCAAAGCAGTAAGTAGTACATGTAACGCAACCTATACCAATAGTAGCAATAGTAGCATTAGTA GTAGCAATAATAATAGCAATAGTTGTGTGGTCCATAGTAATCATAGAATATAGGAAAATATTAAGACAAAGAAAAATAGACAGGTTAATTGATAGACTAA TAGAAAGAGCAGAAGACAGTGGCAATGAGAGTGAAGGAGAAATATCAGCACTTGTGGAGATGGGGGTGGAGATGGGGCACCATGCTCCTTGGGATGTTGA TGATCTGTAGTGCTACAGAAAAATTGTGGGTCACAGTCTATTATGGGGTACCTGTGTGGAAGGAAGCAACCACCACTCTATTTTGTGCATCAGATGCTAA AGCATATGATACAGAGGTACATAATGTTTGGGCCACACATGCCTGTGTACCCACAGACCCCAACCCACAAGAAGTAGTATTGGTAAATGTGACAGAAAAT TTTAACATGTGGAAAAATGACATGGTAGAACAGATGCATGAGGATATAATCAGTTTATGGGATCAAAGCCTAAAGCCATGTGTAAAATTAACCCCACTCT GTGTTAGTTTAAAGTGCACTGATTTGAAGAATGATACTAATACCAATAGTAGTAGCGGGAGAATGATAATGGAGAAAGGAGAGATAAAAAACTGCTCTTT CAATATCAGCACAAGCATAAGAGGTAAGGTGCAGAAAGAATATGCATTTTTTTATAAACTTGATATAATACCAATAGATAATGATACTACCAGCTATAAG TTGACAAGTTGTAACACCTCAGTCATTACACAGGCCTGTCCAAAGGTATCCTTTGAGCCAATTCCCATACATTATTGTGCCCCGGCTGGTTTTGCGATTC TAAAATGTAATAATAAGACGTTCAATGGAACAGGACCATGTACAAATGTCAGCACAGTACAATGTACACATGGAATTAGGCCAGTAGTATCAACTCAACT GCTGTTAAATGGCAGTCTAGCAGAAGAAGAGGTAGTAATTAGATCTGTCAATTTCACGGACAATGCTAAAACCATAATAGTACAGCTGAACACATCTGTA GAAATTAATTGTACAAGACCCAACAACAATACAAGAAAAAGAATCCGTATCCAGAGAGGACCAGGGAGAGCATTTGTTACAATAGGAAAAATAGGAAATA TGAGACAAGCACATTGTAACATTAGTAGAGCAAAATGGAATAACACTTTAAAACAGATAGCTAGCAAATTAAGAGAACAATTTGGAAATAATAAAACAAT AATCTTTAAGCAATCCTCAGGAGGGGACCCAGAAATTGTAACGCACAGTTTTAATTGTGGAGGGGAATTTTTCTACTGTAATTCAACACAACTGTTTAAT AGTACTTGGTTTAATAGTACTTGGAGTACTGAAGGGTCAAATAACACTGAAGGAAGTGACACAATCACCCTCCCATGCAGAATAAAACAAATTATAAACA TGTGGCAGAAAGTAGGAAAAGCAATGTATGCCCCTCCCATCAGTGGACAAATTAGATGTTCATCAAATATTACAGGGCTGCTATTAACAAGAGATGGTGG TAATAGCAACAATGAGTCCGAGATCTTCAGACCTGGAGGAGGAGATATGAGGGACAATTGGAGAAGTGAATTATATAAATATAAAGTAGTAAAAATTGAA CCATTAGGAGTAGCACCCACCAAGGCAAAGAGAAGAGTGGTGCAGAGAGAAAAAAGAGCAGTGGGAATAGGAGCTTTGTTCCTTGGGTTCTTGGGAGCAG CAGGAAGCACTATGGGCGCAGCCTCAATGACGCTGACGGTACAGGCCAGACAATTATTGTCTGGTATAGTGCAGCAGCAGAACAATTTGCTGAGGGCTAT TGAGGCGCAACAGCATCTGTTGCAACTCACAGTCTGGGGCATCAAGCAGCTCCAGGCAAGAATCCTGGCTGTGGAAAGATACCTAAAGGATCAACAGCTC CTGGGGATTTGGGGTTGCTCTGGAAAACTCATTTGCACCACTGCTGTGCCTTGGAATGCTAGTTGGAGTAATAAATCTCTGGAACAGATTTGGAATCACA CGACCTGGATGGAGTGGGACAGAGAAATTAACAATTACACAAGCTTAATACACTCCTTAATTGAAGAATCGCAAAACCAGCAAGAAAAGAATGAACAAGA ATTATTGGAATTAGATAAATGGGCAAGTTTGTGGAATTGGTTTAACATAACAAATTGGCTGTGGTATATAAAATTATTCATAATGATAGTAGGAGGCTTG GTAGGTTTAAGAATAGTTTTTGCTGTACTTTCTATAGTGAATAGAGTTAGGCAGGGATATTCACCATTATCGTTTCAGACCCACCTCCCAACCCCGAGGG GACCCGACAGGCCCGAAGGAATAGAAGAAGAAGGTGGAGAGAGAGACAGAGACAGATCCATTCGATTAGTGAACGGATCCTTGGCACTTATCTGGGACGA TCTGCGGAGCCTGTGCCTCTTCAGCTACCACCGCTTGAGAGACTTACTCTTGATTGTAACGAGGATTGTGGAACTTCTGGGACGCAGGGGGTGGGAAGCC CTCAAATATTGGTGGAATCTCCTACAGTATTGGAGTCAGGAACTAAAGAATAGTGCTGTTAGCTTGCTCAATGCCACAGCCATAGCAGTAGCTGAGGGGA CAGATAGGGTTATAGAAGTAGTACAAGGAGCTTGTAGAGCTATTCGCCACATACCTAGAAGAATAAGACAGGGCTTGGAAAGGATTTTGCTATAAGATGG GTGGCAAGTGGTCAAAAAGTAGTGTGATTGGATGGCCTACTGTAAGGGAAAGAATGAGACGAGCTGAGCCAGCAGCAGATAGGGTGGGAGCAGCATCTCG AGACCTGGAAAAACATGGAGCAATCACAAGTAGCAATACAGCAGCTACCAATGCTGCTTGTGCCTGGCTAGAAGCACAAGAGGAGGAGGAGGTGGGTTTT CCAGTCACACCTCAGGTACCTTTAAGACCAATGACTTACAAGGCAGCTGTAGATCTTAGCCACTTTTTAAAAGAAAAGGGGGGACTGGAAGGGCTAATTC ACTCCCAAAGAAGACAAGATATCCTTGATCTGTGGATCTACCACACACAAGGCTACTTCCCTGATTAGCAGAACTACACACCAGGGCCAGGGGTCAGATA TCCACTGACCTTTGGATGGTGCTACAAGCTAGTACCAGTTGAGCCAGATAAGATAGAAGAGGCCAATAAAGGAGAGAACACCAGCTTGTTACACCCTGTG AGCCTGCATGGGATGGATGACCCGGAGAGAGAAGTGTTAGAGTGGAGGTTTGACAGCCGCCTAGCATTTCATCACGTGGCCCGAGAGCTGCATCCGGAGT ACTTCAAGAACTGCTGACATCGAGCTTGCTACAAGGGACTTTCCGCTGGGGACTTTCCAGGGAGGCGTGGCCTGGGCGGGACTGGGGAGTGGCGAGCCCT CAGATCCTGCATATAAGCAGCTGCTTTTTGCCTGTACTGGGTCTCTCTGGTTAGACCAGATCTGAGCCTGGGAGCTCTCTGGCTAACTAGGGAACCCACT GCTTAAGCCTCAATAAAGCTTGCCTTGAGTGCTTCAAGTAGTGTGTGCCCGTCTGTTGTGTGACTCTGGTAACTAGAGATCCCTCAGACCCTTTTAGTCA GTGTGGAAAATCTCTAGCA HIV_B.K03455.HXB2.amplicons.fasta >ref|HIV_B.K03455.HXB2|reg|PRRT| CCCTCAGGTCACTCTTTGGCAACGACCCCTCGTCACAATAAAGATAGGGGGGCAACTAAAGGAAGCTCTATTAGATACAG GAGCAGATGATACAGTATTAGAAGAAATGAGTTTGCCAGGAAGATGGAAACCAAAAATGATAGGGGGAATTGGAGGTTTT ATCAAAGTAAGACAGTATGATCAGATACTCATAGAAATCTGTGGACATAAAGCTATAGGTACAGTATTAGTAGGACCTAC ACCTGTCAACATAATTGGAAGAAATCTGTTGACTCAGATTGGTTGCACTTTAAATTTTCCCATTAGCCCTATTGAGACTG TACCAGTAAAATTAAAGCCAGGAATGGATGGCCCAAAAGTTAAACAATGGCCATTGACAGAAGAAAAAATAAAAGCATTA GTAGAAATTTGTACAGAGATGGAAAAGGAAGGGAAAATTTCAAAAATTGGGCCTGAAAATCCATACAATACTCCAGTATT TGCCATAAAGAAAAAAGACAGTACTAAATGGAGAAAATTAGTAGATTTCAGAGAACTTAATAAGAGAACTCAAGACTTCT GGGAAGTTCAATTAGGAATACCACATCCCGCAGGGTTAAAAAAGAAAAAATCAGTAACAGTACTGGATGTGGGTGATGCA TATTTTTCAGTTCCCTTAGATGAAGACTTCAGGAAGTATACTGCATTTACCATACCTAGTATAAACAATGAGACACCAGG GATTAGATATCAGTACAATGTGCTTCCACAGGGATGGAAAGGATCACCAGCAATATTCCAAAGTAGCATGACAAAAATCT TAGAGCCTTTTAGAAAACAAAATCCAGACATAGTTATCTATCAATACATGGATGATTTGTATGTAGGATCTGACTTAGAA ATAGGGCAGCATAGAACAAAAATAGAGGAGCTGAGACAACATCTGTTGAGGTGGGGACTTACCACACCAGACAAAAAACA TCAGAAAGAACCTCCATTCCTTTGGATGGGTTATGAACTCCATCCTGATAAATGGACAGTACAGCCTATAGTGCTGCCAG AAAAAGACAGCTGGACTGTCAATGACATACAGAAGTTAGTGGGGAAATTGAATTGGGCAAGTCAGATTTACCCAGGGATT AAAGTAAGGCAATTATGTAAACTCCTTAGAGGAACCAAAGCACTAACAGAAGTAATACCACTAACAGAAGAAGCAGAGCT AGAACTGGCAGAAAACAGAGAGATTCTAAAAGAACCAGTACATGGAGTGTATTATGACCCATCAAAAGACTTAATAGCAG AAATACAGAAGCAGGGGCAAGGCCAATGGACATATCAAATTTATCAAGAGCCATTTAAAAATCTGAAAACAGGAAAATAT GCAAGAATGAGGGGTGCCCACACTAATGATGTAAAACAATTAACAGAGGCAGTGCAAAAAATAACCACAGAAAGCATAGT AATATGGGGAAAGACTCCTAAATTTAAACTGCCCATACAAAAGGAAACATGGGAAACATGGTGGACAGAGTATTGGCAAG CCACCTGGATTCCTGAGTGGGAGTTTGTTAATACCCCTCCCTTAGTGAAATTATGGTACCAGTTAGAGAAAGAACCCATA GTAGGAGCAGAAACCTTC >ref|HIV_B.K03455.HXB2|reg|INT| TTTTTAGATGGAATAGATAAGGCCCAAGATGAACATGAGAAATATCACAGTAATTGGAGAGCAATGGCTAGTGATTTTAA CCTGCCACCTGTAGTAGCAAAAGAAATAGTAGCCAGCTGTGATAAATGTCAGCTAAAAGGAGAAGCCATGCATGGACAAG TAGACTGTAGTCCAGGAATATGGCAACTAGATTGTACACATTTAGAAGGAAAAGTTATCCTGGTAGCAGTTCATGTAGCC AGTGGATATATAGAAGCAGAAGTTATTCCAGCAGAAACAGGGCAGGAAACAGCATATTTTCTTTTAAAATTAGCAGGAAG ATGGCCAGTAAAAACAATACATACTGACAATGGCAGCAATTTCACCGGTGCTACGGTTAGGGCCGCCTGTTGGTGGGCGG GAATCAAGCAGGAATTTGGAATTCCCTACAATCCCCAAAGTCAAGGAGTAGTAGAATCTATGAATAAAGAATTAAAGAAA ATTATAGGACAGGTAAGAGATCAGGCTGAACATCTTAAGACAGCAGTACAAATGGCAGTATTCATCCACAATTTTAAAAG AAAAGGGGGGATTGGGGGGTACAGTGCAGGGGAAAGAATAGTAGACATAATAGCAACAGACATACAAACTAAAGAATTAC AAAAACAAATTACAAAAATTCAAAATTTTCGGGTTTATTACAGGGACAGCAGAAATCCACTTTGGAAAGGACCAGCAAAG CTCCTCTGGAAAGGTGAAGGGGCAGTAGTAATACAAGATAATAGTGACATAAAAGTAGTGCCAAGAAGAAAAGCAAAGAT CATTAGGGATTATGGAAAACAGATGGCAGGTGATGATTGTGTGGCAAGTAGACAGGATGAGGAT >ref|HIV_B.K03455.HXB2|reg|gp120| CAGTAGATCCTAGACTAGAGCCCTGGAAGCATCCAGGAAGTCAGCCTAAAACTGCTTGTACCAATTGCTATTGTAAAAAG TGTTGCTTTCATTGCCAAGTTTGTTTCATAACAAAAGCCTTAGGCATCTCCTATGGCAGGAAGAAGCGGAGACAGCGACG AAGAGCTCATCAGAACAGTCAGACTCATCAAGCTTCTCTATCAAAGCAGTAAGTAGTACATGTAACGCAACCTATACCAA TAGTAGCAATAGTAGCATTAGTAGTAGCAATAATAATAGCAATAGTTGTGTGGTCCATAGTAATCATAGAATATAGGAAA ATATTAAGACAAAGAAAAATAGACAGGTTAATTGATAGACTAATAGAAAGAGCAGAAGACAGTGGCAATGAGAGTGAAGG AGAAATATCAGCACTTGTGGAGATGGGGGTGGAGATGGGGCACCATGCTCCTTGGGATGTTGATGATCTGTAGTGCTACA GAAAAATTGTGGGTCACAGTCTATTATGGGGTACCTGTGTGGAAGGAAGCAACCACCACTCTATTTTGTGCATCAGATGC TAAAGCATATGATACAGAGGTACATAATGTTTGGGCCACACATGCCTGTGTACCCACAGACCCCAACCCACAAGAAGTAG TATTGGTAAATGTGACAGAAAATTTTAACATGTGGAAAAATGACATGGTAGAACAGATGCATGAGGATATAATCAGTTTA TGGGATCAAAGCCTAAAGCCATGTGTAAAATTAACCCCACTCTGTGTTAGTTTAAAGTGCACTGATTTGAAGAATGATAC TAATACCAATAGTAGTAGCGGGAGAATGATAATGGAGAAAGGAGAGATAAAAAACTGCTCTTTCAATATCAGCACAAGCA TAAGAGGTAAGGTGCAGAAAGAATATGCATTTTTTTATAAACTTGATATAATACCAATAGATAATGATACTACCAGCTAT AAGTTGACAAGTTGTAACACCTCAGTCATTACACAGGCCTGTCCAAAGGTATCCTTTGAGCCAATTCCCATACATTATTG TGCCCCGGCTGGTTTTGCGATTCTAAAATGTAATAATAAGACGTTCAATGGAACAGGACCATGTACAAATGTCAGCACAG TACAATGTACACATGGAATTAGGCCAGTAGTATCAACTCAACTGCTGTTAAATGGCAGTCTAGCAGAAGAAGAGGTAGTA ATTAGATCTGTCAATTTCACGGACAATGCTAAAACCATAATAGTACAGCTGAACACATCTGTAGAAATTAATTGTACAAG ACCCAACAACAATACAAGAAAAAGAATCCGTATCCAGAGAGGACCAGGGAGAGCATTTGTTACAATAGGAAAAATAGGAA ATATGAGACAAGCACATTGTAACATTAGTAGAGCAAAATGGAATAACACTTTAAAACAGATAGCTAGCAAATTAAGAGAA CAATTTGGAAATAATAAAACAATAATCTTTAAGCAATCCTCAGGAGGGGACCCAGAAATTGTAACGCACAGTTTTAATTG TGGAGGGGAATTTTTCTACTGTAATTCAACACAACTGTTTAATAGTACTTGGTTTAATAGTACTTGGAGTACTGAAGGGT CAAATAACACTGAAGGAAGTGACACAATCACCCTCCCATGCAGAATAAAACAAATTATAAACATGTGGCAGAAAGTAGGA AAAGCAATGTATGCCCCTCCCATCAGTGGACAAATTAGATGTTCATCAAATATTACAGGGCTGCTATTAACAAGAGATGG TGGTAATAGCAACAATGAGTCCGAGATCTTCAGACCTGGAGGAGGAGATATGAGGGACAATTGGAGAAGTGAATTATATA AATATAAAGTAGTAAAAATTGAACCATTAGGAGTAGCACCCACCAAGGCAAAGAGAAGAGTGGTGCAGAGAGAAAAAAGA HIV_B.K03455.HXB2.gtf HIV_B.K03455.HXB2 LANL amplicon 2252 3869 . + 2 name \"PRRT\"; primary_cds \"2252-2549\"; alt_cds \"2550-3869\"; HIV_B.K03455.HXB2 LANL amplicon 4230 5093 . + 0 name \"INT\"; primary_cds \"2085-5096\"; alt_cds \"5098-5619\"; HIV_B.K03455.HXB2 LANL amplicon 6225 7757 . + 1 name \"gp120\"; primary_cds \"6225-8795\"; alt_cds \"5831-6223\";","title":"Reference Files"},{"location":"phylo/","text":"This stage includes phylogenomics stages. Phylo Quick-Start HAPHPIPE includes three modules for phylogenomics: multiple_align , model_test , and build_tree . These three modules are sufficient to turn your consensus and/or haplotype sequences from the other modules into a phylogenetic tree! For purposes of this quick-start guide, we will demonstrate the modules to create a tree from HIV pol consensus sequences. Step 1: Alignment After running either of the assembly pipelines, final.fna files will be located in directories named ./<SampleID>/haphpipe_assemble_0[1|2] . For the multiple_align module, we need to create a list of all of these directories. We can do so easily with one command (shown for haphpipe_assemble_01 output: ls -d ./SRR*/haphpipe_assemble_01 > ./dir_list.txt Now, we will align all of these final.fna files: haphpipe multiple_align --dir_list dir_list.txt --ref_gtf refs/HIV_B.K03455.HXB2.gtf The output will be located in a new directory, hp_multiple_align . The alignment of pol sequences is the file alignment_region00.fasta . Step 2: Model Selection Now, we will use the model_test module to determine the best-fit evolutionary model for our data. This is an input to the tree building module. We will use this command to generate best-fit models available in RAxML: haphpipe model_test --seqs hp_multiple_align/alignment_region00.fasta --run_id alignment_region00 --template raxml The ModelTest output will be written to a file called modeltest_results.out and a summary of all the best models will be written to modeltest_results_summary.tsv . Examples of both are below. ModelTest-NG Output -------------------------------------------------------------------------------- ModelTest-NG vx.y.z Input data: MSA: multiple_align/alignment.fasta Tree: Maximum likelihood file: - #taxa: 6 #sites: 1975 #patterns: 114 Max. thread mem: 0 MB Output: Log: /var/folders/lv/dqdkd8957_3fv6yxsyfsvn0r0000gn/T/tmpHP_model_testud4sc4ya/samp12_modeltest_results.log Starting tree: /var/folders/lv/dqdkd8957_3fv6yxsyfsvn0r0000gn/T/tmpHP_model_testud4sc4ya/samp12_modeltest_results.tree Results: /var/folders/lv/dqdkd8957_3fv6yxsyfsvn0r0000gn/T/tmpHP_model_testud4sc4ya/samp12_modeltest_results.out Selection options: # dna schemes: 11 # dna models: 88 include model parameters: Uniform: true p-inv (+I): true gamma (+G): true both (+I+G): true free rates (+R): false fixed freqs: true estimated freqs: true #categories: 4 gamma rates mode: mean asc bias: none epsilon (opt): 0.01 epsilon (par): 0.05 keep branches: false Additional options: verbosity: very low threads: 1/6 RNG seed: 12345 subtree repeats: enabled -------------------------------------------------------------------------------- BIC model K lnL score delta weight -------------------------------------------------------------------------------- 1 HKY 4 -5380.7535 10860.1551 0.0000 0.7049 2 TrN 5 -5378.2013 10862.6391 2.4839 0.2036 3 TPM1uf 5 -5379.8699 10865.9763 5.8211 0.0384 4 TPM3uf 5 -5380.7175 10867.6716 7.5164 0.0164 5 HKY+G4 5 -5380.8440 10867.9246 7.7694 0.0145 6 HKY+I 5 -5381.4917 10869.2200 9.0648 0.0076 7 TIM3 6 -5378.1637 10870.1523 9.9971 0.0048 8 TrN+G4 6 -5378.3069 10870.4387 10.2836 0.0041 9 TPM2uf+G4 6 -5378.9908 10871.8064 11.6512 0.0021 10 TrN+I 6 -5379.0181 10871.8610 11.7059 0.0020 -------------------------------------------------------------------------------- Best model according to BIC --------------------------- Model: HKY lnL: -5380.7535 Frequencies: 0.3695 0.1709 0.2219 0.2377 Subst. Rates: 1.0000 2.4741 1.0000 1.0000 2.4741 1.0000 Inv. sites prop: - Gamma shape: - Score: 10860.1551 Weight: 0.7049 --------------------------- Parameter importances --------------------------- P.Inv: 0.0100 Gamma: 0.0216 Gamma-Inv: 0.0002 Frequencies: 1.0000 --------------------------- Model averaged estimates --------------------------- P.Inv: 0.0215 Alpha: 94.2337 Alpha-P.Inv: 91.7960 P.Inv-Alpha: 0.0214 Frequencies: 0.3700 0.1702 0.2226 0.2372 Commands: > phyml -i multiple_align/alignment.fasta -m 010010 -f m -v 0 -a 0 -c 1 -o tlr > raxmlHPC-SSE3 -s multiple_align/alignment.fasta -c 1 -m GTRCATX -n EXEC_NAME -p PARSIMONY_SEED > raxml-ng --msa multiple_align/alignment.fasta --model HKY > paup -s multiple_align/alignment.fasta > iqtree -s multiple_align/alignment.fasta -m HKY AIC model K lnL score delta weight -------------------------------------------------------------------------------- 1 TrN 5 -5378.2013 10784.4026 0.0000 0.2246 2 TIM2+G4 7 -5376.4972 10784.9944 0.5919 0.1671 3 TIM3 6 -5378.1637 10786.3274 1.9249 0.0858 4 TIM2+I 7 -5377.2570 10786.5141 2.1115 0.0782 5 TrN+G4 6 -5378.3069 10786.6138 2.2113 0.0744 6 TIM1+G4 7 -5377.3549 10786.7098 2.3073 0.0709 7 HKY 4 -5380.7535 10787.5069 3.1044 0.0476 8 TPM1uf 5 -5379.8699 10787.7398 3.3372 0.0423 9 TPM2uf+G4 6 -5378.9908 10787.9815 3.5790 0.0375 10 TrN+I 6 -5379.0181 10788.0362 3.6336 0.0365 -------------------------------------------------------------------------------- Best model according to AIC --------------------------- Model: TrN lnL: -5378.2013 Frequencies: 0.3720 0.1679 0.2249 0.2352 Subst. Rates: 1.0000 2.2008 1.0000 1.0000 3.1065 1.0000 Inv. sites prop: - Gamma shape: - Score: 10784.4026 Weight: 0.2246 --------------------------- Parameter importances --------------------------- P.Inv: 0.1262 Gamma: 0.3943 Gamma-Inv: 0.0384 Frequencies: 1.0000 --------------------------- Model averaged estimates --------------------------- P.Inv: 0.0216 Alpha: 93.2595 Alpha-P.Inv: 94.4193 P.Inv-Alpha: 0.0216 Frequencies: 0.3718 0.1684 0.2238 0.2359 Commands: > phyml -i multiple_align/alignment.fasta -m 010020 -f m -v 0 -a 0 -c 1 -o tlr > raxmlHPC-SSE3 -s multiple_align/alignment.fasta -c 1 -m GTRCATX -n EXEC_NAME -p PARSIMONY_SEED > raxml-ng --msa multiple_align/alignment.fasta --model TrN > paup -s multiple_align/alignment.fasta > iqtree -s multiple_align/alignment.fasta -m TrN AICc model K lnL score delta weight -------------------------------------------------------------------------------- 1 TrN 5 -5378.2013 10784.4026 0.0000 0.2246 2 TIM2+G4 7 -5376.4972 10784.9944 0.5919 0.1671 3 TIM3 6 -5378.1637 10786.3274 1.9249 0.0858 4 TIM2+I 7 -5377.2570 10786.5141 2.1115 0.0782 5 TrN+G4 6 -5378.3069 10786.6138 2.2113 0.0744 6 TIM1+G4 7 -5377.3549 10786.7098 2.3073 0.0709 7 HKY 4 -5380.7535 10787.5069 3.1044 0.0476 8 TPM1uf 5 -5379.8699 10787.7398 3.3372 0.0423 9 TPM2uf+G4 6 -5378.9908 10787.9815 3.5790 0.0375 10 TrN+I 6 -5379.0181 10788.0362 3.6336 0.0365 -------------------------------------------------------------------------------- Best model according to AICc --------------------------- Model: TrN lnL: -5378.2013 Frequencies: 0.3720 0.1679 0.2249 0.2352 Subst. Rates: 1.0000 2.2008 1.0000 1.0000 3.1065 1.0000 Inv. sites prop: - Gamma shape: - Score: 10784.4026 Weight: 0.2246 --------------------------- Parameter importances --------------------------- P.Inv: 0.1262 Gamma: 0.3943 Gamma-Inv: 0.0384 Frequencies: 1.0000 --------------------------- Model averaged estimates --------------------------- P.Inv: 0.0216 Alpha: 93.2595 Alpha-P.Inv: 94.4193 P.Inv-Alpha: 0.0216 Frequencies: 0.3718 0.1684 0.2238 0.2359 Commands: > phyml -i multiple_align/alignment.fasta -m 010020 -f m -v 0 -a 0 -c 1 -o tlr > raxmlHPC-SSE3 -s multiple_align/alignment.fasta -c 1 -m GTRCATX -n EXEC_NAME -p PARSIMONY_SEED > raxml-ng --msa multiple_align/alignment.fasta --model TrN > paup -s multiple_align/alignment.fasta > iqtree -s multiple_align/alignment.fasta -m TrN Done ModelTest-NG Output Summary File Criteria Best Model hp_multiple_align/alignment.fasta BIC HKY hp_multiple_align/alignment.fasta AIC TrN hp_multiple_align/alignment.fasta AICc TrN Step 3: Build a Tree Now, we will use build_tree_NG to build our tree! You should use the best model outputted in model_test for the --model argument (here we are using GTR). The --all option will automatically run a full maximum likelihood & bootstrapping analysis for us: haphpipe build_tree_NG --seqs hp_multiple_align/alignment_region00.fasta --all --model GTR The output will be written to a new directory, hp_build_tree . The best tree file from RAxML will be outputted as hp_tree.raxml.support . This tree can then be annotated in programs such as FigTree or iTOL . Phylogenomics Pipelines For users who would like to build a full pipeline to run assembly and phylogenetics stages in one go, we recommend adapting the demo pipeline ( haphpipe_demo ) for this purpose. See the demo page for more details. multiple_align Align consensus sequences using MAFFT ( documentation ). Input can be a list of directories which contain final.fna and/or ph_haplotypes.fna files or a fasta file, or both (in which case the sequences in the FASTA file are combined with the final.fna and/or ph_haplotypes.fna files retreived before the alignment. Sequences will be separated by amplicons using a supplied GTF file before alignment (unless the --alignall option is specified). This module may also be used to separate files by amplicons (without aligning) by specifying the --fastaonly option. Alignments are by default outputted as FASTA files, although PHYLIP ( --phylipout ) or CLUSTAL ( --clustalout ) output options are also available. Many options from MAFFT are available in this module. Please refer to the MAFFT documentation above for information about these options. Usage: haphpipe multiple_align [MAFFT OPTIONS] [HAPHPIPE OPTIONS] --seqs <FASTA> --dir_list <TXT> --ref_gtf <GTF> [--outdir] (or): hp_multiple_align [MAFFT OPTIONS] [HAPHPIPE OPTIONS] --seqs <FASTA> --dir_list <TXT> --ref_gtf <GTF> [--outdir] Output files: alignment files in FASTA format (default), one per amplicon (or one alignment.fasta file if using --alignall option) Note: MAFFT stores intermediate files in a temporary directory located in /tmp. More information is available here . Input/Output Arguments: Option Description --seqs SEQS FASTA file with sequences to be aligned --dir_list DIR_LIST List of directories which include either a final.fna or ph_haplotypes.fna file, one on each line --ref_gtf REF_GTF Reference GTF file --out_align OUT_ALIGN Name for alignment file --nuc Assume nucleotide (default: False) --amino Assume amino (default: False) --clustalout Clustal output format (default: False) --phylipout PHYLIP output format (default: False) --inputorder Output order same as input (default: False) --reorder Output order aligned (default: False) --treeout Guide tree is output to the input.tree file (default:False) --quiet_mafft Do not report progress (default: False) --outdir OUTDIR Output directory MAFFT Options: Option Description --algo ALGO Use different algorithm in command: linsi, ginsi, einsi, fftnsi, fftns, nwns, nwnsi --auto Automatically select algorithm (default: False) --sixmerpair Calculate distance based on shared 6mers, on by default (default: False) --globalpair Use Needleman-Wunsch algorithm (default: False) --localpair Use Smith-Waterman algorithm (default: False) --genafpair Use local algorithm with generalized affine gap cost (default: False) --fastapair Use FASTA for pairwise alignment (default: False) --weighti WEIGHTI Weighting factor for consistency term --retree RETREE Number of times to build guide tree --maxiterate MAXITERATE Number of cycles for iterative refinement --noscore Do not check alignment score in iterative alignment (default: False) --memsave Use Myers-Miller algorithm (default: False) --parttree Use fast tree-building method with 6mer distance (default: False) --dpparttree Use PartTree algorithm with distances based on DP (default: False) --fastaparttree Use PartTree algorithm with distances based on FASTA (default: False) --partsize PARTSIZE Number of partitions for PartTree --groupsize GROUPSIZE Max number of sequences for PartTree MAFFT Parameters: Option Description --lop LOP Gap opening penalty --lep LEP Offset value --lexp LEXP Gap extension penalty --LOP LOP Gap opening penalty to skip alignment --LEXP LEXP Gap extension penalty to skip alignment --bl BL BLOSUM matrix: 30, 45, 62, or 80 --jtt JTT JTT PAM number >0 --tm TM Transmembrane PAM number >0 --aamatrix AAMATRIX Path to user-defined AA scoring matrix --fmodel Incorporate AA/nuc composition info into scoring matrix (default: False) Options: Option Description --ncpu NCPU Number of CPU to use (default: 1) --quiet Do not write output to console (silence stdout and stderr) (default: False) --logfile LOGFILE Name for log file (output) --debug Print commands but do not run (default: False) --fastaonly Output fasta files separated by region but do not align (default: False) --alignall Do not separate files by region, align entire file (default: False) Example usage: haphpipe multiple_align --dir_list demo_sra_list.txt --ref_gtf HIV_B.K03455.HXB2.gtf --phylipout --logfile demo_multiple_align.log model_test Select the best-fit model of evolution from an alignment file using ModelTest-NG ( documentation ). Input is an alignment in FASTA or PHYLIP format. Output is ModelTest-NG results (text file) containing information for the best performing models. Usage: haphpipe model_test [ModelTest-NG OPTIONS] [HAPHPIPE OPTIONS] --seqs <FASTA> [--outdir] (or): hp_model_test [ModelTest-NG OPTIONS] [HAPHPIPE OPTIONS] --seqs <FASTA> [--outdir] Output files: ModelTest-NG output file ( modeltest_results.out ). Input/Output Arguments: Option Description --seqs SEQS Alignment in FASTA or PHYLIP format --outname Name for output file --outdir OUTDIR Output directory ModelTest-NG Options: Option Description --data_type Data type: nt or aa (default: nt) --partitions Partitions file --seed Seed for random number generator --topology TOPOLOGY Starting topology: ml, mp, fixed-ml-jc, fixed-ml-gtr, fixed-mp, random, or user (default: ml) --utree User-defined starting tree --force Force output overriding (default: False) --asc_bias ASC_BIAS Ascertainment bias correction: lewis, felsenstein, or stamatakis --frequencies FREQUENCIES Candidate model frequencies: e (estimated) or f (fixed) --het HET Set rate heterogeneity: u (uniform), i (invariant sites +I), g (gamma +G), or f (bothinvariant sites and gamma +I+G) --models MODELS Text file with candidate models, one per line --schemes SCHEMES Number of predefined DNA substitution schemes evaluated: 3, 5, 7, 11, or 203 --template TEMPLATE Set candidate models according to a specified tool: raxml, phyml, mrbayes, or paup Options: Option Description --ncpu NCPU Number of CPU to use (default: 1) --quiet Do not write output to console (silence stdout and stderr) (default: False) --logfile LOGFILE Name for log file (output) --debug Print commands but do not run (default: False) --keep_tmp Keep temporary directory (default: False) Example usage: haphpipe model_test --seqs multiple_align/alignment.fasta build_tree_NG Phylogeny reconstruction with RAxML-NG ( documentation ). Input is an alignment (FASTA or PHYLIP format). Output is a tree file. Please see the RAxML-NG documentation for a full description of RAxML-NG options. Usage: haphpipe build_tree_NG [RAxML OPTIONS] [HAPHPIPE OPTIONS] --seqs <FASTA> --output_name <TXT> [--outdir] (or): hp build_tree_NG [RAxML OPTIONS] [HAPHPIPE OPTIONS] --seqs <FASTA> --output_name <TXT> [--outdir] Output files: File Description %PREFIX.raxml.bestTree Outputs the best-scoring Maximum Likelihood tree %PREFIX.raxml.bestPartitionTrees Best-scoring ML tree for each partition %PREFIX.raxml.bestModel Optimized parameters for the highest-scoring ML tree %PREFIX.raxml.bootstraps Trees generated for every bootstrap replicate %PREFIX.raxml.bootstrapMSA. .phy Bootstrap replicate alignments %PREFIX.raxml.ckp Contains last log record if RAxML-NG has not finished successfully %PREFIX.raxml.consensusTree Consensus tree: estimates support for each clade of the final tree %PREFIX.raxml.log Screen log %PREFIX.raxml.mlTrees Maximum Likelihood trees for each starting tree %PREFIX.raxml.startTree The starting trees for each Maximum Likelihood inference %PREFIX.raxml.support Best-scoring Maximum Likelihood tree with bootstrap values %PREFIX.raxml.terrace Trees residing on a terrace (same likelihood or parsimony score) in compressed Newick form %PREFIX.raxml.terraceNewick Trees residing on a terrace in multi-line standard Newick form Input/Output Arguments: Option RAxML-NG Equivalent Description --seqs SEQS --msa Input alignment in PHYLIP or FASTA format --output_name NAME --prefix Run name for trees (default: build_tree.tre) --outdir OUTDIR --prefix Output directory (default: .) RAxML-NG Options: Option RAxML Equivalent-NG Description --model MODEL --model Substitution model OR path to partition file --all --all Run bootstrap search and find best ML tree --branch_length BRANCH_LENGTH --brlen Specify branch linkage model --consense --consense Build a consensus tree --rand_tree RAND_TREE --tree rand Start tree option: start from a random topology --pars_tree PARS_TREE --tree pars Start tree option: parsimony-based randomized stepwise addition algorithm --user_tree USER_TREE --tree Start tree option: upload custom tree in Newick format --search --search Predefined start tree: 10 random and 10 parsimony in v0.8.0 and later --search_1random --search1 Predefined start tree: 1 random --constraint_tree CONSTRAINT_TREE --tree-constraint Specify topological constraint tree --outgroup OUTGROUP --outgroup Outgroup to root inferred tree --bsconverge --bsconverge Posteriori bootstrap convergence test --bs_msa --bsmsa Bootstrap replicate alignments --bs_trees BS_TREES --bs-trees Number of bootstrap trees OR autoMRE --bs_tree_cutoff BS_TREE_CUTOFF --bs-cutoff Specify bootstopping cutoff value --bs_metric BS_METRIC --bs-metric Compare bootstrap support values --bootstrap --bootstrap Non-parametric bootstrap analysis --check --check Alignment sanity check --log LOG --log Options for output verbosity --loglh --loglh Compute and print the likelihood of the tree(s) without optimization or generating output files --terrace TERRACE --terrace Check if a tree is on a phylogenetic terrace. Options: Option Description --version check RAxML-NG version --seed SEED Seed for random numbers --redo Run even if there are existing files with the same name --keep_tmp Keep temporary directory --quiet Do not write output to console (silence stdout and stderr) (default: False) --logfile LOGFILE Name for log file (output) --debug Print commands but do not run (default: False) --ncpu NCPU Number of CPU to use (default: 1) Example usage: haphpipe build_tree_NG --all --seqs hp_alignments/alignment.fasta --model GTR","title":"Phylo"},{"location":"phylo/#phylo-quick-start","text":"HAPHPIPE includes three modules for phylogenomics: multiple_align , model_test , and build_tree . These three modules are sufficient to turn your consensus and/or haplotype sequences from the other modules into a phylogenetic tree! For purposes of this quick-start guide, we will demonstrate the modules to create a tree from HIV pol consensus sequences. Step 1: Alignment After running either of the assembly pipelines, final.fna files will be located in directories named ./<SampleID>/haphpipe_assemble_0[1|2] . For the multiple_align module, we need to create a list of all of these directories. We can do so easily with one command (shown for haphpipe_assemble_01 output: ls -d ./SRR*/haphpipe_assemble_01 > ./dir_list.txt Now, we will align all of these final.fna files: haphpipe multiple_align --dir_list dir_list.txt --ref_gtf refs/HIV_B.K03455.HXB2.gtf The output will be located in a new directory, hp_multiple_align . The alignment of pol sequences is the file alignment_region00.fasta . Step 2: Model Selection Now, we will use the model_test module to determine the best-fit evolutionary model for our data. This is an input to the tree building module. We will use this command to generate best-fit models available in RAxML: haphpipe model_test --seqs hp_multiple_align/alignment_region00.fasta --run_id alignment_region00 --template raxml The ModelTest output will be written to a file called modeltest_results.out and a summary of all the best models will be written to modeltest_results_summary.tsv . Examples of both are below. ModelTest-NG Output -------------------------------------------------------------------------------- ModelTest-NG vx.y.z Input data: MSA: multiple_align/alignment.fasta Tree: Maximum likelihood file: - #taxa: 6 #sites: 1975 #patterns: 114 Max. thread mem: 0 MB Output: Log: /var/folders/lv/dqdkd8957_3fv6yxsyfsvn0r0000gn/T/tmpHP_model_testud4sc4ya/samp12_modeltest_results.log Starting tree: /var/folders/lv/dqdkd8957_3fv6yxsyfsvn0r0000gn/T/tmpHP_model_testud4sc4ya/samp12_modeltest_results.tree Results: /var/folders/lv/dqdkd8957_3fv6yxsyfsvn0r0000gn/T/tmpHP_model_testud4sc4ya/samp12_modeltest_results.out Selection options: # dna schemes: 11 # dna models: 88 include model parameters: Uniform: true p-inv (+I): true gamma (+G): true both (+I+G): true free rates (+R): false fixed freqs: true estimated freqs: true #categories: 4 gamma rates mode: mean asc bias: none epsilon (opt): 0.01 epsilon (par): 0.05 keep branches: false Additional options: verbosity: very low threads: 1/6 RNG seed: 12345 subtree repeats: enabled -------------------------------------------------------------------------------- BIC model K lnL score delta weight -------------------------------------------------------------------------------- 1 HKY 4 -5380.7535 10860.1551 0.0000 0.7049 2 TrN 5 -5378.2013 10862.6391 2.4839 0.2036 3 TPM1uf 5 -5379.8699 10865.9763 5.8211 0.0384 4 TPM3uf 5 -5380.7175 10867.6716 7.5164 0.0164 5 HKY+G4 5 -5380.8440 10867.9246 7.7694 0.0145 6 HKY+I 5 -5381.4917 10869.2200 9.0648 0.0076 7 TIM3 6 -5378.1637 10870.1523 9.9971 0.0048 8 TrN+G4 6 -5378.3069 10870.4387 10.2836 0.0041 9 TPM2uf+G4 6 -5378.9908 10871.8064 11.6512 0.0021 10 TrN+I 6 -5379.0181 10871.8610 11.7059 0.0020 -------------------------------------------------------------------------------- Best model according to BIC --------------------------- Model: HKY lnL: -5380.7535 Frequencies: 0.3695 0.1709 0.2219 0.2377 Subst. Rates: 1.0000 2.4741 1.0000 1.0000 2.4741 1.0000 Inv. sites prop: - Gamma shape: - Score: 10860.1551 Weight: 0.7049 --------------------------- Parameter importances --------------------------- P.Inv: 0.0100 Gamma: 0.0216 Gamma-Inv: 0.0002 Frequencies: 1.0000 --------------------------- Model averaged estimates --------------------------- P.Inv: 0.0215 Alpha: 94.2337 Alpha-P.Inv: 91.7960 P.Inv-Alpha: 0.0214 Frequencies: 0.3700 0.1702 0.2226 0.2372 Commands: > phyml -i multiple_align/alignment.fasta -m 010010 -f m -v 0 -a 0 -c 1 -o tlr > raxmlHPC-SSE3 -s multiple_align/alignment.fasta -c 1 -m GTRCATX -n EXEC_NAME -p PARSIMONY_SEED > raxml-ng --msa multiple_align/alignment.fasta --model HKY > paup -s multiple_align/alignment.fasta > iqtree -s multiple_align/alignment.fasta -m HKY AIC model K lnL score delta weight -------------------------------------------------------------------------------- 1 TrN 5 -5378.2013 10784.4026 0.0000 0.2246 2 TIM2+G4 7 -5376.4972 10784.9944 0.5919 0.1671 3 TIM3 6 -5378.1637 10786.3274 1.9249 0.0858 4 TIM2+I 7 -5377.2570 10786.5141 2.1115 0.0782 5 TrN+G4 6 -5378.3069 10786.6138 2.2113 0.0744 6 TIM1+G4 7 -5377.3549 10786.7098 2.3073 0.0709 7 HKY 4 -5380.7535 10787.5069 3.1044 0.0476 8 TPM1uf 5 -5379.8699 10787.7398 3.3372 0.0423 9 TPM2uf+G4 6 -5378.9908 10787.9815 3.5790 0.0375 10 TrN+I 6 -5379.0181 10788.0362 3.6336 0.0365 -------------------------------------------------------------------------------- Best model according to AIC --------------------------- Model: TrN lnL: -5378.2013 Frequencies: 0.3720 0.1679 0.2249 0.2352 Subst. Rates: 1.0000 2.2008 1.0000 1.0000 3.1065 1.0000 Inv. sites prop: - Gamma shape: - Score: 10784.4026 Weight: 0.2246 --------------------------- Parameter importances --------------------------- P.Inv: 0.1262 Gamma: 0.3943 Gamma-Inv: 0.0384 Frequencies: 1.0000 --------------------------- Model averaged estimates --------------------------- P.Inv: 0.0216 Alpha: 93.2595 Alpha-P.Inv: 94.4193 P.Inv-Alpha: 0.0216 Frequencies: 0.3718 0.1684 0.2238 0.2359 Commands: > phyml -i multiple_align/alignment.fasta -m 010020 -f m -v 0 -a 0 -c 1 -o tlr > raxmlHPC-SSE3 -s multiple_align/alignment.fasta -c 1 -m GTRCATX -n EXEC_NAME -p PARSIMONY_SEED > raxml-ng --msa multiple_align/alignment.fasta --model TrN > paup -s multiple_align/alignment.fasta > iqtree -s multiple_align/alignment.fasta -m TrN AICc model K lnL score delta weight -------------------------------------------------------------------------------- 1 TrN 5 -5378.2013 10784.4026 0.0000 0.2246 2 TIM2+G4 7 -5376.4972 10784.9944 0.5919 0.1671 3 TIM3 6 -5378.1637 10786.3274 1.9249 0.0858 4 TIM2+I 7 -5377.2570 10786.5141 2.1115 0.0782 5 TrN+G4 6 -5378.3069 10786.6138 2.2113 0.0744 6 TIM1+G4 7 -5377.3549 10786.7098 2.3073 0.0709 7 HKY 4 -5380.7535 10787.5069 3.1044 0.0476 8 TPM1uf 5 -5379.8699 10787.7398 3.3372 0.0423 9 TPM2uf+G4 6 -5378.9908 10787.9815 3.5790 0.0375 10 TrN+I 6 -5379.0181 10788.0362 3.6336 0.0365 -------------------------------------------------------------------------------- Best model according to AICc --------------------------- Model: TrN lnL: -5378.2013 Frequencies: 0.3720 0.1679 0.2249 0.2352 Subst. Rates: 1.0000 2.2008 1.0000 1.0000 3.1065 1.0000 Inv. sites prop: - Gamma shape: - Score: 10784.4026 Weight: 0.2246 --------------------------- Parameter importances --------------------------- P.Inv: 0.1262 Gamma: 0.3943 Gamma-Inv: 0.0384 Frequencies: 1.0000 --------------------------- Model averaged estimates --------------------------- P.Inv: 0.0216 Alpha: 93.2595 Alpha-P.Inv: 94.4193 P.Inv-Alpha: 0.0216 Frequencies: 0.3718 0.1684 0.2238 0.2359 Commands: > phyml -i multiple_align/alignment.fasta -m 010020 -f m -v 0 -a 0 -c 1 -o tlr > raxmlHPC-SSE3 -s multiple_align/alignment.fasta -c 1 -m GTRCATX -n EXEC_NAME -p PARSIMONY_SEED > raxml-ng --msa multiple_align/alignment.fasta --model TrN > paup -s multiple_align/alignment.fasta > iqtree -s multiple_align/alignment.fasta -m TrN Done ModelTest-NG Output Summary File Criteria Best Model hp_multiple_align/alignment.fasta BIC HKY hp_multiple_align/alignment.fasta AIC TrN hp_multiple_align/alignment.fasta AICc TrN Step 3: Build a Tree Now, we will use build_tree_NG to build our tree! You should use the best model outputted in model_test for the --model argument (here we are using GTR). The --all option will automatically run a full maximum likelihood & bootstrapping analysis for us: haphpipe build_tree_NG --seqs hp_multiple_align/alignment_region00.fasta --all --model GTR The output will be written to a new directory, hp_build_tree . The best tree file from RAxML will be outputted as hp_tree.raxml.support . This tree can then be annotated in programs such as FigTree or iTOL . Phylogenomics Pipelines For users who would like to build a full pipeline to run assembly and phylogenetics stages in one go, we recommend adapting the demo pipeline ( haphpipe_demo ) for this purpose. See the demo page for more details.","title":"Phylo Quick-Start"},{"location":"phylo/#multiple_align","text":"Align consensus sequences using MAFFT ( documentation ). Input can be a list of directories which contain final.fna and/or ph_haplotypes.fna files or a fasta file, or both (in which case the sequences in the FASTA file are combined with the final.fna and/or ph_haplotypes.fna files retreived before the alignment. Sequences will be separated by amplicons using a supplied GTF file before alignment (unless the --alignall option is specified). This module may also be used to separate files by amplicons (without aligning) by specifying the --fastaonly option. Alignments are by default outputted as FASTA files, although PHYLIP ( --phylipout ) or CLUSTAL ( --clustalout ) output options are also available. Many options from MAFFT are available in this module. Please refer to the MAFFT documentation above for information about these options. Usage: haphpipe multiple_align [MAFFT OPTIONS] [HAPHPIPE OPTIONS] --seqs <FASTA> --dir_list <TXT> --ref_gtf <GTF> [--outdir] (or): hp_multiple_align [MAFFT OPTIONS] [HAPHPIPE OPTIONS] --seqs <FASTA> --dir_list <TXT> --ref_gtf <GTF> [--outdir] Output files: alignment files in FASTA format (default), one per amplicon (or one alignment.fasta file if using --alignall option) Note: MAFFT stores intermediate files in a temporary directory located in /tmp. More information is available here . Input/Output Arguments: Option Description --seqs SEQS FASTA file with sequences to be aligned --dir_list DIR_LIST List of directories which include either a final.fna or ph_haplotypes.fna file, one on each line --ref_gtf REF_GTF Reference GTF file --out_align OUT_ALIGN Name for alignment file --nuc Assume nucleotide (default: False) --amino Assume amino (default: False) --clustalout Clustal output format (default: False) --phylipout PHYLIP output format (default: False) --inputorder Output order same as input (default: False) --reorder Output order aligned (default: False) --treeout Guide tree is output to the input.tree file (default:False) --quiet_mafft Do not report progress (default: False) --outdir OUTDIR Output directory MAFFT Options: Option Description --algo ALGO Use different algorithm in command: linsi, ginsi, einsi, fftnsi, fftns, nwns, nwnsi --auto Automatically select algorithm (default: False) --sixmerpair Calculate distance based on shared 6mers, on by default (default: False) --globalpair Use Needleman-Wunsch algorithm (default: False) --localpair Use Smith-Waterman algorithm (default: False) --genafpair Use local algorithm with generalized affine gap cost (default: False) --fastapair Use FASTA for pairwise alignment (default: False) --weighti WEIGHTI Weighting factor for consistency term --retree RETREE Number of times to build guide tree --maxiterate MAXITERATE Number of cycles for iterative refinement --noscore Do not check alignment score in iterative alignment (default: False) --memsave Use Myers-Miller algorithm (default: False) --parttree Use fast tree-building method with 6mer distance (default: False) --dpparttree Use PartTree algorithm with distances based on DP (default: False) --fastaparttree Use PartTree algorithm with distances based on FASTA (default: False) --partsize PARTSIZE Number of partitions for PartTree --groupsize GROUPSIZE Max number of sequences for PartTree MAFFT Parameters: Option Description --lop LOP Gap opening penalty --lep LEP Offset value --lexp LEXP Gap extension penalty --LOP LOP Gap opening penalty to skip alignment --LEXP LEXP Gap extension penalty to skip alignment --bl BL BLOSUM matrix: 30, 45, 62, or 80 --jtt JTT JTT PAM number >0 --tm TM Transmembrane PAM number >0 --aamatrix AAMATRIX Path to user-defined AA scoring matrix --fmodel Incorporate AA/nuc composition info into scoring matrix (default: False) Options: Option Description --ncpu NCPU Number of CPU to use (default: 1) --quiet Do not write output to console (silence stdout and stderr) (default: False) --logfile LOGFILE Name for log file (output) --debug Print commands but do not run (default: False) --fastaonly Output fasta files separated by region but do not align (default: False) --alignall Do not separate files by region, align entire file (default: False) Example usage: haphpipe multiple_align --dir_list demo_sra_list.txt --ref_gtf HIV_B.K03455.HXB2.gtf --phylipout --logfile demo_multiple_align.log","title":"multiple_align"},{"location":"phylo/#model_test","text":"Select the best-fit model of evolution from an alignment file using ModelTest-NG ( documentation ). Input is an alignment in FASTA or PHYLIP format. Output is ModelTest-NG results (text file) containing information for the best performing models. Usage: haphpipe model_test [ModelTest-NG OPTIONS] [HAPHPIPE OPTIONS] --seqs <FASTA> [--outdir] (or): hp_model_test [ModelTest-NG OPTIONS] [HAPHPIPE OPTIONS] --seqs <FASTA> [--outdir] Output files: ModelTest-NG output file ( modeltest_results.out ). Input/Output Arguments: Option Description --seqs SEQS Alignment in FASTA or PHYLIP format --outname Name for output file --outdir OUTDIR Output directory ModelTest-NG Options: Option Description --data_type Data type: nt or aa (default: nt) --partitions Partitions file --seed Seed for random number generator --topology TOPOLOGY Starting topology: ml, mp, fixed-ml-jc, fixed-ml-gtr, fixed-mp, random, or user (default: ml) --utree User-defined starting tree --force Force output overriding (default: False) --asc_bias ASC_BIAS Ascertainment bias correction: lewis, felsenstein, or stamatakis --frequencies FREQUENCIES Candidate model frequencies: e (estimated) or f (fixed) --het HET Set rate heterogeneity: u (uniform), i (invariant sites +I), g (gamma +G), or f (bothinvariant sites and gamma +I+G) --models MODELS Text file with candidate models, one per line --schemes SCHEMES Number of predefined DNA substitution schemes evaluated: 3, 5, 7, 11, or 203 --template TEMPLATE Set candidate models according to a specified tool: raxml, phyml, mrbayes, or paup Options: Option Description --ncpu NCPU Number of CPU to use (default: 1) --quiet Do not write output to console (silence stdout and stderr) (default: False) --logfile LOGFILE Name for log file (output) --debug Print commands but do not run (default: False) --keep_tmp Keep temporary directory (default: False) Example usage: haphpipe model_test --seqs multiple_align/alignment.fasta","title":"model_test"},{"location":"phylo/#build_tree_ng","text":"Phylogeny reconstruction with RAxML-NG ( documentation ). Input is an alignment (FASTA or PHYLIP format). Output is a tree file. Please see the RAxML-NG documentation for a full description of RAxML-NG options. Usage: haphpipe build_tree_NG [RAxML OPTIONS] [HAPHPIPE OPTIONS] --seqs <FASTA> --output_name <TXT> [--outdir] (or): hp build_tree_NG [RAxML OPTIONS] [HAPHPIPE OPTIONS] --seqs <FASTA> --output_name <TXT> [--outdir] Output files: File Description %PREFIX.raxml.bestTree Outputs the best-scoring Maximum Likelihood tree %PREFIX.raxml.bestPartitionTrees Best-scoring ML tree for each partition %PREFIX.raxml.bestModel Optimized parameters for the highest-scoring ML tree %PREFIX.raxml.bootstraps Trees generated for every bootstrap replicate %PREFIX.raxml.bootstrapMSA. .phy Bootstrap replicate alignments %PREFIX.raxml.ckp Contains last log record if RAxML-NG has not finished successfully %PREFIX.raxml.consensusTree Consensus tree: estimates support for each clade of the final tree %PREFIX.raxml.log Screen log %PREFIX.raxml.mlTrees Maximum Likelihood trees for each starting tree %PREFIX.raxml.startTree The starting trees for each Maximum Likelihood inference %PREFIX.raxml.support Best-scoring Maximum Likelihood tree with bootstrap values %PREFIX.raxml.terrace Trees residing on a terrace (same likelihood or parsimony score) in compressed Newick form %PREFIX.raxml.terraceNewick Trees residing on a terrace in multi-line standard Newick form Input/Output Arguments: Option RAxML-NG Equivalent Description --seqs SEQS --msa Input alignment in PHYLIP or FASTA format --output_name NAME --prefix Run name for trees (default: build_tree.tre) --outdir OUTDIR --prefix Output directory (default: .) RAxML-NG Options: Option RAxML Equivalent-NG Description --model MODEL --model Substitution model OR path to partition file --all --all Run bootstrap search and find best ML tree --branch_length BRANCH_LENGTH --brlen Specify branch linkage model --consense --consense Build a consensus tree --rand_tree RAND_TREE --tree rand Start tree option: start from a random topology --pars_tree PARS_TREE --tree pars Start tree option: parsimony-based randomized stepwise addition algorithm --user_tree USER_TREE --tree Start tree option: upload custom tree in Newick format --search --search Predefined start tree: 10 random and 10 parsimony in v0.8.0 and later --search_1random --search1 Predefined start tree: 1 random --constraint_tree CONSTRAINT_TREE --tree-constraint Specify topological constraint tree --outgroup OUTGROUP --outgroup Outgroup to root inferred tree --bsconverge --bsconverge Posteriori bootstrap convergence test --bs_msa --bsmsa Bootstrap replicate alignments --bs_trees BS_TREES --bs-trees Number of bootstrap trees OR autoMRE --bs_tree_cutoff BS_TREE_CUTOFF --bs-cutoff Specify bootstopping cutoff value --bs_metric BS_METRIC --bs-metric Compare bootstrap support values --bootstrap --bootstrap Non-parametric bootstrap analysis --check --check Alignment sanity check --log LOG --log Options for output verbosity --loglh --loglh Compute and print the likelihood of the tree(s) without optimization or generating output files --terrace TERRACE --terrace Check if a tree is on a phylogenetic terrace. Options: Option Description --version check RAxML-NG version --seed SEED Seed for random numbers --redo Run even if there are existing files with the same name --keep_tmp Keep temporary directory --quiet Do not write output to console (silence stdout and stderr) (default: False) --logfile LOGFILE Name for log file (output) --debug Print commands but do not run (default: False) --ncpu NCPU Number of CPU to use (default: 1) Example usage: haphpipe build_tree_NG --all --seqs hp_alignments/alignment.fasta --model GTR","title":"build_tree_NG"}]}