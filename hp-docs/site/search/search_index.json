{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About HAPHPIPE HA plotype and PH ylodynamics pipeline for viral assembly, population genetics, and phylodynamics. In this User Guide, we assume basic familiarity with conda environments, basic bash knowledge and knowledge of general next-generation sequencing (NGS) concepts. For more information regarding all of these, see helpful links and FAQ . Remember that a directory is a simply a folder nesting system, similar to what you see on your computer. Where you see \"folder\" below you can also say \"directory.\" HAPHPIPE is intended only for Linux and Mac OS X platforms. If you are a Windows user, see this section on our install page . This User Guide was developed jointly by undergraduate and graduate students to be accessible for users at all stages. Margaret C. Steiner, Keylie M. Gibson, Matthew L. Bendall, and Uzma Rentia all contributed to developing this User Guide and testing HAPHPIPE. See our protocol paper (link coming soon) for more information and this paper (link coming soon) for our validation study. Citing HAPHPIPE When using HAPHPIPE, please cite our article when it is available. For now, reference the GitHub website: https://github.com/gwcbi/haphpipe The HAPHPIPE suite Each stage can be run on its own. Stages are grouped into 4 categories: hp_reads , hp_assemble , hp_haplotype , and hp_annotate . More detailed description of command line options for each stage are available in the below sections. To view all available stages in HAPHPIPE, run: haphpipe -h Output will look like: Program: haphpipe (haplotype and phylodynamics pipeline) Version: 0.8.1 Commands: -- Reads sample_reads subsample reads using seqtk trim_reads trim reads using Trimmomatic join_reads join reads using FLASh ec_reads error correct reads using SPAdes -- Assemble assemble_denovo assemble reads denovo assemble_amplicons assemble contigs to amplicon regions assemble_scaffold assemble contigs to genome align_reads align reads to reference call_variants call variants vcf_to_consensus create consensus sequence from VCF refine_assembly iterative refinement: align - variants - consensus finalize_assembly finalize consensus sequence -- Haplotype predict_haplo assemble haplotypes with PredictHaplo ph_parser parse output from PredictHaplo. -- Annotate pairwise_align align consensus to an annotated reference extract_pairwise extract sequence regions from pairwise alignment annotate_from_ref annotate consensus from reference annotation -- Miscellaneous demo setup demo directory and test data HAPHPIPE consists of a suite of sub-commands under each stage that are invoked as follows: haphpipe [stage] [sub-command] [options] For example, to join paired end reads, one would invoke the following: haphpipe join_reads --fq1 trimmed_1.fastq --fq2 trimmed_2.fastq","title":"Home"},{"location":"#about-haphpipe","text":"HA plotype and PH ylodynamics pipeline for viral assembly, population genetics, and phylodynamics. In this User Guide, we assume basic familiarity with conda environments, basic bash knowledge and knowledge of general next-generation sequencing (NGS) concepts. For more information regarding all of these, see helpful links and FAQ . Remember that a directory is a simply a folder nesting system, similar to what you see on your computer. Where you see \"folder\" below you can also say \"directory.\" HAPHPIPE is intended only for Linux and Mac OS X platforms. If you are a Windows user, see this section on our install page . This User Guide was developed jointly by undergraduate and graduate students to be accessible for users at all stages. Margaret C. Steiner, Keylie M. Gibson, Matthew L. Bendall, and Uzma Rentia all contributed to developing this User Guide and testing HAPHPIPE. See our protocol paper (link coming soon) for more information and this paper (link coming soon) for our validation study.","title":"About HAPHPIPE"},{"location":"#citing-haphpipe","text":"When using HAPHPIPE, please cite our article when it is available. For now, reference the GitHub website: https://github.com/gwcbi/haphpipe","title":"Citing HAPHPIPE"},{"location":"#the-haphpipe-suite","text":"Each stage can be run on its own. Stages are grouped into 4 categories: hp_reads , hp_assemble , hp_haplotype , and hp_annotate . More detailed description of command line options for each stage are available in the below sections. To view all available stages in HAPHPIPE, run: haphpipe -h Output will look like: Program: haphpipe (haplotype and phylodynamics pipeline) Version: 0.8.1 Commands: -- Reads sample_reads subsample reads using seqtk trim_reads trim reads using Trimmomatic join_reads join reads using FLASh ec_reads error correct reads using SPAdes -- Assemble assemble_denovo assemble reads denovo assemble_amplicons assemble contigs to amplicon regions assemble_scaffold assemble contigs to genome align_reads align reads to reference call_variants call variants vcf_to_consensus create consensus sequence from VCF refine_assembly iterative refinement: align - variants - consensus finalize_assembly finalize consensus sequence -- Haplotype predict_haplo assemble haplotypes with PredictHaplo ph_parser parse output from PredictHaplo. -- Annotate pairwise_align align consensus to an annotated reference extract_pairwise extract sequence regions from pairwise alignment annotate_from_ref annotate consensus from reference annotation -- Miscellaneous demo setup demo directory and test data HAPHPIPE consists of a suite of sub-commands under each stage that are invoked as follows: haphpipe [stage] [sub-command] [options] For example, to join paired end reads, one would invoke the following: haphpipe join_reads --fq1 trimmed_1.fastq --fq2 trimmed_2.fastq","title":"The HAPHPIPE suite"},{"location":"adv/","text":"Making your own pipelines with Bash This is an example of how to make your own pipeline with bash. This example uses four NGS samples for COVID-19 from NCBI and puts them through whole genome de novo assembly. SRA accession numbers: SRR11140744 SRR11140746 SRR11140748 SRR11140750 Step 0 - Obtaining samples. We will assume that you have downloaded the reads from NCBI or know how to use fastq-dump. We will show the fastq-dump command here that we used to download the files. This part is not included in the pipeline. for sra in SRR11140744 SRR11140746 SRR11140748 SRR11140750; do fastq-dump --outdir ${sra} --split-files --origfmt ${sra} done Output is: Read 503344 spots for SRR11140744 Written 503344 spots for SRR11140744 Read 358971 spots for SRR11140746 Written 358971 spots for SRR11140746 Read 421395 spots for SRR11140748 Written 421395 spots for SRR11140748 Read 17657 spots for SRR11140750 Written 17657 spots for SRR11140750 You're starting directory should look like for this example, whether the reads were obtained manually or downloaded with fastq-dump: . \u251c\u2500\u2500 SRR11140744 | \u251c\u2500\u2500 SRR11140744_1.fastq | \u2514\u2500\u2500 SRR11140744_2.fastq \u251c\u2500\u2500 SRR11140746 | \u251c\u2500\u2500 SRR11140746_1.fastq | \u2514\u2500\u2500 SRR11140746_2.fastq \u251c\u2500\u2500 SRR11140748 | \u251c\u2500\u2500 SRR11140748_1.fastq | \u2514\u2500\u2500 SRR11140748_2.fastq \u2514\u2500\u2500 SRR11140750 \u251c\u2500\u2500 SRR11140750_1.fastq \u2514\u2500\u2500 SRR11140750_2.fastq We also know we will need a reference genome to help scaffold the contigs. We have downloaded the COVID19 reference genome here as a FASTA file. Step 1 - Evaluate which modules you want to use. View all the module options using haphpipe -h . We have decided that we want to sample the reads ( sample_reads ), trim the reads ( trim_reads ), error correct the reads ( ec_reads ). We want to do genome de novo assembly, so we want to use assemble_denovo and assemble_scaffold . We then want to do refinement of the assembly ( refine_assembly ) and finalize the assembly ( finaliza_assembly ). Step 2 - Document files and options needed for each module. As we go through each module, we will take note of what we need and which input is specific for an individual sample (i.e., the input fastq reads will be different per sample, but we probably want the same number of reads for each sample.) We know all modules have the option for a logfile. We'll include that in the bash scripting, but don't need to make notes of a logfile for each module. Part 2A - Sample reads. Upon viewing the haphpipe sample_reads -h we see that we need to provide fastq reads 1 and 2, an output directory, and number of reads desired. Sample specific options: fastq read 1 fastq read 2 output directory Not sample specific options: number of reads desired Part 2B - Trim reads. Upon viewing the haphpipe trim_reads -h we see that we need to provide fastq reads 1 and 2, an output directory, number of reads, and number of CPUs desired. There is an option to change the timming commands, but we'll keep default. Sample specific options: fastq read 1 -- we will want these to be the subsampled read file fastq read 2 -- we will want these to be the subsampled read file output directory Not sample specific options: number of reads desired ncpu Part 2C - Error correct reads. Upon viewing the haphpipe ec_reads -h we see that we need to provide fastq reads 1 and 2, an output directory, and number of CPUs desired. Sample specific options: fastq read 1 -- we will want these to be the trimmed read file fastq read 2 -- we will want these to be the trimmed read file output directory Not sample specific options: ncpu Part 2D - De novo assembly. Upon viewing the haphpipe assemble_denovo -h we see that we need to provide fastq reads 1 and 2, an output directory, and number of CPUs desired. Because we previously completed error correction. We will not want to include it here (i.e., we will invoke the option command --no_error_correction ). Sample specific options: fastq read 1 -- we will want these to be the error corrected reads fastq read 2 -- we will want these to be the error corrected reads output directory Not sample specific options: ncpu no error correction option Part 2E - Assemble scaffold. Upon viewing the haphpipe assemble_scaffold -h we see that we need to provide fasta file containing assembled contigs, an output directory, name to append to scaffold sequence and reference fasta desired. There is an option to change the timming commands, but we'll keep default. Sample specific options: assembled contigs file output directory sequence name Not sample specific options: reference fasta Part 2F - Refine assembly. Upon viewing the haphpipe refine_assembly -h we see that we need to provide fastq reads 1 and 2, an output directory, a reference sequence to refine, a sample ID and a maximum number of refinement steps, and number of CPUs desired. Sample specific options: fastq read 1 -- we will want these to be the error corrected reads fastq read 2 -- we will want these to be the error corrected reads output directory reference fasta -- we will want this to be the sample's assembled scaffold Sample ID Not sample specific options: ncpu maximum number of refinement steps (we'll do 3 for sake of simplicity and time) Part 2G - Finalize assembly. Upon viewing the haphpipe finalize_assembly -h we see that we need to provide fastq reads 1 and 2, an output directory, a reference sequence to finalize, a sample ID and a maximum number of refinement steps, and number of CPUs desired. We could replace the preset bowtie2 option, but we will leave it as default for this sample pipeline. Sample specific options: fastq read 1 -- we will want these to be the error corrected reads fastq read 2 -- we will want these to be the error corrected reads output directory reference fasta -- we will want this to be the sample's refined fasta sequence Sample ID Not sample specific options: ncpu Part 2H - Gather the needed initial input for each sample. We need to gather the necessary files that we will need to input for each sample so that we can make a script that takes in input files. We know from the User Guide that we can look at the output/input file types and names here . Because each module has a standard output file name, we will only need to be specific about the input for the raw fastq files for each sample. Therefore we need: input raw fastq read 1 input raw fastq read 2 reference genome to scaffold against (this will be a covid19 reference) output directory for each sample sample name The other options we can code into the bash script for each module, since they will be the same for every sample in this analysis. Step 3 - Create a bash script for each module. Now we will format a bash script for each module. Part 3A - Input options. Because we have a list of needed input options (specified by the user), we need to make a bash command to take in the inputs. First, we will specify the script name. We can do this one of two ways. i) explicitly or 2) through a command. i) SN='covid_genome' # this sets the script name (SN variable) to covid_genome ii) SN=$(basename $0) # this sets the script name (SN variable) to whatever the script filename is. If the script's file name is covid.sh then it is set as that. If the file name is this_is_file it will be set as that. Because we are in charge of this script, we will explicitly set it. Second, we want to set some input information for the user. We can do this as such: read -r -d '' USAGE EOF USAGE: $SN [read1] [read2] [reference_fasta] [samp_id] outdir ----- COVID19 Genome Assembly Pipeline ----- This pipeline implements genome assembly using a denovo approach. Reads are error-corrected and used to refine the scaffolded assembly, with up to 3 refinement steps. This pipeline is used as an example for advanced usage - making own pipeline in the User Guide. Input: read1: Fastq file for read 1. May be compressed (.gz) read2: Fastq file for read 2. May be compressed (.gz) reference_fasta: Reference sequence (fasta) samp_id: Sample ID outdir: Output directory (default is [sample_dir]/$SN) EOF Third, we want to provide help information for the script. We can do that using the avoce code, which has been saved in the variable $USAGE . Therefore, #--- Read command line args and if arg is -h, provide the usage information [[ -n $1 ]] [[ $1 == '-h' ]] echo $USAGE exit 0 If pipeline.sh -h is invoked here, then the output is: USAGE: covid_genome_assembly [read1] [read2] [reference_fasta] [samp_id] outdir ----- COVID19 Genome Assembly Pipeline ----- This pipeline implements genome assembly using a denovo approach. Reads are error-corrected and used to refine the scaffolded assembly, with up to 3 refinement steps. This pipeline is used as an example for advanced usage - making own pipeline in the User Guide. Input: read1: Fastq file for read 1. May be compressed (.gz) read2: Fastq file for read 2. May be compressed (.gz) reference_fasta: Reference sequence (fasta) samp_id: Sample ID outdir: Output directory (default is [sample_dir]/covid_genome_assembly) Note that this output looks identical to the information we put into $USAGE variable above in the second part of this section. Fourth, we want to read in the command with the provided input options. Because we are using a bash script, the position of the input files is imparative. #--- Read command line args [[ -n $1 ]] raw1= $1 [[ -n $2 ]] raw2= $2 [[ -n $3 ]] refFA= $3 [[ -n $4 ]] sampid= $4 [[ -n $5 ]] outdir= $5 Fifth, we want to check that the input files are provided and are not empty. We also want to set the outdirectory. #--- Check that files are provided and exist [[ -z ${raw1+x} ]] echo FAILED: read1 is not set echo $USAGE exit 1 [[ ! -e $raw1 ]] echo [---$SN---] ($(date)) FAILED: file $raw1 does not exist exit 1 [[ -z ${raw2+x} ]] echo FAILED: read2 is not set echo $USAGE exit 1 [[ ! -e $raw2 ]] echo [---$SN---] ($(date)) FAILED: file $raw2 does not exist exit 1 [[ -z ${refFA+x} ]] echo FAILED: refFA is not set echo $USAGE exit 1 [[ ! -e $refFA ]] echo [---$SN---] ($(date)) FAILED: file $refFA does not exist exit 1 [[ -z ${sampid+x} ]] echo FAILED: sampid is not set echo $USAGE exit 1 #--- Set outdirectory [[ -z ${outdir+x} ]] outdir=$(dirname $raw1)/$SN mkdir -p $outdir Sixth, we want to set the number of CPUs to use throughout the script. #--- Determine CPUs to use # First examines NCPU environment variable, then nproc, finally sets to 1 [[ -n $NCPU ]] ncpu=$NCPU [[ -z $ncpu ]] ncpu=$(nproc 2 /dev/null) [[ -z $ncpu ]] ncpu=1 Seventh, we want to print out the variables and files. echo [---$SN---] ($(date)) read1: $raw1 echo [---$SN---] ($(date)) read2: $raw2 echo [---$SN---] ($(date)) reference_fasta: $refFA echo [---$SN---] ($(date)) samp_id: $sampid echo [---$SN---] ($(date)) outdir: $outdir echo [---$SN---] ($(date)) num CPU: $ncpu Finally, because here at GWU CBI, we like to time everything, we include a line of code to start a timer and end the timer: #--- Start the timer t1=$(date + %s ) #### Put haphpipe module scripts here #---Complete job t2=$(date + %s ) diff=$(($t2-$t1)) echo [---$SN---] ($(date)) $(($diff / 60)) minutes and $(($diff % 60)) seconds elapsed. echo [---$SN---] ($(date)) $SN COMPLETE. After Part 3A, our pipeline file should read as such: #!/usr/bin/env bash ############################################################################### # This pipeline implements genome assembly using a denovo approach. Reads are # error-corrected and used to refine the scaffolded assembly, with up to 3 # refinement steps. This pipeline is used as an example for advanced usage # - making own pipeline in the User Guide. ############################################################################### SN='covid_genome_assembly' read -r -d '' USAGE EOF USAGE: $SN [read1] [read2] [reference_fasta] [samp_id] outdir ----- COVID19 Genome Assembly Pipeline ----- This pipeline implements genome assembly using a denovo approach. Reads are error-corrected and used to refine the scaffolded assembly, with up to 3 refinement steps. This pipeline is used as an example for advanced usage - making own pipeline in the User Guide. Input: read1: Fastq file for read 1. May be compressed (.gz) read2: Fastq file for read 2. May be compressed (.gz) reference_fasta: Reference sequence (fasta) samp_id: Sample ID outdir: Output directory (default is [sample_dir]/$SN) EOF #--- Read command line args and if arg is -h, provide the usage information [[ -n $1 ]] [[ $1 == '-h' ]] echo $USAGE exit 0 #--- Read command line args [[ -n $1 ]] raw1= $1 [[ -n $2 ]] raw2= $2 [[ -n $3 ]] refFA= $3 [[ -n $4 ]] refGTF= $4 [[ -n $5 ]] sampid= $5 [[ -n $6 ]] outdir= $6 #--- Check that files are provided and exist [[ -z ${raw1+x} ]] echo FAILED: read1 is not set echo $USAGE exit 1 [[ ! -e $raw1 ]] echo [---$SN---] ($(date)) FAILED: file $raw1 does not exist exit 1 [[ -z ${raw2+x} ]] echo FAILED: read2 is not set echo $USAGE exit 1 [[ ! -e $raw2 ]] echo [---$SN---] ($(date)) FAILED: file $raw2 does not exist exit 1 [[ -z ${refFA+x} ]] echo FAILED: refFA is not set echo $USAGE exit 1 [[ ! -e $refFA ]] echo [---$SN---] ($(date)) FAILED: file $refFA does not exist exit 1 [[ -z ${refGTF+x} ]] echo FAILED: refGTF is not set echo $USAGE exit 1 [[ ! -e $refGTF ]] echo [---$SN---] ($(date)) FAILED: file $refGTF does not exist exit 1 [[ -z ${sampid+x} ]] echo FAILED: sampid is not set echo $USAGE exit 1 #--- Set outdirectory [[ -z ${outdir+x} ]] outdir=$(dirname $raw1)/$SN mkdir -p $outdir #--- Determine CPUs to use # First examines NCPU environment variable, then nproc, finally sets to 1 [[ -n $NCPU ]] ncpu=$NCPU [[ -z $ncpu ]] ncpu=$(nproc 2 /dev/null) [[ -z $ncpu ]] ncpu=1 echo [---$SN---] ($(date)) read1: $raw1 echo [---$SN---] ($(date)) read2: $raw2 echo [---$SN---] ($(date)) reference_fasta: $refFA echo [---$SN---] ($(date)) reference_gtf: $refGTF echo [---$SN---] ($(date)) samp_id: $sampid echo [---$SN---] ($(date)) outdir: $outdir echo [---$SN---] ($(date)) num CPU: $ncpu #--- Start the timer t1=$(date + %s ) #### Put haphpipe module scripts here #---Complete job t2=$(date + %s ) diff=$(($t2-$t1)) echo [---$SN---] ($(date)) $(($diff / 60)) minutes and $(($diff % 60)) seconds elapsed. echo [---$SN---] ($(date)) $SN COMPLETE. Part 3B - Sample reads. Now we will begin constructing bash scripts for each module. Each module will follow a similar trend. We will set the stage name. We will echo stage name to terminal. We will check to make sure the files that are the output of the stage are not already present. If they are present, we will skip the stage and continue on (no need to repeat the stage). If the files are not present, we will complete the stage and call the command. If the command completes, we will print to terminal. If command fails, we will print that to terminal and quit the script. Remember, you can find the output file names here . The ouput names for this stage are: sample_1.fastq and sample_2.fastq . We have also decided to subsample the number of reads to 50,000 reads for each sample. Now this in the input for the base haphpipe command for this stage: haphpipe sample_reads\\ --fq1 $raw1\\ --fq2 $raw2\\ --nreads 50000\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} The entire stage's bash script is here: stage= sample_reads echo -e \\n[---$SN---] ($(date)) Stage: $stage # if the sampled files are present, skip this stage. Otherwise, call sample_reads if [[ -e $outdir/sample_1.fastq -e ${outdir}/sample_2.fastq ]]; then echo [---$SN---] ($(date)) EXISTS: $stage sample_1.fastq, sample_2.fastq else # this reads in the sample_reads command and saves it in the variable cmd read -r -d '' cmd EOF haphpipe sample_reads\\ --fq1 $raw1\\ --fq2 $raw2\\ --nreads 50000\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi Part 3C - Trim reads. Remember, you can find the output file names here . The ouput names for this stage are: trimmed_1.fastq and trimmed_2.fastq . Now, because we are doing trimming after the sampled reads module, we need to change the input fastq reads for this module to be the fastq reads output from the previous step (sample reads). Therefore, the input fastq files are named sample_1.fastq and sample_2.fastq . Also remember that these files are now contained in the outdirectory specified by the input, so we have to list the path to the input fastq files. Now this in the input for the base haphpipe command for this stage: haphpipe trim_reads\\ --ncpu $ncpu\\ --fq1 ${outdir}/sample_1.fastq\\ --fq2 ${outdir}/sample_2.fastq\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} The entire stage's bash script is here: stage= trim_reads echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/trimmed_1.fastq -e ${outdir}/trimmed_2.fastq ]]; then echo [---$SN---] ($(date)) EXISTS: $stage trimmed_1.fastq,trimmed_2.fastq else read -r -d '' cmd EOF haphpipe trim_reads\\ --ncpu $ncpu\\ --fq1 ${outdir}/sample_1.fastq\\ --fq2 ${outdir}/sample_2.fastq\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi Part 3D - Error correct reads. Remember, you can find the output file names here . The ouput names for this stage are: corrected_1.fastq and corrected_2.fastq . Now, because we are doing error correction after the trimming module, we need to change the input fastq reads for this module to be the fastq reads output from the previous step (trimmed reads). Therefore, the input fastq files are named trimmed_1.fastq and trimmed_2.fastq . Also remember that these files are now contained in the outdirectory specified by the input (just like the sampled reads in the previous step), so we have to list the path to the input fastq files. Now this in the input for the base haphpipe command for this stage: haphpipe ec_reads\\ --ncpu $ncpu\\ --fq1 ${outdir}/trimmed_1.fastq\\ --fq2 ${outdir}/trimmed_2.fastq\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} The entire stage's bash script is here: stage= ec_reads echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/corrected_1.fastq -e $outdir/corrected_2.fastq ]]; then echo [---$SN---] ($(date)) EXISTS: $stage corrected_1.fastq,corrected_2.fastq else read -r -d '' cmd EOF haphpipe ec_reads\\ --ncpu $ncpu\\ --fq1 ${outdir}/trimmed_1.fastq\\ --fq2 ${outdir}/trimmed_2.fastq\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi Part 3E - De novo assembly. Remember, you can find the output file names here . The ouput name for this stage is denovo_contigs.fna . Now, because we are doing denovo assembly after the error correction module, we need to change the input fastq reads for this module to be the fastq reads output from the previous step (error corrected reads). Therefore, the input fastq files are named corrected_1.fastq and corrected_2.fastq . Also remember that these files are now contained in the outdirectory specified by the input, so we have to list the path to the input fastq files. Finally, remember we want to specify that we do NOT want to do another round of error correction. Now this in the input for the base haphpipe command for this stage: haphpipe assemble_denovo\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --no_error_correction\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} The entire stage's bash script is here: stage= assemble_denovo echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/denovo_contigs.fna ]]; then echo [---$SN---] ($(date)) EXISTS: $stage denovo_contigs.fna else read -r -d '' cmd EOF haphpipe assemble_denovo\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --no_error_correction\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi Part 3F - Assemble scaffold. Remember, you can find the output file names here . The ouput name for this stage is scaffold_assembly.fa . There are other outputs, but we focus on this one for further use in the refinement and finalize modules. Now, because we are doing scaffold assembly after the denovo assembly module, we need to specify that the input file is the output contig file ( denovo_contigs.fna ). Again, remember that this file is now contained in the outdirectory specified by the input, so we have to list the path too. We also have to use our input reference fasta file and input sampleID from the script command. Now this in the input for the base haphpipe command for this stage: haphpipe assemble_scaffold\\ --contigs_fa ${outdir}/denovo_contigs.fna\\ --ref_fa ${refFA}\\ --seqname ${sampid}\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} The entire stage's bash script is here: stage= assemble_scaffold echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/scaffold_assembly.fa ]]; then echo [---$SN---] ($(date)) EXISTS: $stage scaffold_assembly.fa else read -r -d '' cmd EOF haphpipe assemble_scaffold\\ --contigs_fa ${outdir}/denovo_contigs.fna\\ --ref_fa ${refFA}\\ --seqname ${sampid}\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi Part 3G - Refine assembly. Remember, you can find the output file names here . The ouput name for this stage is refined.fna . Now, because we are doing refining the assembly after the scaffold assembly module, we need to change the input fastq reads for this module to be the fastq reads output from the error correction step (error corrected reads). Therefore, the input fastq files are named corrected_1.fastq and corrected_2.fastq . Also remember that these files are now contained in the outdirectory specified by the input, so we have to list the path to the input fastq files. Finally, we need to specify that the input reference file is the scaffold_assembly.fa file from the previous scaffold assembly step (above), which is also located in the outdirectory. Now this in the input for the base haphpipe command for this stage: hp_refine_assembly\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --ref_fa ${outdir}/scaffold_assembly.fa\\ --sample_id ${sampid}\\ --max_step 5\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} The entire stage's bash script is here: stage= refine_assembly echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e ${outdir}/refined.fna ]]; then echo [---$SN---] ($(date)) EXISTS: $stage refined.fna else read -r -d '' cmd EOF hp_refine_assembly\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --ref_fa ${outdir}/scaffold_assembly.fa\\ --sample_id ${sampid}\\ --max_step 5\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi Part 3H - Finalize assembly. Remember, you can find the output file names here . The ouput files for this stage are: final.fna , final.bam , final.vcf.gz . Now, because we are doing finalizing the assembly after the refinement module, we need to change the input fastq reads for this module to be the fastq reads output from the error correction step (error corrected reads). Therefore, the input fastq files are named corrected_1.fastq and corrected_2.fastq . Also remember that these files are now contained in the outdirectory specified by the input, so we have to list the path to the input fastq files. Finally, we need to specify that the input reference file is the refined.fna file from the previous refinement step (above), which is also located in the outdirectory. Now this in the input for the base haphpipe command for this stage: hp_finalize_assembly\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --sample_id ${sampid}\\ --ref_fa ${outdir}/refined.fna\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} The entire stage's bash script is here: stage= finalize_assembly echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e ${outdir}/final.fna -e ${outdir}/final.bam -e ${outdir}/final.vcf.gz ]]; then echo [---$SN---] ($(date)) EXISTS: $stage final.fna,final.bam,final.vcf.gz else read -r -d '' cmd EOF hp_finalize_assembly\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --sample_id ${sampid}\\ --ref_fa ${outdir}/refined.fna\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi Part 3I - Gather all the individual module scripts into the final pipeline script. For readable code, we will separate each module with ### like so: ############################################################################### # Step #: description here ############################################################################### insert code here .. Once we concatenate all our code, we end up with this: ############################################################################### # Step 1: Sample Reads ############################################################################### stage= sample_reads echo -e \\n[---$SN---] ($(date)) Stage: $stage # if the sampled files are present, skip this stage. Otherwise, call sample_reads if [[ -e $outdir/sample_1.fastq -e ${outdir}/sample_2.fastq ]]; then echo [---$SN---] ($(date)) EXISTS: $stage sample_1.fastq, sample_2.fastq else # this reads in the sample_reads command and saves it in the variable cmd read -r -d '' cmd EOF haphpipe sample_reads\\ --fq1 $raw1\\ --fq2 $raw2\\ --nreads 50000\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi ############################################################################### # Step 2: Trim Reads ############################################################################### stage= trim_reads echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/trimmed_1.fastq -e ${outdir}/trimmed_2.fastq ]]; then echo [---$SN---] ($(date)) EXISTS: $stage trimmed_1.fastq,trimmed_2.fastq else read -r -d '' cmd EOF haphpipe trim_reads\\ --ncpu $ncpu\\ --fq1 $raw1\\ --fq2 $raw2\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi ############################################################################### # Step 3: Error correction using Spades ############################################################################### stage= ec_reads echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/corrected_1.fastq -e $outdir/corrected_2.fastq ]]; then echo [---$SN---] ($(date)) EXISTS: $stage corrected_1.fastq,corrected_2.fastq else read -r -d '' cmd EOF haphpipe ec_reads\\ --ncpu $ncpu\\ --fq1 ${outdir}/trimmed_1.fastq\\ --fq2 ${outdir}/trimmed_2.fastq\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi ############################################################################### # Step 4: Denovo assembly ############################################################################### stage= assemble_denovo echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/denovo_contigs.fna ]]; then echo [---$SN---] ($(date)) EXISTS: $stage denovo_contigs.fna else read -r -d '' cmd EOF haphpipe assemble_denovo\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --no_error_correction\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi ############################################################################### # Step 5: Scaffold assembly ############################################################################### stage= assemble_scaffold echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/scaffold_assembly.fa ]]; then echo [---$SN---] ($(date)) EXISTS: $stage scaffold_assembly.fa else read -r -d '' cmd EOF haphpipe assemble_scaffold\\ --contigs_fa ${outdir}/denovo_contigs.fna\\ --ref_fa ${refFA}\\ --seqname ${sampid}\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi ############################################################################### # Step 5: Refine assembly ############################################################################### stage= refine_assembly echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e ${outdir}/refined.fna ]]; then echo [---$SN---] ($(date)) EXISTS: $stage refined.fna else read -r -d '' cmd EOF hp_refine_assembly\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --ref_fa ${outdir}/scaffold_assembly.fa\\ --sample_id ${sampid}\\ --max_step 5\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi ############################################################################### # Step 5: Finalize assembly ############################################################################### stage= finalize_assembly echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e ${outdir}/final.fna -e ${outdir}/final.bam -e ${outdir}/final.vcf.gz ]]; then echo [---$SN---] ($(date)) EXISTS: $stage final.fna,final.bam,final.vcf.gz else read -r -d '' cmd EOF hp_finalize_assembly\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --sample_id ${sampid}\\ --ref_fa ${outdir}/refined.fna\\ ${quiet} --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi Step 4 - Combine both the input code and bash scripts for each module into a single script. We named this script covid_genome_assembly.sh and the entire code is: #!/usr/bin/env bash ############################################################################### # This pipeline implements genome assembly using a denovo approach. Reads are # error-corrected and used to refine the scaffolded assembly, with up to 3 # refinement steps. This pipeline is used as an example for advanced usage # - making own pipeline in the User Guide. ############################################################################### SN='covid_genome_assembly' read -r -d '' USAGE EOF USAGE: $SN [read1] [read2] [reference_fasta] [samp_id] outdir ----- COVID19 Genome Assembly Pipeline ----- This pipeline implements genome assembly using a denovo approach. Reads are error-corrected and used to refine the scaffolded assembly, with up to 3 refinement steps. This pipeline is used as an example for advanced usage - making own pipeline in the User Guide. Input: read1: Fastq file for read 1. May be compressed (.gz) read2: Fastq file for read 2. May be compressed (.gz) reference_fasta: Reference sequence (fasta) samp_id: Sample ID outdir: Output directory (default is [sample_dir]/$SN) EOF #--- Read command line args and if arg is -h, provide the usage information [[ -n $1 ]] [[ $1 == '-h' ]] echo $USAGE exit 0 #--- Read command line args [[ -n $1 ]] raw1= $1 [[ -n $2 ]] raw2= $2 [[ -n $3 ]] refFA= $3 [[ -n $4 ]] sampid= $4 [[ -n $5 ]] outdir= $5 #--- Check that files are provided and exist [[ -z ${raw1+x} ]] echo FAILED: read1 is not set echo $USAGE exit 1 [[ ! -e $raw1 ]] echo [---$SN---] ($(date)) FAILED: file $raw1 does not exist exit 1 [[ -z ${raw2+x} ]] echo FAILED: read2 is not set echo $USAGE exit 1 [[ ! -e $raw2 ]] echo [---$SN---] ($(date)) FAILED: file $raw2 does not exist exit 1 [[ -z ${refFA+x} ]] echo FAILED: refFA is not set echo $USAGE exit 1 [[ ! -e $refFA ]] echo [---$SN---] ($(date)) FAILED: file $refFA does not exist exit 1 [[ -z ${sampid+x} ]] echo FAILED: sampid is not set echo $USAGE exit 1 #--- Set outdirectory [[ -z ${outdir+x} ]] outdir=$(dirname $raw1)/$SN mkdir -p $outdir #--- Determine CPUs to use # First examines NCPU environment variable, then nproc, finally sets to 1 [[ -n $NCPU ]] ncpu=$NCPU [[ -z $ncpu ]] ncpu=$(nproc 2 /dev/null) [[ -z $ncpu ]] ncpu=1 echo [---$SN---] ($(date)) read1: $raw1 echo [---$SN---] ($(date)) read2: $raw2 echo [---$SN---] ($(date)) reference_fasta: $refFA echo [---$SN---] ($(date)) samp_id: $sampid echo [---$SN---] ($(date)) outdir: $outdir echo [---$SN---] ($(date)) num CPU: $ncpu #--- Start the timer t1=$(date + %s ) ############################################################################### # Step 1: Sample Reads ############################################################################### stage= sample_reads echo -e \\n[---$SN---] ($(date)) Stage: $stage # if the sampled files are present, skip this stage. Otherwise, call sample_reads if [[ -e $outdir/sample_1.fastq -e ${outdir}/sample_2.fastq ]]; then echo [---$SN---] ($(date)) EXISTS: $stage sample_1.fastq, sample_2.fastq else # this reads in the sample_reads command and saves it in the variable cmd read -r -d '' cmd EOF haphpipe sample_reads\\ --fq1 $raw1\\ --fq2 $raw2\\ --nreads 50000\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi ############################################################################### # Step 2: Trim Reads ############################################################################### stage= trim_reads echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/trimmed_1.fastq -e ${outdir}/trimmed_2.fastq ]]; then echo [---$SN---] ($(date)) EXISTS: $stage trimmed_1.fastq,trimmed_2.fastq else read -r -d '' cmd EOF haphpipe trim_reads\\ --ncpu $ncpu\\ --fq1 $raw1\\ --fq2 $raw2\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi ############################################################################### # Step 3: Error correction using Spades ############################################################################### stage= ec_reads echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/corrected_1.fastq -e $outdir/corrected_2.fastq ]]; then echo [---$SN---] ($(date)) EXISTS: $stage corrected_1.fastq,corrected_2.fastq else read -r -d '' cmd EOF haphpipe ec_reads\\ --ncpu $ncpu\\ --fq1 ${outdir}/trimmed_1.fastq\\ --fq2 ${outdir}/trimmed_2.fastq\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi ############################################################################### # Step 4: Denovo assembly ############################################################################### stage= assemble_denovo echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/denovo_contigs.fna ]]; then echo [---$SN---] ($(date)) EXISTS: $stage denovo_contigs.fna else read -r -d '' cmd EOF haphpipe assemble_denovo\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --no_error_correction\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi ############################################################################### # Step 5: Scaffold assembly ############################################################################### stage= assemble_scaffold echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/scaffold_assembly.fa ]]; then echo [---$SN---] ($(date)) EXISTS: $stage scaffold_assembly.fa else read -r -d '' cmd EOF haphpipe assemble_scaffold\\ --contigs_fa ${outdir}/denovo_contigs.fna\\ --ref_fa ${refFA}\\ --seqname ${sampid}\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi ############################################################################### # Step 5: Refine assembly ############################################################################### stage= refine_assembly echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e ${outdir}/refined.fna ]]; then echo [---$SN---] ($(date)) EXISTS: $stage refined.fna else read -r -d '' cmd EOF hp_refine_assembly\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --ref_fa ${outdir}/scaffold_assembly.fa\\ --sample_id ${sampid}\\ --max_step 5\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi ############################################################################### # Step 5: Finalize assembly ############################################################################### stage= finalize_assembly echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e ${outdir}/final.fna -e ${outdir}/final.bam -e ${outdir}/final.vcf.gz ]]; then echo [---$SN---] ($(date)) EXISTS: $stage final.fna,final.bam,final.vcf.gz else read -r -d '' cmd EOF hp_finalize_assembly\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --sample_id ${sampid}\\ --ref_fa ${outdir}/refined.fna\\ ${quiet} --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi #### Put haphpipe module scripts here #---Complete job t2=$(date + %s ) diff=$(($t2-$t1)) echo [---$SN---] ($(date)) $(($diff / 60)) minutes and $(($diff % 60)) seconds elapsed. echo [---$SN---] ($(date)) $SN COMPLETE. Step 5 - Executing the script. Example to run: bash covid_genome_assembly.sh SRR11140750/SRR11140750_1.fastq SRR11140750/SRR11140750_2.fastq SARSCoV2.NC_045512.COVID19.fasta SRR11140750 Example to run in a loop over all the samples: for sra in SRR11140744 SRR11140746 SRR11140748 SRR11140750; do bash covid_genome_assembly.sh ${sra}/${sra}_1.fastq ${sra}/${sra}_2.fastq SARSCoV2.NC_045512.COVID19.fasta ${sra} covid_genome_assembly done Directories should look like such after running this script: . \u251c\u2500\u2500 SRR11140744 | \u251c\u2500\u2500 SRR11140744_1.fastq | \u251c\u2500\u2500 SRR11140744_2.fastq | \u2514\u2500\u2500 covid_genome_assembly | \u251c\u2500\u2500 corrected_1.fastq | \u251c\u2500\u2500 corrected_2.fastq | \u251c\u2500\u2500 corrected_U.fastq | \u251c\u2500\u2500 denovo_contigs.fna | \u251c\u2500\u2500 denovo_summary.txt | \u251c\u2500\u2500 haphpipe.out | \u251c\u2500\u2500 final.bam | \u251c\u2500\u2500 final.bam.bai | \u251c\u2500\u2500 final_bt2.out | \u251c\u2500\u2500 final.fna | \u251c\u2500\u2500 final.vcf.gz | \u251c\u2500\u2500 final.vcf.gz.tbi | \u251c\u2500\u2500 refined.01.fna | \u251c\u2500\u2500 refined_bt2.01.out | \u251c\u2500\u2500 refined.fna | \u251c\u2500\u2500 refined_bt2.out | \u251c\u2500\u2500 refined_summary.out | \u251c\u2500\u2500 sample_1.fastq | \u251c\u2500\u2500 sample_2.fastq | \u251c\u2500\u2500 scaffold_aligned.fa | \u251c\u2500\u2500 scaffold_assembly.fa | \u251c\u2500\u2500 scaffold_imputed.fa | \u251c\u2500\u2500 scaffold_padded.out | \u251c\u2500\u2500 trimmed_1.fastq | \u251c\u2500\u2500 trimmed_2.fastq | \u251c\u2500\u2500 trimmed_U.fastq | \u2514\u2500\u2500 trimmomatic_summary.out \u251c\u2500\u2500 SRR11140746 | \u251c\u2500\u2500 SRR11140746_1.fastq | \u251c\u2500\u2500 SRR11140746_2.fastq | \u2514\u2500\u2500 covid_genome_assembly ....","title":"Advanced Users"},{"location":"adv/#making-your-own-pipelines-with-bash","text":"This is an example of how to make your own pipeline with bash. This example uses four NGS samples for COVID-19 from NCBI and puts them through whole genome de novo assembly. SRA accession numbers: SRR11140744 SRR11140746 SRR11140748 SRR11140750 Step 0 - Obtaining samples. We will assume that you have downloaded the reads from NCBI or know how to use fastq-dump. We will show the fastq-dump command here that we used to download the files. This part is not included in the pipeline. for sra in SRR11140744 SRR11140746 SRR11140748 SRR11140750; do fastq-dump --outdir ${sra} --split-files --origfmt ${sra} done Output is: Read 503344 spots for SRR11140744 Written 503344 spots for SRR11140744 Read 358971 spots for SRR11140746 Written 358971 spots for SRR11140746 Read 421395 spots for SRR11140748 Written 421395 spots for SRR11140748 Read 17657 spots for SRR11140750 Written 17657 spots for SRR11140750 You're starting directory should look like for this example, whether the reads were obtained manually or downloaded with fastq-dump: . \u251c\u2500\u2500 SRR11140744 | \u251c\u2500\u2500 SRR11140744_1.fastq | \u2514\u2500\u2500 SRR11140744_2.fastq \u251c\u2500\u2500 SRR11140746 | \u251c\u2500\u2500 SRR11140746_1.fastq | \u2514\u2500\u2500 SRR11140746_2.fastq \u251c\u2500\u2500 SRR11140748 | \u251c\u2500\u2500 SRR11140748_1.fastq | \u2514\u2500\u2500 SRR11140748_2.fastq \u2514\u2500\u2500 SRR11140750 \u251c\u2500\u2500 SRR11140750_1.fastq \u2514\u2500\u2500 SRR11140750_2.fastq We also know we will need a reference genome to help scaffold the contigs. We have downloaded the COVID19 reference genome here as a FASTA file. Step 1 - Evaluate which modules you want to use. View all the module options using haphpipe -h . We have decided that we want to sample the reads ( sample_reads ), trim the reads ( trim_reads ), error correct the reads ( ec_reads ). We want to do genome de novo assembly, so we want to use assemble_denovo and assemble_scaffold . We then want to do refinement of the assembly ( refine_assembly ) and finalize the assembly ( finaliza_assembly ). Step 2 - Document files and options needed for each module. As we go through each module, we will take note of what we need and which input is specific for an individual sample (i.e., the input fastq reads will be different per sample, but we probably want the same number of reads for each sample.) We know all modules have the option for a logfile. We'll include that in the bash scripting, but don't need to make notes of a logfile for each module. Part 2A - Sample reads. Upon viewing the haphpipe sample_reads -h we see that we need to provide fastq reads 1 and 2, an output directory, and number of reads desired. Sample specific options: fastq read 1 fastq read 2 output directory Not sample specific options: number of reads desired Part 2B - Trim reads. Upon viewing the haphpipe trim_reads -h we see that we need to provide fastq reads 1 and 2, an output directory, number of reads, and number of CPUs desired. There is an option to change the timming commands, but we'll keep default. Sample specific options: fastq read 1 -- we will want these to be the subsampled read file fastq read 2 -- we will want these to be the subsampled read file output directory Not sample specific options: number of reads desired ncpu Part 2C - Error correct reads. Upon viewing the haphpipe ec_reads -h we see that we need to provide fastq reads 1 and 2, an output directory, and number of CPUs desired. Sample specific options: fastq read 1 -- we will want these to be the trimmed read file fastq read 2 -- we will want these to be the trimmed read file output directory Not sample specific options: ncpu Part 2D - De novo assembly. Upon viewing the haphpipe assemble_denovo -h we see that we need to provide fastq reads 1 and 2, an output directory, and number of CPUs desired. Because we previously completed error correction. We will not want to include it here (i.e., we will invoke the option command --no_error_correction ). Sample specific options: fastq read 1 -- we will want these to be the error corrected reads fastq read 2 -- we will want these to be the error corrected reads output directory Not sample specific options: ncpu no error correction option Part 2E - Assemble scaffold. Upon viewing the haphpipe assemble_scaffold -h we see that we need to provide fasta file containing assembled contigs, an output directory, name to append to scaffold sequence and reference fasta desired. There is an option to change the timming commands, but we'll keep default. Sample specific options: assembled contigs file output directory sequence name Not sample specific options: reference fasta Part 2F - Refine assembly. Upon viewing the haphpipe refine_assembly -h we see that we need to provide fastq reads 1 and 2, an output directory, a reference sequence to refine, a sample ID and a maximum number of refinement steps, and number of CPUs desired. Sample specific options: fastq read 1 -- we will want these to be the error corrected reads fastq read 2 -- we will want these to be the error corrected reads output directory reference fasta -- we will want this to be the sample's assembled scaffold Sample ID Not sample specific options: ncpu maximum number of refinement steps (we'll do 3 for sake of simplicity and time) Part 2G - Finalize assembly. Upon viewing the haphpipe finalize_assembly -h we see that we need to provide fastq reads 1 and 2, an output directory, a reference sequence to finalize, a sample ID and a maximum number of refinement steps, and number of CPUs desired. We could replace the preset bowtie2 option, but we will leave it as default for this sample pipeline. Sample specific options: fastq read 1 -- we will want these to be the error corrected reads fastq read 2 -- we will want these to be the error corrected reads output directory reference fasta -- we will want this to be the sample's refined fasta sequence Sample ID Not sample specific options: ncpu Part 2H - Gather the needed initial input for each sample. We need to gather the necessary files that we will need to input for each sample so that we can make a script that takes in input files. We know from the User Guide that we can look at the output/input file types and names here . Because each module has a standard output file name, we will only need to be specific about the input for the raw fastq files for each sample. Therefore we need: input raw fastq read 1 input raw fastq read 2 reference genome to scaffold against (this will be a covid19 reference) output directory for each sample sample name The other options we can code into the bash script for each module, since they will be the same for every sample in this analysis. Step 3 - Create a bash script for each module. Now we will format a bash script for each module. Part 3A - Input options. Because we have a list of needed input options (specified by the user), we need to make a bash command to take in the inputs. First, we will specify the script name. We can do this one of two ways. i) explicitly or 2) through a command. i) SN='covid_genome' # this sets the script name (SN variable) to covid_genome ii) SN=$(basename $0) # this sets the script name (SN variable) to whatever the script filename is. If the script's file name is covid.sh then it is set as that. If the file name is this_is_file it will be set as that. Because we are in charge of this script, we will explicitly set it. Second, we want to set some input information for the user. We can do this as such: read -r -d '' USAGE EOF USAGE: $SN [read1] [read2] [reference_fasta] [samp_id] outdir ----- COVID19 Genome Assembly Pipeline ----- This pipeline implements genome assembly using a denovo approach. Reads are error-corrected and used to refine the scaffolded assembly, with up to 3 refinement steps. This pipeline is used as an example for advanced usage - making own pipeline in the User Guide. Input: read1: Fastq file for read 1. May be compressed (.gz) read2: Fastq file for read 2. May be compressed (.gz) reference_fasta: Reference sequence (fasta) samp_id: Sample ID outdir: Output directory (default is [sample_dir]/$SN) EOF Third, we want to provide help information for the script. We can do that using the avoce code, which has been saved in the variable $USAGE . Therefore, #--- Read command line args and if arg is -h, provide the usage information [[ -n $1 ]] [[ $1 == '-h' ]] echo $USAGE exit 0 If pipeline.sh -h is invoked here, then the output is: USAGE: covid_genome_assembly [read1] [read2] [reference_fasta] [samp_id] outdir ----- COVID19 Genome Assembly Pipeline ----- This pipeline implements genome assembly using a denovo approach. Reads are error-corrected and used to refine the scaffolded assembly, with up to 3 refinement steps. This pipeline is used as an example for advanced usage - making own pipeline in the User Guide. Input: read1: Fastq file for read 1. May be compressed (.gz) read2: Fastq file for read 2. May be compressed (.gz) reference_fasta: Reference sequence (fasta) samp_id: Sample ID outdir: Output directory (default is [sample_dir]/covid_genome_assembly) Note that this output looks identical to the information we put into $USAGE variable above in the second part of this section. Fourth, we want to read in the command with the provided input options. Because we are using a bash script, the position of the input files is imparative. #--- Read command line args [[ -n $1 ]] raw1= $1 [[ -n $2 ]] raw2= $2 [[ -n $3 ]] refFA= $3 [[ -n $4 ]] sampid= $4 [[ -n $5 ]] outdir= $5 Fifth, we want to check that the input files are provided and are not empty. We also want to set the outdirectory. #--- Check that files are provided and exist [[ -z ${raw1+x} ]] echo FAILED: read1 is not set echo $USAGE exit 1 [[ ! -e $raw1 ]] echo [---$SN---] ($(date)) FAILED: file $raw1 does not exist exit 1 [[ -z ${raw2+x} ]] echo FAILED: read2 is not set echo $USAGE exit 1 [[ ! -e $raw2 ]] echo [---$SN---] ($(date)) FAILED: file $raw2 does not exist exit 1 [[ -z ${refFA+x} ]] echo FAILED: refFA is not set echo $USAGE exit 1 [[ ! -e $refFA ]] echo [---$SN---] ($(date)) FAILED: file $refFA does not exist exit 1 [[ -z ${sampid+x} ]] echo FAILED: sampid is not set echo $USAGE exit 1 #--- Set outdirectory [[ -z ${outdir+x} ]] outdir=$(dirname $raw1)/$SN mkdir -p $outdir Sixth, we want to set the number of CPUs to use throughout the script. #--- Determine CPUs to use # First examines NCPU environment variable, then nproc, finally sets to 1 [[ -n $NCPU ]] ncpu=$NCPU [[ -z $ncpu ]] ncpu=$(nproc 2 /dev/null) [[ -z $ncpu ]] ncpu=1 Seventh, we want to print out the variables and files. echo [---$SN---] ($(date)) read1: $raw1 echo [---$SN---] ($(date)) read2: $raw2 echo [---$SN---] ($(date)) reference_fasta: $refFA echo [---$SN---] ($(date)) samp_id: $sampid echo [---$SN---] ($(date)) outdir: $outdir echo [---$SN---] ($(date)) num CPU: $ncpu Finally, because here at GWU CBI, we like to time everything, we include a line of code to start a timer and end the timer: #--- Start the timer t1=$(date + %s ) #### Put haphpipe module scripts here #---Complete job t2=$(date + %s ) diff=$(($t2-$t1)) echo [---$SN---] ($(date)) $(($diff / 60)) minutes and $(($diff % 60)) seconds elapsed. echo [---$SN---] ($(date)) $SN COMPLETE. After Part 3A, our pipeline file should read as such: #!/usr/bin/env bash ############################################################################### # This pipeline implements genome assembly using a denovo approach. Reads are # error-corrected and used to refine the scaffolded assembly, with up to 3 # refinement steps. This pipeline is used as an example for advanced usage # - making own pipeline in the User Guide. ############################################################################### SN='covid_genome_assembly' read -r -d '' USAGE EOF USAGE: $SN [read1] [read2] [reference_fasta] [samp_id] outdir ----- COVID19 Genome Assembly Pipeline ----- This pipeline implements genome assembly using a denovo approach. Reads are error-corrected and used to refine the scaffolded assembly, with up to 3 refinement steps. This pipeline is used as an example for advanced usage - making own pipeline in the User Guide. Input: read1: Fastq file for read 1. May be compressed (.gz) read2: Fastq file for read 2. May be compressed (.gz) reference_fasta: Reference sequence (fasta) samp_id: Sample ID outdir: Output directory (default is [sample_dir]/$SN) EOF #--- Read command line args and if arg is -h, provide the usage information [[ -n $1 ]] [[ $1 == '-h' ]] echo $USAGE exit 0 #--- Read command line args [[ -n $1 ]] raw1= $1 [[ -n $2 ]] raw2= $2 [[ -n $3 ]] refFA= $3 [[ -n $4 ]] refGTF= $4 [[ -n $5 ]] sampid= $5 [[ -n $6 ]] outdir= $6 #--- Check that files are provided and exist [[ -z ${raw1+x} ]] echo FAILED: read1 is not set echo $USAGE exit 1 [[ ! -e $raw1 ]] echo [---$SN---] ($(date)) FAILED: file $raw1 does not exist exit 1 [[ -z ${raw2+x} ]] echo FAILED: read2 is not set echo $USAGE exit 1 [[ ! -e $raw2 ]] echo [---$SN---] ($(date)) FAILED: file $raw2 does not exist exit 1 [[ -z ${refFA+x} ]] echo FAILED: refFA is not set echo $USAGE exit 1 [[ ! -e $refFA ]] echo [---$SN---] ($(date)) FAILED: file $refFA does not exist exit 1 [[ -z ${refGTF+x} ]] echo FAILED: refGTF is not set echo $USAGE exit 1 [[ ! -e $refGTF ]] echo [---$SN---] ($(date)) FAILED: file $refGTF does not exist exit 1 [[ -z ${sampid+x} ]] echo FAILED: sampid is not set echo $USAGE exit 1 #--- Set outdirectory [[ -z ${outdir+x} ]] outdir=$(dirname $raw1)/$SN mkdir -p $outdir #--- Determine CPUs to use # First examines NCPU environment variable, then nproc, finally sets to 1 [[ -n $NCPU ]] ncpu=$NCPU [[ -z $ncpu ]] ncpu=$(nproc 2 /dev/null) [[ -z $ncpu ]] ncpu=1 echo [---$SN---] ($(date)) read1: $raw1 echo [---$SN---] ($(date)) read2: $raw2 echo [---$SN---] ($(date)) reference_fasta: $refFA echo [---$SN---] ($(date)) reference_gtf: $refGTF echo [---$SN---] ($(date)) samp_id: $sampid echo [---$SN---] ($(date)) outdir: $outdir echo [---$SN---] ($(date)) num CPU: $ncpu #--- Start the timer t1=$(date + %s ) #### Put haphpipe module scripts here #---Complete job t2=$(date + %s ) diff=$(($t2-$t1)) echo [---$SN---] ($(date)) $(($diff / 60)) minutes and $(($diff % 60)) seconds elapsed. echo [---$SN---] ($(date)) $SN COMPLETE. Part 3B - Sample reads. Now we will begin constructing bash scripts for each module. Each module will follow a similar trend. We will set the stage name. We will echo stage name to terminal. We will check to make sure the files that are the output of the stage are not already present. If they are present, we will skip the stage and continue on (no need to repeat the stage). If the files are not present, we will complete the stage and call the command. If the command completes, we will print to terminal. If command fails, we will print that to terminal and quit the script. Remember, you can find the output file names here . The ouput names for this stage are: sample_1.fastq and sample_2.fastq . We have also decided to subsample the number of reads to 50,000 reads for each sample. Now this in the input for the base haphpipe command for this stage: haphpipe sample_reads\\ --fq1 $raw1\\ --fq2 $raw2\\ --nreads 50000\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} The entire stage's bash script is here: stage= sample_reads echo -e \\n[---$SN---] ($(date)) Stage: $stage # if the sampled files are present, skip this stage. Otherwise, call sample_reads if [[ -e $outdir/sample_1.fastq -e ${outdir}/sample_2.fastq ]]; then echo [---$SN---] ($(date)) EXISTS: $stage sample_1.fastq, sample_2.fastq else # this reads in the sample_reads command and saves it in the variable cmd read -r -d '' cmd EOF haphpipe sample_reads\\ --fq1 $raw1\\ --fq2 $raw2\\ --nreads 50000\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi Part 3C - Trim reads. Remember, you can find the output file names here . The ouput names for this stage are: trimmed_1.fastq and trimmed_2.fastq . Now, because we are doing trimming after the sampled reads module, we need to change the input fastq reads for this module to be the fastq reads output from the previous step (sample reads). Therefore, the input fastq files are named sample_1.fastq and sample_2.fastq . Also remember that these files are now contained in the outdirectory specified by the input, so we have to list the path to the input fastq files. Now this in the input for the base haphpipe command for this stage: haphpipe trim_reads\\ --ncpu $ncpu\\ --fq1 ${outdir}/sample_1.fastq\\ --fq2 ${outdir}/sample_2.fastq\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} The entire stage's bash script is here: stage= trim_reads echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/trimmed_1.fastq -e ${outdir}/trimmed_2.fastq ]]; then echo [---$SN---] ($(date)) EXISTS: $stage trimmed_1.fastq,trimmed_2.fastq else read -r -d '' cmd EOF haphpipe trim_reads\\ --ncpu $ncpu\\ --fq1 ${outdir}/sample_1.fastq\\ --fq2 ${outdir}/sample_2.fastq\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi Part 3D - Error correct reads. Remember, you can find the output file names here . The ouput names for this stage are: corrected_1.fastq and corrected_2.fastq . Now, because we are doing error correction after the trimming module, we need to change the input fastq reads for this module to be the fastq reads output from the previous step (trimmed reads). Therefore, the input fastq files are named trimmed_1.fastq and trimmed_2.fastq . Also remember that these files are now contained in the outdirectory specified by the input (just like the sampled reads in the previous step), so we have to list the path to the input fastq files. Now this in the input for the base haphpipe command for this stage: haphpipe ec_reads\\ --ncpu $ncpu\\ --fq1 ${outdir}/trimmed_1.fastq\\ --fq2 ${outdir}/trimmed_2.fastq\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} The entire stage's bash script is here: stage= ec_reads echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/corrected_1.fastq -e $outdir/corrected_2.fastq ]]; then echo [---$SN---] ($(date)) EXISTS: $stage corrected_1.fastq,corrected_2.fastq else read -r -d '' cmd EOF haphpipe ec_reads\\ --ncpu $ncpu\\ --fq1 ${outdir}/trimmed_1.fastq\\ --fq2 ${outdir}/trimmed_2.fastq\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi Part 3E - De novo assembly. Remember, you can find the output file names here . The ouput name for this stage is denovo_contigs.fna . Now, because we are doing denovo assembly after the error correction module, we need to change the input fastq reads for this module to be the fastq reads output from the previous step (error corrected reads). Therefore, the input fastq files are named corrected_1.fastq and corrected_2.fastq . Also remember that these files are now contained in the outdirectory specified by the input, so we have to list the path to the input fastq files. Finally, remember we want to specify that we do NOT want to do another round of error correction. Now this in the input for the base haphpipe command for this stage: haphpipe assemble_denovo\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --no_error_correction\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} The entire stage's bash script is here: stage= assemble_denovo echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/denovo_contigs.fna ]]; then echo [---$SN---] ($(date)) EXISTS: $stage denovo_contigs.fna else read -r -d '' cmd EOF haphpipe assemble_denovo\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --no_error_correction\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi Part 3F - Assemble scaffold. Remember, you can find the output file names here . The ouput name for this stage is scaffold_assembly.fa . There are other outputs, but we focus on this one for further use in the refinement and finalize modules. Now, because we are doing scaffold assembly after the denovo assembly module, we need to specify that the input file is the output contig file ( denovo_contigs.fna ). Again, remember that this file is now contained in the outdirectory specified by the input, so we have to list the path too. We also have to use our input reference fasta file and input sampleID from the script command. Now this in the input for the base haphpipe command for this stage: haphpipe assemble_scaffold\\ --contigs_fa ${outdir}/denovo_contigs.fna\\ --ref_fa ${refFA}\\ --seqname ${sampid}\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} The entire stage's bash script is here: stage= assemble_scaffold echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/scaffold_assembly.fa ]]; then echo [---$SN---] ($(date)) EXISTS: $stage scaffold_assembly.fa else read -r -d '' cmd EOF haphpipe assemble_scaffold\\ --contigs_fa ${outdir}/denovo_contigs.fna\\ --ref_fa ${refFA}\\ --seqname ${sampid}\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi Part 3G - Refine assembly. Remember, you can find the output file names here . The ouput name for this stage is refined.fna . Now, because we are doing refining the assembly after the scaffold assembly module, we need to change the input fastq reads for this module to be the fastq reads output from the error correction step (error corrected reads). Therefore, the input fastq files are named corrected_1.fastq and corrected_2.fastq . Also remember that these files are now contained in the outdirectory specified by the input, so we have to list the path to the input fastq files. Finally, we need to specify that the input reference file is the scaffold_assembly.fa file from the previous scaffold assembly step (above), which is also located in the outdirectory. Now this in the input for the base haphpipe command for this stage: hp_refine_assembly\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --ref_fa ${outdir}/scaffold_assembly.fa\\ --sample_id ${sampid}\\ --max_step 5\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} The entire stage's bash script is here: stage= refine_assembly echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e ${outdir}/refined.fna ]]; then echo [---$SN---] ($(date)) EXISTS: $stage refined.fna else read -r -d '' cmd EOF hp_refine_assembly\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --ref_fa ${outdir}/scaffold_assembly.fa\\ --sample_id ${sampid}\\ --max_step 5\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi Part 3H - Finalize assembly. Remember, you can find the output file names here . The ouput files for this stage are: final.fna , final.bam , final.vcf.gz . Now, because we are doing finalizing the assembly after the refinement module, we need to change the input fastq reads for this module to be the fastq reads output from the error correction step (error corrected reads). Therefore, the input fastq files are named corrected_1.fastq and corrected_2.fastq . Also remember that these files are now contained in the outdirectory specified by the input, so we have to list the path to the input fastq files. Finally, we need to specify that the input reference file is the refined.fna file from the previous refinement step (above), which is also located in the outdirectory. Now this in the input for the base haphpipe command for this stage: hp_finalize_assembly\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --sample_id ${sampid}\\ --ref_fa ${outdir}/refined.fna\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} The entire stage's bash script is here: stage= finalize_assembly echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e ${outdir}/final.fna -e ${outdir}/final.bam -e ${outdir}/final.vcf.gz ]]; then echo [---$SN---] ($(date)) EXISTS: $stage final.fna,final.bam,final.vcf.gz else read -r -d '' cmd EOF hp_finalize_assembly\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --sample_id ${sampid}\\ --ref_fa ${outdir}/refined.fna\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi Part 3I - Gather all the individual module scripts into the final pipeline script. For readable code, we will separate each module with ### like so: ############################################################################### # Step #: description here ############################################################################### insert code here .. Once we concatenate all our code, we end up with this: ############################################################################### # Step 1: Sample Reads ############################################################################### stage= sample_reads echo -e \\n[---$SN---] ($(date)) Stage: $stage # if the sampled files are present, skip this stage. Otherwise, call sample_reads if [[ -e $outdir/sample_1.fastq -e ${outdir}/sample_2.fastq ]]; then echo [---$SN---] ($(date)) EXISTS: $stage sample_1.fastq, sample_2.fastq else # this reads in the sample_reads command and saves it in the variable cmd read -r -d '' cmd EOF haphpipe sample_reads\\ --fq1 $raw1\\ --fq2 $raw2\\ --nreads 50000\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi ############################################################################### # Step 2: Trim Reads ############################################################################### stage= trim_reads echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/trimmed_1.fastq -e ${outdir}/trimmed_2.fastq ]]; then echo [---$SN---] ($(date)) EXISTS: $stage trimmed_1.fastq,trimmed_2.fastq else read -r -d '' cmd EOF haphpipe trim_reads\\ --ncpu $ncpu\\ --fq1 $raw1\\ --fq2 $raw2\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi ############################################################################### # Step 3: Error correction using Spades ############################################################################### stage= ec_reads echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/corrected_1.fastq -e $outdir/corrected_2.fastq ]]; then echo [---$SN---] ($(date)) EXISTS: $stage corrected_1.fastq,corrected_2.fastq else read -r -d '' cmd EOF haphpipe ec_reads\\ --ncpu $ncpu\\ --fq1 ${outdir}/trimmed_1.fastq\\ --fq2 ${outdir}/trimmed_2.fastq\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi ############################################################################### # Step 4: Denovo assembly ############################################################################### stage= assemble_denovo echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/denovo_contigs.fna ]]; then echo [---$SN---] ($(date)) EXISTS: $stage denovo_contigs.fna else read -r -d '' cmd EOF haphpipe assemble_denovo\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --no_error_correction\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi ############################################################################### # Step 5: Scaffold assembly ############################################################################### stage= assemble_scaffold echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/scaffold_assembly.fa ]]; then echo [---$SN---] ($(date)) EXISTS: $stage scaffold_assembly.fa else read -r -d '' cmd EOF haphpipe assemble_scaffold\\ --contigs_fa ${outdir}/denovo_contigs.fna\\ --ref_fa ${refFA}\\ --seqname ${sampid}\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi ############################################################################### # Step 5: Refine assembly ############################################################################### stage= refine_assembly echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e ${outdir}/refined.fna ]]; then echo [---$SN---] ($(date)) EXISTS: $stage refined.fna else read -r -d '' cmd EOF hp_refine_assembly\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --ref_fa ${outdir}/scaffold_assembly.fa\\ --sample_id ${sampid}\\ --max_step 5\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi ############################################################################### # Step 5: Finalize assembly ############################################################################### stage= finalize_assembly echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e ${outdir}/final.fna -e ${outdir}/final.bam -e ${outdir}/final.vcf.gz ]]; then echo [---$SN---] ($(date)) EXISTS: $stage final.fna,final.bam,final.vcf.gz else read -r -d '' cmd EOF hp_finalize_assembly\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --sample_id ${sampid}\\ --ref_fa ${outdir}/refined.fna\\ ${quiet} --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi Step 4 - Combine both the input code and bash scripts for each module into a single script. We named this script covid_genome_assembly.sh and the entire code is: #!/usr/bin/env bash ############################################################################### # This pipeline implements genome assembly using a denovo approach. Reads are # error-corrected and used to refine the scaffolded assembly, with up to 3 # refinement steps. This pipeline is used as an example for advanced usage # - making own pipeline in the User Guide. ############################################################################### SN='covid_genome_assembly' read -r -d '' USAGE EOF USAGE: $SN [read1] [read2] [reference_fasta] [samp_id] outdir ----- COVID19 Genome Assembly Pipeline ----- This pipeline implements genome assembly using a denovo approach. Reads are error-corrected and used to refine the scaffolded assembly, with up to 3 refinement steps. This pipeline is used as an example for advanced usage - making own pipeline in the User Guide. Input: read1: Fastq file for read 1. May be compressed (.gz) read2: Fastq file for read 2. May be compressed (.gz) reference_fasta: Reference sequence (fasta) samp_id: Sample ID outdir: Output directory (default is [sample_dir]/$SN) EOF #--- Read command line args and if arg is -h, provide the usage information [[ -n $1 ]] [[ $1 == '-h' ]] echo $USAGE exit 0 #--- Read command line args [[ -n $1 ]] raw1= $1 [[ -n $2 ]] raw2= $2 [[ -n $3 ]] refFA= $3 [[ -n $4 ]] sampid= $4 [[ -n $5 ]] outdir= $5 #--- Check that files are provided and exist [[ -z ${raw1+x} ]] echo FAILED: read1 is not set echo $USAGE exit 1 [[ ! -e $raw1 ]] echo [---$SN---] ($(date)) FAILED: file $raw1 does not exist exit 1 [[ -z ${raw2+x} ]] echo FAILED: read2 is not set echo $USAGE exit 1 [[ ! -e $raw2 ]] echo [---$SN---] ($(date)) FAILED: file $raw2 does not exist exit 1 [[ -z ${refFA+x} ]] echo FAILED: refFA is not set echo $USAGE exit 1 [[ ! -e $refFA ]] echo [---$SN---] ($(date)) FAILED: file $refFA does not exist exit 1 [[ -z ${sampid+x} ]] echo FAILED: sampid is not set echo $USAGE exit 1 #--- Set outdirectory [[ -z ${outdir+x} ]] outdir=$(dirname $raw1)/$SN mkdir -p $outdir #--- Determine CPUs to use # First examines NCPU environment variable, then nproc, finally sets to 1 [[ -n $NCPU ]] ncpu=$NCPU [[ -z $ncpu ]] ncpu=$(nproc 2 /dev/null) [[ -z $ncpu ]] ncpu=1 echo [---$SN---] ($(date)) read1: $raw1 echo [---$SN---] ($(date)) read2: $raw2 echo [---$SN---] ($(date)) reference_fasta: $refFA echo [---$SN---] ($(date)) samp_id: $sampid echo [---$SN---] ($(date)) outdir: $outdir echo [---$SN---] ($(date)) num CPU: $ncpu #--- Start the timer t1=$(date + %s ) ############################################################################### # Step 1: Sample Reads ############################################################################### stage= sample_reads echo -e \\n[---$SN---] ($(date)) Stage: $stage # if the sampled files are present, skip this stage. Otherwise, call sample_reads if [[ -e $outdir/sample_1.fastq -e ${outdir}/sample_2.fastq ]]; then echo [---$SN---] ($(date)) EXISTS: $stage sample_1.fastq, sample_2.fastq else # this reads in the sample_reads command and saves it in the variable cmd read -r -d '' cmd EOF haphpipe sample_reads\\ --fq1 $raw1\\ --fq2 $raw2\\ --nreads 50000\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi ############################################################################### # Step 2: Trim Reads ############################################################################### stage= trim_reads echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/trimmed_1.fastq -e ${outdir}/trimmed_2.fastq ]]; then echo [---$SN---] ($(date)) EXISTS: $stage trimmed_1.fastq,trimmed_2.fastq else read -r -d '' cmd EOF haphpipe trim_reads\\ --ncpu $ncpu\\ --fq1 $raw1\\ --fq2 $raw2\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi ############################################################################### # Step 3: Error correction using Spades ############################################################################### stage= ec_reads echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/corrected_1.fastq -e $outdir/corrected_2.fastq ]]; then echo [---$SN---] ($(date)) EXISTS: $stage corrected_1.fastq,corrected_2.fastq else read -r -d '' cmd EOF haphpipe ec_reads\\ --ncpu $ncpu\\ --fq1 ${outdir}/trimmed_1.fastq\\ --fq2 ${outdir}/trimmed_2.fastq\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi ############################################################################### # Step 4: Denovo assembly ############################################################################### stage= assemble_denovo echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/denovo_contigs.fna ]]; then echo [---$SN---] ($(date)) EXISTS: $stage denovo_contigs.fna else read -r -d '' cmd EOF haphpipe assemble_denovo\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --no_error_correction\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi ############################################################################### # Step 5: Scaffold assembly ############################################################################### stage= assemble_scaffold echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e $outdir/scaffold_assembly.fa ]]; then echo [---$SN---] ($(date)) EXISTS: $stage scaffold_assembly.fa else read -r -d '' cmd EOF haphpipe assemble_scaffold\\ --contigs_fa ${outdir}/denovo_contigs.fna\\ --ref_fa ${refFA}\\ --seqname ${sampid}\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi ############################################################################### # Step 5: Refine assembly ############################################################################### stage= refine_assembly echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e ${outdir}/refined.fna ]]; then echo [---$SN---] ($(date)) EXISTS: $stage refined.fna else read -r -d '' cmd EOF hp_refine_assembly\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --ref_fa ${outdir}/scaffold_assembly.fa\\ --sample_id ${sampid}\\ --max_step 5\\ --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi ############################################################################### # Step 5: Finalize assembly ############################################################################### stage= finalize_assembly echo -e \\n[---$SN---] ($(date)) Stage: $stage if [[ -e ${outdir}/final.fna -e ${outdir}/final.bam -e ${outdir}/final.vcf.gz ]]; then echo [---$SN---] ($(date)) EXISTS: $stage final.fna,final.bam,final.vcf.gz else read -r -d '' cmd EOF hp_finalize_assembly\\ --ncpu $ncpu\\ --fq1 ${outdir}/corrected_1.fastq\\ --fq2 ${outdir}/corrected_2.fastq\\ --sample_id ${sampid}\\ --ref_fa ${outdir}/refined.fna\\ ${quiet} --logfile ${outdir}/haphpipe.out\\ --outdir ${outdir} EOF echo -e [---$SN---] ($(date)) $stage command:\\n\\n$cmd\\n eval $cmd [[ $? -eq 0 ]] echo [---$SN---] ($(date)) COMPLETED: $stage || \\ ( echo [---$SN---] ($(date)) FAILED: $stage exit 1 ) fi #### Put haphpipe module scripts here #---Complete job t2=$(date + %s ) diff=$(($t2-$t1)) echo [---$SN---] ($(date)) $(($diff / 60)) minutes and $(($diff % 60)) seconds elapsed. echo [---$SN---] ($(date)) $SN COMPLETE. Step 5 - Executing the script. Example to run: bash covid_genome_assembly.sh SRR11140750/SRR11140750_1.fastq SRR11140750/SRR11140750_2.fastq SARSCoV2.NC_045512.COVID19.fasta SRR11140750 Example to run in a loop over all the samples: for sra in SRR11140744 SRR11140746 SRR11140748 SRR11140750; do bash covid_genome_assembly.sh ${sra}/${sra}_1.fastq ${sra}/${sra}_2.fastq SARSCoV2.NC_045512.COVID19.fasta ${sra} covid_genome_assembly done Directories should look like such after running this script: . \u251c\u2500\u2500 SRR11140744 | \u251c\u2500\u2500 SRR11140744_1.fastq | \u251c\u2500\u2500 SRR11140744_2.fastq | \u2514\u2500\u2500 covid_genome_assembly | \u251c\u2500\u2500 corrected_1.fastq | \u251c\u2500\u2500 corrected_2.fastq | \u251c\u2500\u2500 corrected_U.fastq | \u251c\u2500\u2500 denovo_contigs.fna | \u251c\u2500\u2500 denovo_summary.txt | \u251c\u2500\u2500 haphpipe.out | \u251c\u2500\u2500 final.bam | \u251c\u2500\u2500 final.bam.bai | \u251c\u2500\u2500 final_bt2.out | \u251c\u2500\u2500 final.fna | \u251c\u2500\u2500 final.vcf.gz | \u251c\u2500\u2500 final.vcf.gz.tbi | \u251c\u2500\u2500 refined.01.fna | \u251c\u2500\u2500 refined_bt2.01.out | \u251c\u2500\u2500 refined.fna | \u251c\u2500\u2500 refined_bt2.out | \u251c\u2500\u2500 refined_summary.out | \u251c\u2500\u2500 sample_1.fastq | \u251c\u2500\u2500 sample_2.fastq | \u251c\u2500\u2500 scaffold_aligned.fa | \u251c\u2500\u2500 scaffold_assembly.fa | \u251c\u2500\u2500 scaffold_imputed.fa | \u251c\u2500\u2500 scaffold_padded.out | \u251c\u2500\u2500 trimmed_1.fastq | \u251c\u2500\u2500 trimmed_2.fastq | \u251c\u2500\u2500 trimmed_U.fastq | \u2514\u2500\u2500 trimmomatic_summary.out \u251c\u2500\u2500 SRR11140746 | \u251c\u2500\u2500 SRR11140746_1.fastq | \u251c\u2500\u2500 SRR11140746_2.fastq | \u2514\u2500\u2500 covid_genome_assembly ....","title":"Making your own pipelines with Bash"},{"location":"demos/","text":"Demo Module (add)","title":"Demos"},{"location":"demos/#demo-module","text":"(add)","title":"Demo Module"},{"location":"expipes/","text":"The example pipelines are written in bash scripting language. The reference files used in both examples are included in the demo data. To run in haphpipe, execute one of the following lines: haphpipe_assemble_01 read1.fq.gz read2.fq.gz ../refs/HIV_B.K03455.HXB2.fasta ../refs/HIV_B.K03455.HXB2.gtf sampleID haphpipe_assemble_02 read1.fq.gz read2.fq.gz ../refs/HIV_B.K03455.HXB2.amplicons.fasta sampleID Pipeline 1 implements amplicon assembly using a de novo approach. Reads are error-corrected and used to refine the initial assembly, with up to 5 refinement steps. Pipeline 2 implements amplicon assembly using a reference-based mapping approach. Reads are error-corrected and used to refine the initial assembly, with up to 5 refinement steps. Pipeline 1: haphpipe_assemble_01 This pipeline implements de novo assembly. Reads are first trimmed ( trim_reads ) and used as input for denovo assembly ( assemble_denovo ). The de novo assembly stage automatically performs error correction on the trimmed reads. The assembled contigs are used as input for amplicon assembly ( assemble_amplicons ) along with reference FASTA and GTF files. The assembly is then iteratively refined up to five times ( refine_assembly ) by mapping corrected reads to the assembled FASTA file and lastly finalized ( finalize_assembly ), resulting in a FASTA file with final consensus sequences, final VCF, and aligned BAM file. To see the input information for Pipeline 1, use the -h option again like so: haphpipe_assemble_01 -h , and it will show the output: USAGE: haphpipe_assemble_01 [read1] [read2] [reference_fasta] [reference_gtf] [samp_id] outdir ----- HAPHPIPE assembly pipeline 01 ----- This pipeline implements amplicon assembly using a denovo approach. Reads are error-corrected and used to refine the initial assembly, with up to 5 refinement steps. Input: read1: Fastq file for read 1. May be compressed (.gz) read2: Fastq file for read 2. May be compressed (.gz) reference_fasta: Reference sequence (fasta) reference_gtf: Amplicon regions (GTF) samp_id: Sample ID outdir: Output directory (default is [sample_dir]/haphpipe_assemble_01) General command to execute pipeline 1: haphpipe_assemble_01 samp/read1.fq.gz samp/read2.fq.gz refs/ref.fasta refs/ref.gtf samp Example command to run with demo samples: haphpipe_assemble_01 SRR8525886/SRR8525886_1.fastq SRR8525886/SRR8525886_2.fastq refs/HIV_B.K03455.HXB2.fasta refs/HIV_B.K03455.HXB2.gtf SRR8525886 SRR8525886 Pipeline 2: haphpipe_assemble_02 This pipeline implements reference-based mapping assembly. Reads are first trimmed ( trim_reads ) and error-corrected ( ec_reads ). The corrected reads are used as input for reference-based mapping assembly ( refine_assembly ) for up to five iterations. Lastly, the assembly is finalized ( finalize_assembly ) by mapping reads onto the refined reference sequence. The final output is a FASTA file with final consensus sequences, final VCF, and aligned BAM file. (insert Figure 2 here) To see the input information for Pipeline 1, use the -h option again like so: haphpipe_assemble_02 -h , and it will show the output: USAGE: haphpipe_assemble_02 [read1] [read2] [amplicons_fasta] [samp_id] outdir ----- HAPHPIPE assembly pipeline 02 ----- This pipeline implements amplicon assembly using a reference-based approach. Reads are error-corrected and aligned to provided amplicon reference with up to five refinement steps. Input: read1: Fastq file for read 1. May be compressed (.gz) read2: Fastq file for read 2. May be compressed (.gz) amplicons_fasta: Amplicon reference sequence (fasta) samp_id: Sample ID outdir: Output directory (default is sample_dir/haphpipe_assemble_02) General command to execute pipeline 1: haphpipe_assemble_02 SRR8525886/SRR8525886_1.fastq SRR8525886/SRR8525886_2.fastq refs/HIV_B.K03455.HXB2.amplicons fasta SRR8525886 SRR8525886 Example command to run with demo samples: insert","title":"Example Pipelines"},{"location":"expipes/#pipeline-1-haphpipe_assemble_01","text":"This pipeline implements de novo assembly. Reads are first trimmed ( trim_reads ) and used as input for denovo assembly ( assemble_denovo ). The de novo assembly stage automatically performs error correction on the trimmed reads. The assembled contigs are used as input for amplicon assembly ( assemble_amplicons ) along with reference FASTA and GTF files. The assembly is then iteratively refined up to five times ( refine_assembly ) by mapping corrected reads to the assembled FASTA file and lastly finalized ( finalize_assembly ), resulting in a FASTA file with final consensus sequences, final VCF, and aligned BAM file. To see the input information for Pipeline 1, use the -h option again like so: haphpipe_assemble_01 -h , and it will show the output: USAGE: haphpipe_assemble_01 [read1] [read2] [reference_fasta] [reference_gtf] [samp_id] outdir ----- HAPHPIPE assembly pipeline 01 ----- This pipeline implements amplicon assembly using a denovo approach. Reads are error-corrected and used to refine the initial assembly, with up to 5 refinement steps. Input: read1: Fastq file for read 1. May be compressed (.gz) read2: Fastq file for read 2. May be compressed (.gz) reference_fasta: Reference sequence (fasta) reference_gtf: Amplicon regions (GTF) samp_id: Sample ID outdir: Output directory (default is [sample_dir]/haphpipe_assemble_01) General command to execute pipeline 1: haphpipe_assemble_01 samp/read1.fq.gz samp/read2.fq.gz refs/ref.fasta refs/ref.gtf samp Example command to run with demo samples: haphpipe_assemble_01 SRR8525886/SRR8525886_1.fastq SRR8525886/SRR8525886_2.fastq refs/HIV_B.K03455.HXB2.fasta refs/HIV_B.K03455.HXB2.gtf SRR8525886 SRR8525886","title":"Pipeline 1: haphpipe_assemble_01"},{"location":"expipes/#pipeline-2-haphpipe_assemble_02","text":"This pipeline implements reference-based mapping assembly. Reads are first trimmed ( trim_reads ) and error-corrected ( ec_reads ). The corrected reads are used as input for reference-based mapping assembly ( refine_assembly ) for up to five iterations. Lastly, the assembly is finalized ( finalize_assembly ) by mapping reads onto the refined reference sequence. The final output is a FASTA file with final consensus sequences, final VCF, and aligned BAM file. (insert Figure 2 here) To see the input information for Pipeline 1, use the -h option again like so: haphpipe_assemble_02 -h , and it will show the output: USAGE: haphpipe_assemble_02 [read1] [read2] [amplicons_fasta] [samp_id] outdir ----- HAPHPIPE assembly pipeline 02 ----- This pipeline implements amplicon assembly using a reference-based approach. Reads are error-corrected and aligned to provided amplicon reference with up to five refinement steps. Input: read1: Fastq file for read 1. May be compressed (.gz) read2: Fastq file for read 2. May be compressed (.gz) amplicons_fasta: Amplicon reference sequence (fasta) samp_id: Sample ID outdir: Output directory (default is sample_dir/haphpipe_assemble_02) General command to execute pipeline 1: haphpipe_assemble_02 SRR8525886/SRR8525886_1.fastq SRR8525886/SRR8525886_2.fastq refs/HIV_B.K03455.HXB2.amplicons fasta SRR8525886 SRR8525886 Example command to run with demo samples: insert","title":"Pipeline 2: haphpipe_assemble_02"},{"location":"faq/","text":"HAPHPIPE NGS Using the Command Line Conda Bash","title":"FAQ"},{"location":"faq/#haphpipe","text":"","title":"HAPHPIPE"},{"location":"faq/#ngs","text":"","title":"NGS"},{"location":"faq/#using-the-command-line","text":"","title":"Using the Command Line"},{"location":"faq/#conda","text":"","title":"Conda"},{"location":"faq/#bash","text":"","title":"Bash"},{"location":"help/","text":"Conda Bioconda Bioconda channels Bash Command line tutorial List of bash commands Bash scripting tutorial Additional scripting help: linuxconfig.org flaviocopesc.com ryanstutorials.net NGS overview: NYU resource Accessing on a PC using a virtual machine Guide to options VirtualBox","title":"Helpful Resources"},{"location":"help/#conda","text":"Bioconda Bioconda channels","title":"Conda"},{"location":"help/#bash","text":"Command line tutorial List of bash commands Bash scripting tutorial Additional scripting help: linuxconfig.org flaviocopesc.com ryanstutorials.net","title":"Bash"},{"location":"help/#ngs-overview","text":"NYU resource","title":"NGS overview:"},{"location":"help/#accessing-on-a-pc-using-a-virtual-machine","text":"Guide to options VirtualBox","title":"Accessing on a PC using a virtual machine"},{"location":"hp_annotate/","text":"hp_annotate includes stages to annotate consensus sequence(s). Use -h after any command for a list of options. pairwise_align Apply correct coordinate system to final sequence(s) to facilitate downstream analyses. Input is the final sequence file in FASTA format, a reference sequence in FASTA format, and a reference GFT file. Output is a JSON file to be used in extract_pairwise . Usage: haphpipe pairwise_align [SETTINGS] --amplicons_fa FASTA --ref_fa FASTA --ref_gtf GTF [--outdir] (or): hp_pairwise_align [SETTINGS] --amplicons_fa FASTA --ref_fa FASTA --ref_gtf GTF [--outdir] Output files: pairwise_aligned.json Input/Output Arguments: Option Description --amplicons_fa Fasta file with assembled amplicons. --ref_fa Reference fasta file. --ref_gtf GTF format file containing amplicon regions. Primary and alternate coding regions should be provided in the attribute field (for amino acid alignment). --outdir Output directory (default: False). Settings: Option Description --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe pairwise_align --amplicons_fa final.fna --ref_fa HIV_B.K03455.HXB2.fasta --ref_gtf HIV_B.K03455.HXB2.gtf extract_pairwise Extract sequence regions from the pairwise alignment produced in pairwise_align . Input is the JSON file from pairwise_align . Output is either an unaligned nucleotide FASTA file, an aligned nucleotide FASTA file, an amino acid FASTA file, an amplicon GTF file, or a tab-separated values (TSV) file (default: nucleotide FASTA with regions of interest from GTF file used in pairwise_align ). Usage: haphpipe extract_pairwise [OPTIONS] [SETTINGS] --align_json JSON [--outdir] (or): hp_extract_pairwise [OPTIONS] [SETTINGS] --align_json JSON [--outdir] Output files: stdout.fasta Input/Output Arguments: Option Description --align_json JSON file describing alignment (output of pairwise_align stage). --outfile Output file (default: stdout). Options: Option Description --outfmt Format for output: nuc_fa, aln_fa, amp_gtf, ost, or prot_fa (default: nuc_fa). --refreg Reference region. String format is ref:start-stop. For example, the region string to extract pol when aligned to HXB2 is HIV_B.K03455.HXB2:2085-5096. Settings: Option Description --debug Print commands but do not run (default: False). Example usage: haphpipe extract_pairwise --align_json pairwise_aligned.json --refreg HIV_B.K03455.HXB2:2085-5096 summary_stats Report summary statistics from an alignment and/or haplotype calling as TXT and TSV files. Input is a list of paths to directories (TXT format, one per line), each of which contain the following files: final_bt2.out , trimmomatic_summary.out , final.bam , final.fna , and final.vcf.gz . If applicable, also input a list of directories containing PredictHaplo summary files ( ph_summary.txt ). If amplicons were used in assembly, use the --amplicons option to report statistics per amplicon. Usage: haphpipe summary_stats [SETTINGS] --dir_list TXT [--ph_list TXT ] [--amplicons] [--outdir] (or): hp_summary_stats [SETTINGS] --dir_list TXT [--ph_list TXT ] [--amplicons] [--outdir] Output files: summary_stats.txt, summary_stats.tsv, PH_summary_stats.tsv Input/Output Arguments: Option Description --dir_list List of directories which include the required files, one on each line. --ph_list List of directories which include haplotype summary files, one on each line. --amplicons Amplicons used in assembly (default: False). Settings: Option Description --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Name for log file. --debug Print commands but do not run (default: False). Example usage: haphpipe summary_stats --dir_list demo_sra_list.txt --ph_list demo_sra_ph_list.txt --amplicons","title":"Annotate"},{"location":"hp_annotate/#pairwise_align","text":"Apply correct coordinate system to final sequence(s) to facilitate downstream analyses. Input is the final sequence file in FASTA format, a reference sequence in FASTA format, and a reference GFT file. Output is a JSON file to be used in extract_pairwise . Usage: haphpipe pairwise_align [SETTINGS] --amplicons_fa FASTA --ref_fa FASTA --ref_gtf GTF [--outdir] (or): hp_pairwise_align [SETTINGS] --amplicons_fa FASTA --ref_fa FASTA --ref_gtf GTF [--outdir] Output files: pairwise_aligned.json Input/Output Arguments: Option Description --amplicons_fa Fasta file with assembled amplicons. --ref_fa Reference fasta file. --ref_gtf GTF format file containing amplicon regions. Primary and alternate coding regions should be provided in the attribute field (for amino acid alignment). --outdir Output directory (default: False). Settings: Option Description --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe pairwise_align --amplicons_fa final.fna --ref_fa HIV_B.K03455.HXB2.fasta --ref_gtf HIV_B.K03455.HXB2.gtf","title":"pairwise_align"},{"location":"hp_annotate/#extract_pairwise","text":"Extract sequence regions from the pairwise alignment produced in pairwise_align . Input is the JSON file from pairwise_align . Output is either an unaligned nucleotide FASTA file, an aligned nucleotide FASTA file, an amino acid FASTA file, an amplicon GTF file, or a tab-separated values (TSV) file (default: nucleotide FASTA with regions of interest from GTF file used in pairwise_align ). Usage: haphpipe extract_pairwise [OPTIONS] [SETTINGS] --align_json JSON [--outdir] (or): hp_extract_pairwise [OPTIONS] [SETTINGS] --align_json JSON [--outdir] Output files: stdout.fasta Input/Output Arguments: Option Description --align_json JSON file describing alignment (output of pairwise_align stage). --outfile Output file (default: stdout). Options: Option Description --outfmt Format for output: nuc_fa, aln_fa, amp_gtf, ost, or prot_fa (default: nuc_fa). --refreg Reference region. String format is ref:start-stop. For example, the region string to extract pol when aligned to HXB2 is HIV_B.K03455.HXB2:2085-5096. Settings: Option Description --debug Print commands but do not run (default: False). Example usage: haphpipe extract_pairwise --align_json pairwise_aligned.json --refreg HIV_B.K03455.HXB2:2085-5096","title":"extract_pairwise"},{"location":"hp_annotate/#summary_stats","text":"Report summary statistics from an alignment and/or haplotype calling as TXT and TSV files. Input is a list of paths to directories (TXT format, one per line), each of which contain the following files: final_bt2.out , trimmomatic_summary.out , final.bam , final.fna , and final.vcf.gz . If applicable, also input a list of directories containing PredictHaplo summary files ( ph_summary.txt ). If amplicons were used in assembly, use the --amplicons option to report statistics per amplicon. Usage: haphpipe summary_stats [SETTINGS] --dir_list TXT [--ph_list TXT ] [--amplicons] [--outdir] (or): hp_summary_stats [SETTINGS] --dir_list TXT [--ph_list TXT ] [--amplicons] [--outdir] Output files: summary_stats.txt, summary_stats.tsv, PH_summary_stats.tsv Input/Output Arguments: Option Description --dir_list List of directories which include the required files, one on each line. --ph_list List of directories which include haplotype summary files, one on each line. --amplicons Amplicons used in assembly (default: False). Settings: Option Description --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Name for log file. --debug Print commands but do not run (default: False). Example usage: haphpipe summary_stats --dir_list demo_sra_list.txt --ph_list demo_sra_ph_list.txt --amplicons","title":"summary_stats"},{"location":"hp_assemble/","text":"Stages in hp_assemble are designed to construct consensus sequence(s). Input reads (in FASTQ format) are assembled using either denovo assembly or reference-based alignment. Resulting consensus can be further refined. Use -h after any command for a list of options. Reference Files Several modules in this stage use reference files: either a FASTA file containing a reference sequence or a GTF file denoting genome regions for amplicon assembly. For example, with HIV data, we use the file HIV_B.K03455.HXB2.fasta as a reference whole-genome file, HIV_B.K03455.HXB2.amplicons.fasta for amplicon assembly, and HIV_B.K03455.HXB2.gtf as a GTF file. All three files are downloaded in the demo module . assemble_denovo Assemble reads via de novo assembly using SPAdes ( documentation ). Input is reads in FASTQ format. Output is contigs in FNA format. Usage: haphpipe assemble_denovo [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ [--outdir] (or): hp_assemble_denovo [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ [--outdir] Output files: denovo_contigs.fna denovo_summary.txt Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --outdir Output directory (default: current directory). Options: Option Description --no_error_correction Do not perform error correction (default: False) --subsample Use a subsample of reads for assembly --seed Seed for random number generator (ignored if not subsampling) Settings: Option Description --ncpu Number of CPU to use (default: 1). --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe assemble_denovo --fq1 corrected_1.fastq --fq2 corrected_2.fastq --outdir denovo_assembly --no_error_correction TRUE assemble_amplicons Assemble contigs from de novo assembly using both a reference sequence and amplicon regions with MUMMER 3+ ( documentation ). Input is contigs and reference sequence in FASTA format and amplicon regions in GTF format. Usage: haphpipe assemble_amplicons [OPTIONS] [SETTINGS] --contigs_fa FASTA --ref_fa FASTA --ref_gtf GTF [--outdir] (or): hp_assemble_amplicons [OPTIONS] [SETTINGS] --contigs_fa FASTA --ref_fa FASTA --ref_gtf GTF [--outdir] Output files: amplicon_assembly.fna Input/Output Arguments: Option Description --contigs_fa Fasta file with assembled contigs. --ref_fa Fasta file with reference genome to scaffold against. --ref_gtf GTF format file containing amplicon regions. --outdir Output directory (default: current directory). Scaffold Options: Option Description --sample_id Sample ID (default: sampleXX). --padding Bases to include outside reference annotation (default: 50). Settings: Option Description --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe assemble_amplicons --contigs_fa denovo_contigs.fa --ref_fa HIV_B.K03455.HXB2.fasta --ref_gtf HIV_B.K03455.HXB2.gtf assemble_scaffold Scaffold contigs against a reference sequence with MUMMER 3+ ( documentation ). Input is contigs in FASTA format and reference sequence in FASTA format. Output is scaffold assembly, alligned scaffold, imputed scaffold, and padded scaffold in FASTA format. Usage: haphpipe assemble_scaffold [OPTIONS] [SETTINGS] --contigs_fa FASTA --ref_fa FASTA [--outdir] (or): hp_assemble_scaffold [OPTIONS] [SETTINGS] --contigs_fa FASTA --ref_fa FASTA [--outdir] Output files: scaffold_aligned.fa scaffold_assembly.fa scaffold_imputed.fa scaffold_padded.out Input/Output Arguments: Option Description --contigs_fa Fasta file with assembled contigs. --ref_fa Fasta file with reference genome to scaffold against. --outdir Output directory (default: current directory). Options: Option Description --seqname Name to append to scaffold sequence (default: sample01). Settings: Option Description --keep_tmp Additional options (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe assemble_scaffold --contigs_fa denovo_contigs.fa --ref_fa HIV_B.K03455.HXB2.fasta align_reads Map reads to reference sequence (instead of running de novo assembly) using Bowtie2 ( documentation ) and Picard ( documentation ). Input is reads in FASTQ format and reference sequence in FASTA format. Usage: haphpipe align_reads [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ --ref_fa FASTA [--outdir] (or): hp_align_reads [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ --ref_fa FASTA [--outdir] Output files: aligned.bam aligned.bt2.out Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --ref_fa Reference fasta file. --outdir Output directory (default: current directory). Options: Option Description --bt2_preset {very-fast, fast, sensitive,very-sensitive,very-fast-local,fast-local,sensitive-local,very-sensitive-local} --sample_id Sample ID. Used as read group ID in BAM (default: sampleXX). --no_realign Do not realign indels (default: False). --remove_duplicates Remove duplicates from final alignment. Otherwise duplicates are marked but not removed (default: False). --encoding {Phred+33,Phred+64} Quality score encoding. Settings: Option Description --ncpu Number of CPUs to use (default: 1). --xmx Maximum heap size for Java VM, in GB (default: 32). --keep_tmp Additional options (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe align_reads --fq1 corrected_1.fastq --fq2 corrected _2.fastq --ref_fa HIV_B.K03455.HXB2.fasta call_variants Variant calling from alignment using GATK ( documentation ). Input is alignment file in BAM format and reference sequence in FASTA format (either reference from reference-based assembly or consensus final sequence from de novo assembly). Output is a Variant Call File (VCF) format file. Usage: haphpipe call_variants [OPTIONS] [SETTINGS] --aln_bam BAM --ref_fa FASTA [--outdir] (or): hp_call_variants [OPTIONS] [SETTINGS] --aln_bam BAM --ref_fa FASTA [--outdir] Output files: variants.vcf.gz Input/Output Arguments: Option Description --aln_bam Alignment file. --ref_fa Reference fasta file. --outdir Output directory (default: False). Options: Option Description --emit_all Output calls for all site (default: False). --min_base_qual Minimum base quality required to consider a base for calling (default: 15). Settings: Option Description --ncpu Number of CPUs to use (default: 1). --xmx Maximum heap size for Java VM, in GB (default: 32). --keep_tmp Additional options (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe call_variants --aln_bam alignment.bam --ref_fa HIV_B.K03455.HXB2.fasta vcf_to_consensus Generate a consensus sequence from a VCF file. Input is a VCF file. Output is the consensus sequence in FASTA format. Usage: haphpipe vcf_to_consensus [OPTIONS] [SETTINGS] --vcf FASTQ [--outdir] [--sampidx] (or): hp_vcf_to_consensus [OPTIONS] [SETTINGS] --vcf FASTQ [--outdir] [--sampidx] Output files: consensus.fna Input/Output Arguments: Option Description --vcf VCF file (created with all sites). --outdir Output directory (default: False). --sampidx Index for sample if multi-sample VCF (default: 0). Options: Option Description --min_DP Minimum depth to call site (default: 1). --major Allele fraction to make unambiguous call (default: 0.5). --minor Allele fraction to make ambiguous call (default: 0.2). Settings: Option Description --keep_tmp Additional options (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. Example usage: haphpipe vcf_to_consensus --vcf variants.vcf refine_assembly Map reads to a denovo assembly or reference alignment. Assembly or alignment is iteratively updated. Input is reads in FASTQ format and reference sequence (assembly or reference alignment) in FASTA format. Output is refined assembly in FASTA format. Usage: haphpipe refine_assembly [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ --ref_fa FASTA [--outdir] (or): hp_refine_assembly [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ --ref_fa FASTA [--outdir] Output files: refined.fna Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --ref_fa Reference fasta file. --outdir Output directory (default: False). Options: Option Description --max_step Maximum number of refinement steps (default: 1). --subsample Use a subsample of reads for refinement. --seed Seed for random number generator (ignored if not subsampling). --sample_id Sample ID. Used as read group ID in BAM (default: sampleXX). Settings: Option Description --ncpu Number of CPUs to use (default: 1). --xmx Maximum heap size for Java VM, in GB (default: 32). --keep_tmp Additional options (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe refine_assembly --fq_1 corrected_1.fastq --fq2 corrected_2.fastq --ref_fa HIV_B.K03455.HXB2.fasta finalize_assembly Finalize consensus, map reads to consensus, and call variants. Input is reads in FASTQ format and reference sequence in FASTA format. Output is finalized reference sequence, alignment, and variants (in FASTA, BAM, and VCF formats, respectively). Usage: haphpipe finalize_assembly [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ --ref_fa FASTA [--outdir] (or): hp_finalize_assembly [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ --ref_fa FASTA [--outdir] Output files: final.fna final.ban final.vcf.gz Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --ref_fa Consensus fasta file. --outdir Output directory (default: current directory). Options: Option Description --bt2_preset {very-fast,fast,sensitive,very-sensitive,very-fast-local,fast-local,sensitive-local,very-sensitive-local} Bowtie2 preset to use (default: very-sensitive). --sample_id Sample ID (default: sampleXX). Settings: Option Description --ncpu Number of CPU to use (default: 1). --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe finalize_assembly --fq_1 corrected_1.fastq --fq2 corrected_2.fastq --ref_fa refined.fna","title":"Assemble"},{"location":"hp_assemble/#assemble_denovo","text":"Assemble reads via de novo assembly using SPAdes ( documentation ). Input is reads in FASTQ format. Output is contigs in FNA format. Usage: haphpipe assemble_denovo [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ [--outdir] (or): hp_assemble_denovo [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ [--outdir] Output files: denovo_contigs.fna denovo_summary.txt Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --outdir Output directory (default: current directory). Options: Option Description --no_error_correction Do not perform error correction (default: False) --subsample Use a subsample of reads for assembly --seed Seed for random number generator (ignored if not subsampling) Settings: Option Description --ncpu Number of CPU to use (default: 1). --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe assemble_denovo --fq1 corrected_1.fastq --fq2 corrected_2.fastq --outdir denovo_assembly --no_error_correction TRUE","title":"assemble_denovo"},{"location":"hp_assemble/#assemble_amplicons","text":"Assemble contigs from de novo assembly using both a reference sequence and amplicon regions with MUMMER 3+ ( documentation ). Input is contigs and reference sequence in FASTA format and amplicon regions in GTF format. Usage: haphpipe assemble_amplicons [OPTIONS] [SETTINGS] --contigs_fa FASTA --ref_fa FASTA --ref_gtf GTF [--outdir] (or): hp_assemble_amplicons [OPTIONS] [SETTINGS] --contigs_fa FASTA --ref_fa FASTA --ref_gtf GTF [--outdir] Output files: amplicon_assembly.fna Input/Output Arguments: Option Description --contigs_fa Fasta file with assembled contigs. --ref_fa Fasta file with reference genome to scaffold against. --ref_gtf GTF format file containing amplicon regions. --outdir Output directory (default: current directory). Scaffold Options: Option Description --sample_id Sample ID (default: sampleXX). --padding Bases to include outside reference annotation (default: 50). Settings: Option Description --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe assemble_amplicons --contigs_fa denovo_contigs.fa --ref_fa HIV_B.K03455.HXB2.fasta --ref_gtf HIV_B.K03455.HXB2.gtf","title":"assemble_amplicons"},{"location":"hp_assemble/#assemble_scaffold","text":"Scaffold contigs against a reference sequence with MUMMER 3+ ( documentation ). Input is contigs in FASTA format and reference sequence in FASTA format. Output is scaffold assembly, alligned scaffold, imputed scaffold, and padded scaffold in FASTA format. Usage: haphpipe assemble_scaffold [OPTIONS] [SETTINGS] --contigs_fa FASTA --ref_fa FASTA [--outdir] (or): hp_assemble_scaffold [OPTIONS] [SETTINGS] --contigs_fa FASTA --ref_fa FASTA [--outdir] Output files: scaffold_aligned.fa scaffold_assembly.fa scaffold_imputed.fa scaffold_padded.out Input/Output Arguments: Option Description --contigs_fa Fasta file with assembled contigs. --ref_fa Fasta file with reference genome to scaffold against. --outdir Output directory (default: current directory). Options: Option Description --seqname Name to append to scaffold sequence (default: sample01). Settings: Option Description --keep_tmp Additional options (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe assemble_scaffold --contigs_fa denovo_contigs.fa --ref_fa HIV_B.K03455.HXB2.fasta","title":"assemble_scaffold"},{"location":"hp_assemble/#align_reads","text":"Map reads to reference sequence (instead of running de novo assembly) using Bowtie2 ( documentation ) and Picard ( documentation ). Input is reads in FASTQ format and reference sequence in FASTA format. Usage: haphpipe align_reads [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ --ref_fa FASTA [--outdir] (or): hp_align_reads [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ --ref_fa FASTA [--outdir] Output files: aligned.bam aligned.bt2.out Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --ref_fa Reference fasta file. --outdir Output directory (default: current directory). Options: Option Description --bt2_preset {very-fast, fast, sensitive,very-sensitive,very-fast-local,fast-local,sensitive-local,very-sensitive-local} --sample_id Sample ID. Used as read group ID in BAM (default: sampleXX). --no_realign Do not realign indels (default: False). --remove_duplicates Remove duplicates from final alignment. Otherwise duplicates are marked but not removed (default: False). --encoding {Phred+33,Phred+64} Quality score encoding. Settings: Option Description --ncpu Number of CPUs to use (default: 1). --xmx Maximum heap size for Java VM, in GB (default: 32). --keep_tmp Additional options (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe align_reads --fq1 corrected_1.fastq --fq2 corrected _2.fastq --ref_fa HIV_B.K03455.HXB2.fasta","title":"align_reads"},{"location":"hp_assemble/#call_variants","text":"Variant calling from alignment using GATK ( documentation ). Input is alignment file in BAM format and reference sequence in FASTA format (either reference from reference-based assembly or consensus final sequence from de novo assembly). Output is a Variant Call File (VCF) format file. Usage: haphpipe call_variants [OPTIONS] [SETTINGS] --aln_bam BAM --ref_fa FASTA [--outdir] (or): hp_call_variants [OPTIONS] [SETTINGS] --aln_bam BAM --ref_fa FASTA [--outdir] Output files: variants.vcf.gz Input/Output Arguments: Option Description --aln_bam Alignment file. --ref_fa Reference fasta file. --outdir Output directory (default: False). Options: Option Description --emit_all Output calls for all site (default: False). --min_base_qual Minimum base quality required to consider a base for calling (default: 15). Settings: Option Description --ncpu Number of CPUs to use (default: 1). --xmx Maximum heap size for Java VM, in GB (default: 32). --keep_tmp Additional options (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe call_variants --aln_bam alignment.bam --ref_fa HIV_B.K03455.HXB2.fasta","title":"call_variants"},{"location":"hp_assemble/#vcf_to_consensus","text":"Generate a consensus sequence from a VCF file. Input is a VCF file. Output is the consensus sequence in FASTA format. Usage: haphpipe vcf_to_consensus [OPTIONS] [SETTINGS] --vcf FASTQ [--outdir] [--sampidx] (or): hp_vcf_to_consensus [OPTIONS] [SETTINGS] --vcf FASTQ [--outdir] [--sampidx] Output files: consensus.fna Input/Output Arguments: Option Description --vcf VCF file (created with all sites). --outdir Output directory (default: False). --sampidx Index for sample if multi-sample VCF (default: 0). Options: Option Description --min_DP Minimum depth to call site (default: 1). --major Allele fraction to make unambiguous call (default: 0.5). --minor Allele fraction to make ambiguous call (default: 0.2). Settings: Option Description --keep_tmp Additional options (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. Example usage: haphpipe vcf_to_consensus --vcf variants.vcf","title":"vcf_to_consensus"},{"location":"hp_assemble/#refine_assembly","text":"Map reads to a denovo assembly or reference alignment. Assembly or alignment is iteratively updated. Input is reads in FASTQ format and reference sequence (assembly or reference alignment) in FASTA format. Output is refined assembly in FASTA format. Usage: haphpipe refine_assembly [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ --ref_fa FASTA [--outdir] (or): hp_refine_assembly [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ --ref_fa FASTA [--outdir] Output files: refined.fna Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --ref_fa Reference fasta file. --outdir Output directory (default: False). Options: Option Description --max_step Maximum number of refinement steps (default: 1). --subsample Use a subsample of reads for refinement. --seed Seed for random number generator (ignored if not subsampling). --sample_id Sample ID. Used as read group ID in BAM (default: sampleXX). Settings: Option Description --ncpu Number of CPUs to use (default: 1). --xmx Maximum heap size for Java VM, in GB (default: 32). --keep_tmp Additional options (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe refine_assembly --fq_1 corrected_1.fastq --fq2 corrected_2.fastq --ref_fa HIV_B.K03455.HXB2.fasta","title":"refine_assembly"},{"location":"hp_assemble/#finalize_assembly","text":"Finalize consensus, map reads to consensus, and call variants. Input is reads in FASTQ format and reference sequence in FASTA format. Output is finalized reference sequence, alignment, and variants (in FASTA, BAM, and VCF formats, respectively). Usage: haphpipe finalize_assembly [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ --ref_fa FASTA [--outdir] (or): hp_finalize_assembly [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ --ref_fa FASTA [--outdir] Output files: final.fna final.ban final.vcf.gz Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --ref_fa Consensus fasta file. --outdir Output directory (default: current directory). Options: Option Description --bt2_preset {very-fast,fast,sensitive,very-sensitive,very-fast-local,fast-local,sensitive-local,very-sensitive-local} Bowtie2 preset to use (default: very-sensitive). --sample_id Sample ID (default: sampleXX). Settings: Option Description --ncpu Number of CPU to use (default: 1). --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe finalize_assembly --fq_1 corrected_1.fastq --fq2 corrected_2.fastq --ref_fa refined.fna","title":"finalize_assembly"},{"location":"hp_haplotype/","text":"hp_haplotype includes haplotype assembly stages. HAPHPIPE implements PredictHaplo ( paper ), although other haplotype reconstruction programs can be utilized outside of HAPHPIPE using the final output of HAPHPIPE, typically with the final consensus sequence (FASTA) file, reads (raw, trimmed, and/or corrected), and/or final alignment (BAM) file as input. Use -h after any command for a list of options. predict_haplo Haplotype identification with PredictHaplo. Input is reads in FASTQ format and and reference sequence in FASTA format. Output is the longest global haplotype file and corresponding HTML file. Note: PredictHaplo must be installed separately before running this stage. Usage: haphpipe predict_haplo [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --ref_fa FASTA --interval_txt [TXT] [--outdir] (or): hp_ predict_haplo [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --ref_fa FASTA --interval_txt [TXT] [--outdir] Output files: best.fa Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --ref_fa Reference sequence used to align reads (Fasta). --interval_txt File with intervals to perform haplotype reconstruction. --outdir Output directory (default: current directory). Options: Option Description --min_readlength Minimum read length passed to PredictHaplo (default: 36). Settings: Option Description --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe predict_haplo corrected_1.fastq --fq2 corrected_2.fastq --ref_fa final.fna ph_parser Returns PredictHaplo output as a correctly formatted FASTA file. Input is the output file from predict_haplo (longest global .fas file). Output is a correctly formatted FASTA file. Usage: haphpipe ph_parser [OPTIONS] [SETTINGS] --haplotypes_fa best.fas [--outdir] (or): hp_ph_parser [OPTIONS] [SETTINGS] --haplotypes_fa best.fas [--outdir] Output files: ph_summary.txt ph_haplotypes.fna Input/Output Arguments: Option Description --haplotypes_fa Haplotype file created by PredictHaplo. --outdir Output directory (default: current directory). Options: Option Description --prefix Prefix to add to sequence names. --keep_gaps Do not remove gaps from alignment (default: False). Settings: Option Description --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. Example usage: haphpipe ph_parser PH01.best_1_864.fas Example: By default, PredictHaplo outputs their own unique version of a fasta file. It includes the frequency, some information regarding their unqiue overlapping scores, and their unique confidence scores. This file is always named PH#.best_#_#.fas , where the first number is the reconstructed haplotype number and the next numbers are the start and end of the longest haplotype reconstructed by PredictHaplo. $ cat PH01.best_1_1208.fas reconstructed_0 ;Freq:0.190638 ;Overlap quality scores (5,10):1,1 ;Confidence scores ;~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~n7~~~y~~~~~t~~~i~jjk~~zz ;~~~~{~~~~~N{~{~Sx~~~~~~z~K~F~~~~~~~y~~~~~~~|~~F~wx|~~{~~|~~s |]~~~~~~ ;~m~kj|{|v~{|_`~~~z~~~~~~~jy{y~~~~~a~__~~|~~~~~{wXZ~}~~~qm~xV~~~~~~~}~ ;Q~}~y||~~~}}~~~z~~~~~~{}A~}b|~~u~~|}}|~}}z~}bx~~n||~~||{~}~~d}~bz~~~} ;|~}}~~~}~~~{|}}g{~~~}~r~}}~~~~u~~{kx{~}~}~|~}~}{}~}~}||~~~~[~}~}}~~~~ ;~~}~~~U||U}~|}}~~}}~~~~}u~b|}~w~~~~~{}|wv~}Dxzp{|}~@~~P~}~}~V~~z}~|ry ;q~}|}~}~~}t~o~}~f}~~}{~~~}~{~~~~~~~}~~}~~|~~~M~~}~}x~}c~}^v~~~yzA~}~} ;y}~z}~~~~~~~~~{z~~}~~~}~{}~~~~~}~~~~~~~|~~v~~}~~|y~]|{~||~~~~~~~~||~~ ;Y||~~|Q~|~~~|~~~~~|~~~z~~z~{{{y~~~~~~~~~~w{{~wz|~Z|~z|~~}p|~~|}}~~x}} ;z}~}}|a|}}}}{}|~~}}~}}~}~{~|~}}~}{}|}|}}}~|}}}}{}}}|}}|}|}}}|}}}}~}}} ;||}}}}{}}~}~}}}}}}}}}}}~}}}}}}}}}}}}}}}}}y}~}}}}}}}}}}~|}}}}|}}}}|}}} ;}}}}}}}}|}}}}}}}}~}}}}}}|}}}}}}}}|}}}}}|}}}}|~|}}}}{}}}}}}}}}}}}}}}}} ;}}=}}}{}}}}}}}}}}}}}}}}}}}}}}}}~||}}|~}}}}}{}}}}}|}}|}}}}}}}}}}}}}|}} ;|}~}}}}|}}}}}}}}|}|~J}}}}}}~}}}}}}}}}}}}}}}}}}}}|}}`~}}}}|}}}}}}}}}}~ ;|}}}}}}}}}}}~}}}|}}}}}}}}}}|}}}}}}}}}}}t}}{}`}}}}}}|}~}~}|~}}}|k}}}}} ;|}|~}}|}}~}}~}|}}z}}}}}}}}}}}}}}~}}~}}}~~}|}}}~}}}}}}}}~}~}}||~~}~z}} ;~~}~}||~|~}|{}||~|z~~||}~|~}}|}~~}|}}~}}|}z~~~~{~}{}~y~~~~~{|}}~~|y~~ ;~|~~||~~~~~~~|n~~{~~~~~~~~~~~~~~~ ;EndOfComments ACCAAATGAAAGATTGTACTGAGAGACAGGCTAATTTTTTAGGGAATATCTGGCCTTCCCGCAAGGGAAG GCCAGGGAACTTTCCTCAGAGCAGGCTAGAGCCAACAGCCCCACCAGAAGAGAGCTTCAGGCTTGGGGAA GCAACAACAAAGCCCTCTCAGAAACAGGAGCCAATAGACAAGGAAATGTATCCTTTAACTTCCCTCAGAT CACTCTTTGGCAACGACCCCTTGTCACAATAAAGATAGGGGAGCAACTAAAGGAAGCCCTGTTAGATACA GGAGCAGATGATACAGTATTAGAAGAAATGAATTTGCCAGGAAGATGGAAACCAAGAATGATAGGGGGAA TCGGGGGTTTTATCAAAGTAAGACAGTATGATCAGATAGCCATAGACATCTGTGGCCATAAAGCTATAGG TACAGTATTAGTAGGACCTACACCTGTCAACATAATTGGAAGAAATCTGTTGACTCAGCTTGGTTGCACT TTAAATTTTCCCATTAGTCCTATTGAAACTGTACCAGTAAAATTGAAGCCAGGAATGGATGGCCCAAAGG TTAAACAATGGCCATTGACAGAAGAAAAAATAAAAGCATTAGTAGAAATTTGTACAGAAATGGAAAAAGA AGGGAAAATTTCAAAAATTGGGCCTGAAAATCCATACAATACTCCAGTATTTGCCATAAGGAAAAAAGAC AGTACTAAATGGAGAAAATTAGTAGATTTCAGAGAACTTAATAAGAGAACTCAGGACTTCTGGGAAGTCC AATTAGGAATACCACATCCCTCAGGGTTAAAAAAGAAAAAATCAGTAACAGTACTGGATGCGGGTGATGC ATATTTTTCAGTTCCCTTAGATGAAGACTTCAGAAAGTATACTGCATTTACCATACCTAGTGTAAACAAT GAGACACTAGGGATTAGGTATCAGTACAATGTGCTTCCACAAGGATGGAAAGGATCACCAGCAATATTCC AAGCTAGCATGACAAAAATCTTAGAGCCTTTTAGAAAACAAAATCCAGACATAGTTATCTATCAATACAT GGATGATCTGTATGTAGGATCTGACCTAGAAATAGGGCAGCATAGAACAAAAATAGAGGAACTGAGAGAA CATCTGTTGAGGTGGGGGTTTTGCACACCAGACAAGAAACATCAGAAGGAACCTCCATTCCTTTGGATGG GTTATGAACTCCATCCTT reconstructed_1 ;Freq:0.294104 ;Overlap quality scores (5,10):1,1 ;Confidence scores ;~~~~~~~~z~~~~~~~~~|~y~~~~u}|~~~~~}}~~~~~~~ya~|~~}}~~~y~~~j~YXH~~s~ ;~~~~z~~|~~b|}^}xZ}}~}z~t~r~{}~}x}}yb}~~~}~}u~~}~gu|}}{~|}}~ozsw}|}}h~ ;|l}v|^][t}zz}vw}}s}}}}~v}|~|~~}}}~~}}}}}~}}}}}}zo\\}}}~}vn}iv}}}}}}}}} ;v}}~|u}}}}}}}}}|}}}}}}|}^}}[}~}r}}}{|}}|t|}}{z}}pz}}{}}}}}}}v~}y|~}}} ;}}}}~}}~}}}tz}}`}}}~}}q}}}}}~}y}}|r||}}}}~}}~}}{}}}}}}}}}}}y~}}}}t}}} ;}}}}}}{|}|}}{r}}}}tv~}}}t}m|~}h}}}}}v}}qt}|y|{|u~}}|}~_}~|~~|}~|}~}|e ;Y}~}}~}}~}v}t}}}j}}~}}}~}}~|}}}}|}}|}}}~}}}}};}}~}}x}}|}}yu}}}wy`}|~~ ;{}}x}}}}}~}}}~{v}}}}}{}~}}~}~}}|}}}}}}}}}}u}}y~~{g}L}}}}}|~}{~|}|}|}} ;l}}}}{_}}}}~z~}|}~~|~~x|~}~z~~|~}~~~}~~~}y}~~|k}}Pf}w~~~~T}}}F~~}~z}{ ;{~|}|zZ~s~|}r~}}~~|}}}}}~|}}}}n}}}~}}}}~}}}}}}}}}}}|~}}}}}}~~}}~~~~~} ;N}}}}}|~}}~}}}}{}}}~}}|~~|}|}}}~|}~}}}}}}D}~}}}}}}}}}}}}}}}}|}}z}|}}} ;}k}}}}}}}}}}}}}}}}}}}|}}}}}|}}}}}|}}}}}|}}}}|~{~}}}G}~}}}}}~}}}}}}}}} ;}}h}}~|}|}}}}}}}}}}}}}}}}}}}}}}}}{}}|}}}}|~|}}}}}}}M}}}}}}}~}}}}}}}}} ;}}~|}}}}}~~~}}}~}}|}g}}}}}}}}~|}}}}}}}}}}}}}}}}}|}~t~}|}}}}}}}||~}}}} ;|}}}}~}}}}}}~}}}gi}}~}}}}}}}}}}}}|}}~}}g}~}}v}}}}}|{}}}}}}~}}~|e}}}}} ;}}|}}~}~}~}}~}|~}|}~|||}}}}|~}}~}}}}}}}}|}}~}}~}|~}|y~}~}}}}}|}}~~}~} ;}}~}~}|~}}}~}}}}~}{}}~}}~}}}||~~}||~}x|}{~|}|~~|~}}}|{~}}~}{}}||~}}}~ ;|~}~}}}~|}}~~|,~~~~~~~~~~~~~~m~~~ ;EndOfComments ACCAAATGAAAGATTGTACTGAGAGACAGGCTAATTTTTTAGGGAAGATCTGGCCTTCCCGCGGCGGAAG GCCAGGGAATTTTCTTCAGAGCAGACCAGAGCCAACAGCCCCACCAGAAGAGAGCTTCAGGTTTGGGGAG GAGACAACAACTCCCTCTCAGAAGCAGGAGCCGAAGGACAAGGAAACATATCCTTTAGCTTCCCTCAAAT CACTCTTTGGCAACGACCCCTTGTTACAATAAAGATAGGGGGGCAGCTAAAGGAAGCTCTATTAGATACA GGAGCAGATGACACAGTATTAGAAGAAATGGATTTGCCAGGAAGATGGAAACCAAAAATGATAGGGGGAA TTGGAGGTTTTATCAAAGTAAAACAGTATGATCAGATACCCATAGAAATTTGTGGACATAAAGTGATAGG TACAGTATTAGTAGGACCTACACCTGTCAACATAATTGGAAGAAATCTGTTGACTCAGCTTGGTTGCACT TTAAATTTTCCCATTAGTCCTATTGAAACTGTACCAGTAAAATTGAAGCCAGGAATGGATGGCCCAAAGG TTAAACAATGGCCATTGACAGAAGAAAAAATAAAAGCATTAATAGAAATCTGTGCAGAAATGGAAAAGGA AGGGAAAATTTCAAAAATTGGGCCTGAAAATCCATACAATACTCCAGTATTTGCCATAAGAAAAAAAGAC AGTACTAAATGGAGAAAATTAGTAGATTTCAAAGAACTTAATAAGAGAACTCAGGACTTCTGGGAAGTCC AATTAGGAATACCACATCCCTCAGGGTTAAAAAAGAAAAAGTCAGTAACAGTACTGGATGTGGGTGATGC ATATTTTTCAGTTCCCTTAGATGAAGACTTCAGAAAGTACACTGCATTTACCATACCTAGTGTAAACAAT GAGACACCAGGGATTAGGTATCAGTACAATGTGCTTCCACAAGGATGGAAAGGATCACCAGCAATATTCC AATGTAGCATGACAAAAATCTTAGAACCTTTTAGAAAACAAAATCCAGATATAGTTATCTATCAATACAT GGATGATCTGTATGTAGGATCTGACCTAGAAATAGGGCAGCATAGAACAAAAATAGAGGAACTGAGAGAA CATCTGTTGAGGTGGGGGTTTTGCACACCAGACAAGAAACATCAGAAGGAACCTCCATTCCTTTGGATGG GTTATGAACTCCATCCTT reconstructed_2 ;Freq:0.515259 ;Overlap quality scores (5,10):4,4 ;Confidence scores ;~~~~~~~w~~zz~~~~z~~w~|wy~|~~{|~~|~~~~~~~~}~{~hD~~~}|~|}}t~~_;~tue}}S} ;~}~~p}}l}~P}}ZlvL~z}}h~g~A~x}}}s}}{S~}~}}}}l}~|}SOq}}q}w}}~i\\pz~w~~z~ ;}7~|vzUu4|smtgm}~W}}~~}r}n`f`}{PZLSI:Vs_V_}}}}_ db}}}}}CQ}YJ}t}}}~}}} ;T}}~|}}}}}}}~}}k}}}}}}q}L}}]}}}L}}}tt|~}{q}}{M}lAIu}}}s{}}}~y~}st}|}} ;z}}}}|}}}}}{q|}Z}}}}}}P~}|}|}}E}}cLbi}vu}}|}}|}g}}}}}}|}~}~{}~}|}{}}} ;}~}|}}}{{|~}pp}~~~{|}}~}[yEx}}\\}}}{}N}uoK}}pauSy}}}v~~J}}q}}v~}c}~}\\i ;]|}}~}|}~}r}Y}}}B}}~||~}}}}{}}}|}|~|}~~~~}}~{w}{}~|R|}s|}k{~}}}ra}q}} ;o}}d}~}}}}}~}}|X~}}~}e}}}}}}}|}y|}}}}~}|~}c~~b~~nL~a}}}}}~}}m}}~{}}}} ;g}}}}uLm~l}}`~|n}}~}}|d~~y~u}|tz~~~~|}}~~t}}}tx}|Tu~v}}~}^|~~w~|~}m{} ;u}}}|w;}{}{}r}||}}z|}|~|}z}||}y}}}~}}}|}}~}}}|}y}}}}}}}~}||}}}}}}~}}| ;v{}|}}{}}}|~|}}|}}}}}}}}}}|}}}}}|}~}}}}}}p}}||}}}}}~}}}{}}}}}}}z}}}}} ;}|}}~}}}|}}}}}}}}}}}}}}}}}}}}}}|}||}}|}|}}}}|}|}}}}u}}}}}}}}}}}}}}}}} ;}}g}}}{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}y}}}}}|}m}}}}}}}}}}}}}}}}} ;}}}}}}}}}}}}}}}}}}|}i}}}}}}}}}}|}}}}}}}}}}}}}}}}}}}H}}}}}}}}}|}{}}}}} ;|}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}y}}|}H}}}}}}}}}}}}}}}}}}u}}}}} ;}}}}}}}}}}}}}}}}}z}}}}}}}}}}}}}}}}|}}}|}}}}}}}}}|}}}}}}}}}}}}|}}}}|}} ;}}}}}}}}}}}}||}|}}|}}}}}}|}}|}}}~}|}|}}}|}{}}}}|}}}}|{|}}}}x||}}}|{}} ;}}}}}}}~}}~}~|n}~}}}}}~}}~~|~~~y~, ;EndOfComments ACCAAATGAAAGATTGTACTGAGAGACAGGCTAATTTTTTAGGGAAAATCTGGCCTTCCCACAAGGGAAG GCCAGGGAATTTTCTTCAGAGCAGACCAGAGCCAACAGCCCCACCAGAAGAGAGCTTCAGGTTTGGGGAA GAGACAACAACTCCCTCTCAGAAGCAGGAGCCGATAGACAAGGAAATGTATCCTTTAGCTTCCCTCAAAT CACTCTTTGGCAACGACCCCTCGTCACAATAAAGATAGGGGGGCAACTAAAGGAAGCTCTATTAGATACA GGAGCAGATGATACAGTATTAGAAGAAATGAATTTGCCAGGAAGATGGAAACCAAAAATGATAGGGGGAA TTGGAGGTTTTATCAAAGTAAAACAGTATGATCAGATACCCATAGAAATCTGTGGACATAAAGCTATAGG TACAGTATTAGTAGGACCTACACCTGTCAACATAATTGGAAGAAATCTGTTGACTCAGATTGGTTGCACT TTAAATTTTCCCATTAGTCCTATTGAAACTGTACCAGTAAAATTAAAGCCAGGAATGGATGGCCCAAAAG TTAAACAATGGCCATTGACAGAAGAAAAAATAAAAGCATTAGTAGAAATTTGTACAGAAATGGAAAAGGA AGGGAAAATTTCAAAAATTGGGCCTGAAAATCCATACAATACTCCAGTATTTGCCATAAGGAAAAAAGAC AGTACTAAATGGAGAAAATTAGTAGATTTCAGAGAACTTAATAAGAGAACTCAAGACTTCCGGGAAGTCC AATTAGGAATACCACATCCCTCAGGGTTAAAAAAGAAAAAATCAGTAACAGTACTGGATGTGGGTGATGC ATATTTTTCAGTTCCCTTAGATGAAGACTTCAGAAAGTATACTGCATTTACCATACCTAGTGTAAACAAT GAGACACCAGGGATTAGGTATCAGTACAATGTGCTTCCACAAGGATGGAAAGGATCACCAGCAATATTCC AAGCTAGCATGACAAAAATCTTAGAGCCTTTTAGAAAACAAAATCCAGACATAGTTATCTATCAATACAT GGATGATCTGTATGTAGGATCTGACCTAGAAATAGGGCAGCATAGAACAAAAATAGAGGAACTGAGAGAA CATCTGTTGAGGTGGGGGTTTTGCACACCAGACAAGAAACATCAGAAGGAACCTCCATTCCTTTGGATGG GTTATGAACTCCATCCCC ph_parser takes this output and creates a proper fasta file with each resontructed haplotype and a text file that has the hpalotype diversity estimate. $ cat ph_summary.txt PH_num_hap 3 PH_hap_diversity 0.611668153059 PH_seq_len 1208 $ cat ph_haplotypes.fna reconstructed_0 Freq=0.190638 ACCAAATGAAAGATTGTACTGAGAGACAGGCTAATTTTTTAGGGAATATCTGGCCTTCCCGCAAGGGAAG GCCAGGGAACTTTCCTCAGAGCAGGCTAGAGCCAACAGCCCCACCAGAAGAGAGCTTCAGGCTTGGGGAA GCAACAACAAAGCCCTCTCAGAAACAGGAGCCAATAGACAAGGAAATGTATCCTTTAACTTCCCTCAGAT CACTCTTTGGCAACGACCCCTTGTCACAATAAAGATAGGGGAGCAACTAAAGGAAGCCCTGTTAGATACA GGAGCAGATGATACAGTATTAGAAGAAATGAATTTGCCAGGAAGATGGAAACCAAGAATGATAGGGGGAA TCGGGGGTTTTATCAAAGTAAGACAGTATGATCAGATAGCCATAGACATCTGTGGCCATAAAGCTATAGG TACAGTATTAGTAGGACCTACACCTGTCAACATAATTGGAAGAAATCTGTTGACTCAGCTTGGTTGCACT TTAAATTTTCCCATTAGTCCTATTGAAACTGTACCAGTAAAATTGAAGCCAGGAATGGATGGCCCAAAGG TTAAACAATGGCCATTGACAGAAGAAAAAATAAAAGCATTAGTAGAAATTTGTACAGAAATGGAAAAAGA AGGGAAAATTTCAAAAATTGGGCCTGAAAATCCATACAATACTCCAGTATTTGCCATAAGGAAAAAAGAC AGTACTAAATGGAGAAAATTAGTAGATTTCAGAGAACTTAATAAGAGAACTCAGGACTTCTGGGAAGTCC AATTAGGAATACCACATCCCTCAGGGTTAAAAAAGAAAAAATCAGTAACAGTACTGGATGCGGGTGATGC ATATTTTTCAGTTCCCTTAGATGAAGACTTCAGAAAGTATACTGCATTTACCATACCTAGTGTAAACAAT GAGACACTAGGGATTAGGTATCAGTACAATGTGCTTCCACAAGGATGGAAAGGATCACCAGCAATATTCC AAGCTAGCATGACAAAAATCTTAGAGCCTTTTAGAAAACAAAATCCAGACATAGTTATCTATCAATACAT GGATGATCTGTATGTAGGATCTGACCTAGAAATAGGGCAGCATAGAACAAAAATAGAGGAACTGAGAGAA CATCTGTTGAGGTGGGGGTTTTGCACACCAGACAAGAAACATCAGAAGGAACCTCCATTCCTTTGGATGG GTTATGAACTCCATCCTT reconstructed_1 Freq=0.294104 ACCAAATGAAAGATTGTACTGAGAGACAGGCTAATTTTTTAGGGAAGATCTGGCCTTCCCGCGGCGGAAG GCCAGGGAATTTTCTTCAGAGCAGACCAGAGCCAACAGCCCCACCAGAAGAGAGCTTCAGGTTTGGGGAG GAGACAACAACTCCCTCTCAGAAGCAGGAGCCGAAGGACAAGGAAACATATCCTTTAGCTTCCCTCAAAT CACTCTTTGGCAACGACCCCTTGTTACAATAAAGATAGGGGGGCAGCTAAAGGAAGCTCTATTAGATACA GGAGCAGATGACACAGTATTAGAAGAAATGGATTTGCCAGGAAGATGGAAACCAAAAATGATAGGGGGAA TTGGAGGTTTTATCAAAGTAAAACAGTATGATCAGATACCCATAGAAATTTGTGGACATAAAGTGATAGG TACAGTATTAGTAGGACCTACACCTGTCAACATAATTGGAAGAAATCTGTTGACTCAGCTTGGTTGCACT TTAAATTTTCCCATTAGTCCTATTGAAACTGTACCAGTAAAATTGAAGCCAGGAATGGATGGCCCAAAGG TTAAACAATGGCCATTGACAGAAGAAAAAATAAAAGCATTAATAGAAATCTGTGCAGAAATGGAAAAGGA AGGGAAAATTTCAAAAATTGGGCCTGAAAATCCATACAATACTCCAGTATTTGCCATAAGAAAAAAAGAC AGTACTAAATGGAGAAAATTAGTAGATTTCAAAGAACTTAATAAGAGAACTCAGGACTTCTGGGAAGTCC AATTAGGAATACCACATCCCTCAGGGTTAAAAAAGAAAAAGTCAGTAACAGTACTGGATGTGGGTGATGC ATATTTTTCAGTTCCCTTAGATGAAGACTTCAGAAAGTACACTGCATTTACCATACCTAGTGTAAACAAT GAGACACCAGGGATTAGGTATCAGTACAATGTGCTTCCACAAGGATGGAAAGGATCACCAGCAATATTCC AATGTAGCATGACAAAAATCTTAGAACCTTTTAGAAAACAAAATCCAGATATAGTTATCTATCAATACAT GGATGATCTGTATGTAGGATCTGACCTAGAAATAGGGCAGCATAGAACAAAAATAGAGGAACTGAGAGAA CATCTGTTGAGGTGGGGGTTTTGCACACCAGACAAGAAACATCAGAAGGAACCTCCATTCCTTTGGATGG GTTATGAACTCCATCCTT reconstructed_2 Freq=0.515259 ACCAAATGAAAGATTGTACTGAGAGACAGGCTAATTTTTTAGGGAAAATCTGGCCTTCCCACAAGGGAAG GCCAGGGAATTTTCTTCAGAGCAGACCAGAGCCAACAGCCCCACCAGAAGAGAGCTTCAGGTTTGGGGAA GAGACAACAACTCCCTCTCAGAAGCAGGAGCCGATAGACAAGGAAATGTATCCTTTAGCTTCCCTCAAAT CACTCTTTGGCAACGACCCCTCGTCACAATAAAGATAGGGGGGCAACTAAAGGAAGCTCTATTAGATACA GGAGCAGATGATACAGTATTAGAAGAAATGAATTTGCCAGGAAGATGGAAACCAAAAATGATAGGGGGAA TTGGAGGTTTTATCAAAGTAAAACAGTATGATCAGATACCCATAGAAATCTGTGGACATAAAGCTATAGG TACAGTATTAGTAGGACCTACACCTGTCAACATAATTGGAAGAAATCTGTTGACTCAGATTGGTTGCACT TTAAATTTTCCCATTAGTCCTATTGAAACTGTACCAGTAAAATTAAAGCCAGGAATGGATGGCCCAAAAG TTAAACAATGGCCATTGACAGAAGAAAAAATAAAAGCATTAGTAGAAATTTGTACAGAAATGGAAAAGGA AGGGAAAATTTCAAAAATTGGGCCTGAAAATCCATACAATACTCCAGTATTTGCCATAAGGAAAAAAGAC AGTACTAAATGGAGAAAATTAGTAGATTTCAGAGAACTTAATAAGAGAACTCAAGACTTCCGGGAAGTCC AATTAGGAATACCACATCCCTCAGGGTTAAAAAAGAAAAAATCAGTAACAGTACTGGATGTGGGTGATGC ATATTTTTCAGTTCCCTTAGATGAAGACTTCAGAAAGTATACTGCATTTACCATACCTAGTGTAAACAAT GAGACACCAGGGATTAGGTATCAGTACAATGTGCTTCCACAAGGATGGAAAGGATCACCAGCAATATTCC AAGCTAGCATGACAAAAATCTTAGAGCCTTTTAGAAAACAAAATCCAGACATAGTTATCTATCAATACAT GGATGATCTGTATGTAGGATCTGACCTAGAAATAGGGCAGCATAGAACAAAAATAGAGGAACTGAGAGAA CATCTGTTGAGGTGGGGGTTTTGCACACCAGACAAGAAACATCAGAAGGAACCTCCATTCCTTTGGATGG GTTATGAACTCCATCCCC","title":"Haplotype"},{"location":"hp_haplotype/#predict_haplo","text":"Haplotype identification with PredictHaplo. Input is reads in FASTQ format and and reference sequence in FASTA format. Output is the longest global haplotype file and corresponding HTML file. Note: PredictHaplo must be installed separately before running this stage. Usage: haphpipe predict_haplo [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --ref_fa FASTA --interval_txt [TXT] [--outdir] (or): hp_ predict_haplo [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --ref_fa FASTA --interval_txt [TXT] [--outdir] Output files: best.fa Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --ref_fa Reference sequence used to align reads (Fasta). --interval_txt File with intervals to perform haplotype reconstruction. --outdir Output directory (default: current directory). Options: Option Description --min_readlength Minimum read length passed to PredictHaplo (default: 36). Settings: Option Description --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe predict_haplo corrected_1.fastq --fq2 corrected_2.fastq --ref_fa final.fna","title":"predict_haplo"},{"location":"hp_haplotype/#ph_parser","text":"Returns PredictHaplo output as a correctly formatted FASTA file. Input is the output file from predict_haplo (longest global .fas file). Output is a correctly formatted FASTA file. Usage: haphpipe ph_parser [OPTIONS] [SETTINGS] --haplotypes_fa best.fas [--outdir] (or): hp_ph_parser [OPTIONS] [SETTINGS] --haplotypes_fa best.fas [--outdir] Output files: ph_summary.txt ph_haplotypes.fna Input/Output Arguments: Option Description --haplotypes_fa Haplotype file created by PredictHaplo. --outdir Output directory (default: current directory). Options: Option Description --prefix Prefix to add to sequence names. --keep_gaps Do not remove gaps from alignment (default: False). Settings: Option Description --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. Example usage: haphpipe ph_parser PH01.best_1_864.fas Example: By default, PredictHaplo outputs their own unique version of a fasta file. It includes the frequency, some information regarding their unqiue overlapping scores, and their unique confidence scores. This file is always named PH#.best_#_#.fas , where the first number is the reconstructed haplotype number and the next numbers are the start and end of the longest haplotype reconstructed by PredictHaplo. $ cat PH01.best_1_1208.fas reconstructed_0 ;Freq:0.190638 ;Overlap quality scores (5,10):1,1 ;Confidence scores ;~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~n7~~~y~~~~~t~~~i~jjk~~zz ;~~~~{~~~~~N{~{~Sx~~~~~~z~K~F~~~~~~~y~~~~~~~|~~F~wx|~~{~~|~~s |]~~~~~~ ;~m~kj|{|v~{|_`~~~z~~~~~~~jy{y~~~~~a~__~~|~~~~~{wXZ~}~~~qm~xV~~~~~~~}~ ;Q~}~y||~~~}}~~~z~~~~~~{}A~}b|~~u~~|}}|~}}z~}bx~~n||~~||{~}~~d}~bz~~~} ;|~}}~~~}~~~{|}}g{~~~}~r~}}~~~~u~~{kx{~}~}~|~}~}{}~}~}||~~~~[~}~}}~~~~ ;~~}~~~U||U}~|}}~~}}~~~~}u~b|}~w~~~~~{}|wv~}Dxzp{|}~@~~P~}~}~V~~z}~|ry ;q~}|}~}~~}t~o~}~f}~~}{~~~}~{~~~~~~~}~~}~~|~~~M~~}~}x~}c~}^v~~~yzA~}~} ;y}~z}~~~~~~~~~{z~~}~~~}~{}~~~~~}~~~~~~~|~~v~~}~~|y~]|{~||~~~~~~~~||~~ ;Y||~~|Q~|~~~|~~~~~|~~~z~~z~{{{y~~~~~~~~~~w{{~wz|~Z|~z|~~}p|~~|}}~~x}} ;z}~}}|a|}}}}{}|~~}}~}}~}~{~|~}}~}{}|}|}}}~|}}}}{}}}|}}|}|}}}|}}}}~}}} ;||}}}}{}}~}~}}}}}}}}}}}~}}}}}}}}}}}}}}}}}y}~}}}}}}}}}}~|}}}}|}}}}|}}} ;}}}}}}}}|}}}}}}}}~}}}}}}|}}}}}}}}|}}}}}|}}}}|~|}}}}{}}}}}}}}}}}}}}}}} ;}}=}}}{}}}}}}}}}}}}}}}}}}}}}}}}~||}}|~}}}}}{}}}}}|}}|}}}}}}}}}}}}}|}} ;|}~}}}}|}}}}}}}}|}|~J}}}}}}~}}}}}}}}}}}}}}}}}}}}|}}`~}}}}|}}}}}}}}}}~ ;|}}}}}}}}}}}~}}}|}}}}}}}}}}|}}}}}}}}}}}t}}{}`}}}}}}|}~}~}|~}}}|k}}}}} ;|}|~}}|}}~}}~}|}}z}}}}}}}}}}}}}}~}}~}}}~~}|}}}~}}}}}}}}~}~}}||~~}~z}} ;~~}~}||~|~}|{}||~|z~~||}~|~}}|}~~}|}}~}}|}z~~~~{~}{}~y~~~~~{|}}~~|y~~ ;~|~~||~~~~~~~|n~~{~~~~~~~~~~~~~~~ ;EndOfComments ACCAAATGAAAGATTGTACTGAGAGACAGGCTAATTTTTTAGGGAATATCTGGCCTTCCCGCAAGGGAAG GCCAGGGAACTTTCCTCAGAGCAGGCTAGAGCCAACAGCCCCACCAGAAGAGAGCTTCAGGCTTGGGGAA GCAACAACAAAGCCCTCTCAGAAACAGGAGCCAATAGACAAGGAAATGTATCCTTTAACTTCCCTCAGAT CACTCTTTGGCAACGACCCCTTGTCACAATAAAGATAGGGGAGCAACTAAAGGAAGCCCTGTTAGATACA GGAGCAGATGATACAGTATTAGAAGAAATGAATTTGCCAGGAAGATGGAAACCAAGAATGATAGGGGGAA TCGGGGGTTTTATCAAAGTAAGACAGTATGATCAGATAGCCATAGACATCTGTGGCCATAAAGCTATAGG TACAGTATTAGTAGGACCTACACCTGTCAACATAATTGGAAGAAATCTGTTGACTCAGCTTGGTTGCACT TTAAATTTTCCCATTAGTCCTATTGAAACTGTACCAGTAAAATTGAAGCCAGGAATGGATGGCCCAAAGG TTAAACAATGGCCATTGACAGAAGAAAAAATAAAAGCATTAGTAGAAATTTGTACAGAAATGGAAAAAGA AGGGAAAATTTCAAAAATTGGGCCTGAAAATCCATACAATACTCCAGTATTTGCCATAAGGAAAAAAGAC AGTACTAAATGGAGAAAATTAGTAGATTTCAGAGAACTTAATAAGAGAACTCAGGACTTCTGGGAAGTCC AATTAGGAATACCACATCCCTCAGGGTTAAAAAAGAAAAAATCAGTAACAGTACTGGATGCGGGTGATGC ATATTTTTCAGTTCCCTTAGATGAAGACTTCAGAAAGTATACTGCATTTACCATACCTAGTGTAAACAAT GAGACACTAGGGATTAGGTATCAGTACAATGTGCTTCCACAAGGATGGAAAGGATCACCAGCAATATTCC AAGCTAGCATGACAAAAATCTTAGAGCCTTTTAGAAAACAAAATCCAGACATAGTTATCTATCAATACAT GGATGATCTGTATGTAGGATCTGACCTAGAAATAGGGCAGCATAGAACAAAAATAGAGGAACTGAGAGAA CATCTGTTGAGGTGGGGGTTTTGCACACCAGACAAGAAACATCAGAAGGAACCTCCATTCCTTTGGATGG GTTATGAACTCCATCCTT reconstructed_1 ;Freq:0.294104 ;Overlap quality scores (5,10):1,1 ;Confidence scores ;~~~~~~~~z~~~~~~~~~|~y~~~~u}|~~~~~}}~~~~~~~ya~|~~}}~~~y~~~j~YXH~~s~ ;~~~~z~~|~~b|}^}xZ}}~}z~t~r~{}~}x}}yb}~~~}~}u~~}~gu|}}{~|}}~ozsw}|}}h~ ;|l}v|^][t}zz}vw}}s}}}}~v}|~|~~}}}~~}}}}}~}}}}}}zo\\}}}~}vn}iv}}}}}}}}} ;v}}~|u}}}}}}}}}|}}}}}}|}^}}[}~}r}}}{|}}|t|}}{z}}pz}}{}}}}}}}v~}y|~}}} ;}}}}~}}~}}}tz}}`}}}~}}q}}}}}~}y}}|r||}}}}~}}~}}{}}}}}}}}}}}y~}}}}t}}} ;}}}}}}{|}|}}{r}}}}tv~}}}t}m|~}h}}}}}v}}qt}|y|{|u~}}|}~_}~|~~|}~|}~}|e ;Y}~}}~}}~}v}t}}}j}}~}}}~}}~|}}}}|}}|}}}~}}}}};}}~}}x}}|}}yu}}}wy`}|~~ ;{}}x}}}}}~}}}~{v}}}}}{}~}}~}~}}|}}}}}}}}}}u}}y~~{g}L}}}}}|~}{~|}|}|}} ;l}}}}{_}}}}~z~}|}~~|~~x|~}~z~~|~}~~~}~~~}y}~~|k}}Pf}w~~~~T}}}F~~}~z}{ ;{~|}|zZ~s~|}r~}}~~|}}}}}~|}}}}n}}}~}}}}~}}}}}}}}}}}|~}}}}}}~~}}~~~~~} ;N}}}}}|~}}~}}}}{}}}~}}|~~|}|}}}~|}~}}}}}}D}~}}}}}}}}}}}}}}}}|}}z}|}}} ;}k}}}}}}}}}}}}}}}}}}}|}}}}}|}}}}}|}}}}}|}}}}|~{~}}}G}~}}}}}~}}}}}}}}} ;}}h}}~|}|}}}}}}}}}}}}}}}}}}}}}}}}{}}|}}}}|~|}}}}}}}M}}}}}}}~}}}}}}}}} ;}}~|}}}}}~~~}}}~}}|}g}}}}}}}}~|}}}}}}}}}}}}}}}}}|}~t~}|}}}}}}}||~}}}} ;|}}}}~}}}}}}~}}}gi}}~}}}}}}}}}}}}|}}~}}g}~}}v}}}}}|{}}}}}}~}}~|e}}}}} ;}}|}}~}~}~}}~}|~}|}~|||}}}}|~}}~}}}}}}}}|}}~}}~}|~}|y~}~}}}}}|}}~~}~} ;}}~}~}|~}}}~}}}}~}{}}~}}~}}}||~~}||~}x|}{~|}|~~|~}}}|{~}}~}{}}||~}}}~ ;|~}~}}}~|}}~~|,~~~~~~~~~~~~~~m~~~ ;EndOfComments ACCAAATGAAAGATTGTACTGAGAGACAGGCTAATTTTTTAGGGAAGATCTGGCCTTCCCGCGGCGGAAG GCCAGGGAATTTTCTTCAGAGCAGACCAGAGCCAACAGCCCCACCAGAAGAGAGCTTCAGGTTTGGGGAG GAGACAACAACTCCCTCTCAGAAGCAGGAGCCGAAGGACAAGGAAACATATCCTTTAGCTTCCCTCAAAT CACTCTTTGGCAACGACCCCTTGTTACAATAAAGATAGGGGGGCAGCTAAAGGAAGCTCTATTAGATACA GGAGCAGATGACACAGTATTAGAAGAAATGGATTTGCCAGGAAGATGGAAACCAAAAATGATAGGGGGAA TTGGAGGTTTTATCAAAGTAAAACAGTATGATCAGATACCCATAGAAATTTGTGGACATAAAGTGATAGG TACAGTATTAGTAGGACCTACACCTGTCAACATAATTGGAAGAAATCTGTTGACTCAGCTTGGTTGCACT TTAAATTTTCCCATTAGTCCTATTGAAACTGTACCAGTAAAATTGAAGCCAGGAATGGATGGCCCAAAGG TTAAACAATGGCCATTGACAGAAGAAAAAATAAAAGCATTAATAGAAATCTGTGCAGAAATGGAAAAGGA AGGGAAAATTTCAAAAATTGGGCCTGAAAATCCATACAATACTCCAGTATTTGCCATAAGAAAAAAAGAC AGTACTAAATGGAGAAAATTAGTAGATTTCAAAGAACTTAATAAGAGAACTCAGGACTTCTGGGAAGTCC AATTAGGAATACCACATCCCTCAGGGTTAAAAAAGAAAAAGTCAGTAACAGTACTGGATGTGGGTGATGC ATATTTTTCAGTTCCCTTAGATGAAGACTTCAGAAAGTACACTGCATTTACCATACCTAGTGTAAACAAT GAGACACCAGGGATTAGGTATCAGTACAATGTGCTTCCACAAGGATGGAAAGGATCACCAGCAATATTCC AATGTAGCATGACAAAAATCTTAGAACCTTTTAGAAAACAAAATCCAGATATAGTTATCTATCAATACAT GGATGATCTGTATGTAGGATCTGACCTAGAAATAGGGCAGCATAGAACAAAAATAGAGGAACTGAGAGAA CATCTGTTGAGGTGGGGGTTTTGCACACCAGACAAGAAACATCAGAAGGAACCTCCATTCCTTTGGATGG GTTATGAACTCCATCCTT reconstructed_2 ;Freq:0.515259 ;Overlap quality scores (5,10):4,4 ;Confidence scores ;~~~~~~~w~~zz~~~~z~~w~|wy~|~~{|~~|~~~~~~~~}~{~hD~~~}|~|}}t~~_;~tue}}S} ;~}~~p}}l}~P}}ZlvL~z}}h~g~A~x}}}s}}{S~}~}}}}l}~|}SOq}}q}w}}~i\\pz~w~~z~ ;}7~|vzUu4|smtgm}~W}}~~}r}n`f`}{PZLSI:Vs_V_}}}}_ db}}}}}CQ}YJ}t}}}~}}} ;T}}~|}}}}}}}~}}k}}}}}}q}L}}]}}}L}}}tt|~}{q}}{M}lAIu}}}s{}}}~y~}st}|}} ;z}}}}|}}}}}{q|}Z}}}}}}P~}|}|}}E}}cLbi}vu}}|}}|}g}}}}}}|}~}~{}~}|}{}}} ;}~}|}}}{{|~}pp}~~~{|}}~}[yEx}}\\}}}{}N}uoK}}pauSy}}}v~~J}}q}}v~}c}~}\\i ;]|}}~}|}~}r}Y}}}B}}~||~}}}}{}}}|}|~|}~~~~}}~{w}{}~|R|}s|}k{~}}}ra}q}} ;o}}d}~}}}}}~}}|X~}}~}e}}}}}}}|}y|}}}}~}|~}c~~b~~nL~a}}}}}~}}m}}~{}}}} ;g}}}}uLm~l}}`~|n}}~}}|d~~y~u}|tz~~~~|}}~~t}}}tx}|Tu~v}}~}^|~~w~|~}m{} ;u}}}|w;}{}{}r}||}}z|}|~|}z}||}y}}}~}}}|}}~}}}|}y}}}}}}}~}||}}}}}}~}}| ;v{}|}}{}}}|~|}}|}}}}}}}}}}|}}}}}|}~}}}}}}p}}||}}}}}~}}}{}}}}}}}z}}}}} ;}|}}~}}}|}}}}}}}}}}}}}}}}}}}}}}|}||}}|}|}}}}|}|}}}}u}}}}}}}}}}}}}}}}} ;}}g}}}{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}y}}}}}|}m}}}}}}}}}}}}}}}}} ;}}}}}}}}}}}}}}}}}}|}i}}}}}}}}}}|}}}}}}}}}}}}}}}}}}}H}}}}}}}}}|}{}}}}} ;|}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}y}}|}H}}}}}}}}}}}}}}}}}}u}}}}} ;}}}}}}}}}}}}}}}}}z}}}}}}}}}}}}}}}}|}}}|}}}}}}}}}|}}}}}}}}}}}}|}}}}|}} ;}}}}}}}}}}}}||}|}}|}}}}}}|}}|}}}~}|}|}}}|}{}}}}|}}}}|{|}}}}x||}}}|{}} ;}}}}}}}~}}~}~|n}~}}}}}~}}~~|~~~y~, ;EndOfComments ACCAAATGAAAGATTGTACTGAGAGACAGGCTAATTTTTTAGGGAAAATCTGGCCTTCCCACAAGGGAAG GCCAGGGAATTTTCTTCAGAGCAGACCAGAGCCAACAGCCCCACCAGAAGAGAGCTTCAGGTTTGGGGAA GAGACAACAACTCCCTCTCAGAAGCAGGAGCCGATAGACAAGGAAATGTATCCTTTAGCTTCCCTCAAAT CACTCTTTGGCAACGACCCCTCGTCACAATAAAGATAGGGGGGCAACTAAAGGAAGCTCTATTAGATACA GGAGCAGATGATACAGTATTAGAAGAAATGAATTTGCCAGGAAGATGGAAACCAAAAATGATAGGGGGAA TTGGAGGTTTTATCAAAGTAAAACAGTATGATCAGATACCCATAGAAATCTGTGGACATAAAGCTATAGG TACAGTATTAGTAGGACCTACACCTGTCAACATAATTGGAAGAAATCTGTTGACTCAGATTGGTTGCACT TTAAATTTTCCCATTAGTCCTATTGAAACTGTACCAGTAAAATTAAAGCCAGGAATGGATGGCCCAAAAG TTAAACAATGGCCATTGACAGAAGAAAAAATAAAAGCATTAGTAGAAATTTGTACAGAAATGGAAAAGGA AGGGAAAATTTCAAAAATTGGGCCTGAAAATCCATACAATACTCCAGTATTTGCCATAAGGAAAAAAGAC AGTACTAAATGGAGAAAATTAGTAGATTTCAGAGAACTTAATAAGAGAACTCAAGACTTCCGGGAAGTCC AATTAGGAATACCACATCCCTCAGGGTTAAAAAAGAAAAAATCAGTAACAGTACTGGATGTGGGTGATGC ATATTTTTCAGTTCCCTTAGATGAAGACTTCAGAAAGTATACTGCATTTACCATACCTAGTGTAAACAAT GAGACACCAGGGATTAGGTATCAGTACAATGTGCTTCCACAAGGATGGAAAGGATCACCAGCAATATTCC AAGCTAGCATGACAAAAATCTTAGAGCCTTTTAGAAAACAAAATCCAGACATAGTTATCTATCAATACAT GGATGATCTGTATGTAGGATCTGACCTAGAAATAGGGCAGCATAGAACAAAAATAGAGGAACTGAGAGAA CATCTGTTGAGGTGGGGGTTTTGCACACCAGACAAGAAACATCAGAAGGAACCTCCATTCCTTTGGATGG GTTATGAACTCCATCCCC ph_parser takes this output and creates a proper fasta file with each resontructed haplotype and a text file that has the hpalotype diversity estimate. $ cat ph_summary.txt PH_num_hap 3 PH_hap_diversity 0.611668153059 PH_seq_len 1208 $ cat ph_haplotypes.fna reconstructed_0 Freq=0.190638 ACCAAATGAAAGATTGTACTGAGAGACAGGCTAATTTTTTAGGGAATATCTGGCCTTCCCGCAAGGGAAG GCCAGGGAACTTTCCTCAGAGCAGGCTAGAGCCAACAGCCCCACCAGAAGAGAGCTTCAGGCTTGGGGAA GCAACAACAAAGCCCTCTCAGAAACAGGAGCCAATAGACAAGGAAATGTATCCTTTAACTTCCCTCAGAT CACTCTTTGGCAACGACCCCTTGTCACAATAAAGATAGGGGAGCAACTAAAGGAAGCCCTGTTAGATACA GGAGCAGATGATACAGTATTAGAAGAAATGAATTTGCCAGGAAGATGGAAACCAAGAATGATAGGGGGAA TCGGGGGTTTTATCAAAGTAAGACAGTATGATCAGATAGCCATAGACATCTGTGGCCATAAAGCTATAGG TACAGTATTAGTAGGACCTACACCTGTCAACATAATTGGAAGAAATCTGTTGACTCAGCTTGGTTGCACT TTAAATTTTCCCATTAGTCCTATTGAAACTGTACCAGTAAAATTGAAGCCAGGAATGGATGGCCCAAAGG TTAAACAATGGCCATTGACAGAAGAAAAAATAAAAGCATTAGTAGAAATTTGTACAGAAATGGAAAAAGA AGGGAAAATTTCAAAAATTGGGCCTGAAAATCCATACAATACTCCAGTATTTGCCATAAGGAAAAAAGAC AGTACTAAATGGAGAAAATTAGTAGATTTCAGAGAACTTAATAAGAGAACTCAGGACTTCTGGGAAGTCC AATTAGGAATACCACATCCCTCAGGGTTAAAAAAGAAAAAATCAGTAACAGTACTGGATGCGGGTGATGC ATATTTTTCAGTTCCCTTAGATGAAGACTTCAGAAAGTATACTGCATTTACCATACCTAGTGTAAACAAT GAGACACTAGGGATTAGGTATCAGTACAATGTGCTTCCACAAGGATGGAAAGGATCACCAGCAATATTCC AAGCTAGCATGACAAAAATCTTAGAGCCTTTTAGAAAACAAAATCCAGACATAGTTATCTATCAATACAT GGATGATCTGTATGTAGGATCTGACCTAGAAATAGGGCAGCATAGAACAAAAATAGAGGAACTGAGAGAA CATCTGTTGAGGTGGGGGTTTTGCACACCAGACAAGAAACATCAGAAGGAACCTCCATTCCTTTGGATGG GTTATGAACTCCATCCTT reconstructed_1 Freq=0.294104 ACCAAATGAAAGATTGTACTGAGAGACAGGCTAATTTTTTAGGGAAGATCTGGCCTTCCCGCGGCGGAAG GCCAGGGAATTTTCTTCAGAGCAGACCAGAGCCAACAGCCCCACCAGAAGAGAGCTTCAGGTTTGGGGAG GAGACAACAACTCCCTCTCAGAAGCAGGAGCCGAAGGACAAGGAAACATATCCTTTAGCTTCCCTCAAAT CACTCTTTGGCAACGACCCCTTGTTACAATAAAGATAGGGGGGCAGCTAAAGGAAGCTCTATTAGATACA GGAGCAGATGACACAGTATTAGAAGAAATGGATTTGCCAGGAAGATGGAAACCAAAAATGATAGGGGGAA TTGGAGGTTTTATCAAAGTAAAACAGTATGATCAGATACCCATAGAAATTTGTGGACATAAAGTGATAGG TACAGTATTAGTAGGACCTACACCTGTCAACATAATTGGAAGAAATCTGTTGACTCAGCTTGGTTGCACT TTAAATTTTCCCATTAGTCCTATTGAAACTGTACCAGTAAAATTGAAGCCAGGAATGGATGGCCCAAAGG TTAAACAATGGCCATTGACAGAAGAAAAAATAAAAGCATTAATAGAAATCTGTGCAGAAATGGAAAAGGA AGGGAAAATTTCAAAAATTGGGCCTGAAAATCCATACAATACTCCAGTATTTGCCATAAGAAAAAAAGAC AGTACTAAATGGAGAAAATTAGTAGATTTCAAAGAACTTAATAAGAGAACTCAGGACTTCTGGGAAGTCC AATTAGGAATACCACATCCCTCAGGGTTAAAAAAGAAAAAGTCAGTAACAGTACTGGATGTGGGTGATGC ATATTTTTCAGTTCCCTTAGATGAAGACTTCAGAAAGTACACTGCATTTACCATACCTAGTGTAAACAAT GAGACACCAGGGATTAGGTATCAGTACAATGTGCTTCCACAAGGATGGAAAGGATCACCAGCAATATTCC AATGTAGCATGACAAAAATCTTAGAACCTTTTAGAAAACAAAATCCAGATATAGTTATCTATCAATACAT GGATGATCTGTATGTAGGATCTGACCTAGAAATAGGGCAGCATAGAACAAAAATAGAGGAACTGAGAGAA CATCTGTTGAGGTGGGGGTTTTGCACACCAGACAAGAAACATCAGAAGGAACCTCCATTCCTTTGGATGG GTTATGAACTCCATCCTT reconstructed_2 Freq=0.515259 ACCAAATGAAAGATTGTACTGAGAGACAGGCTAATTTTTTAGGGAAAATCTGGCCTTCCCACAAGGGAAG GCCAGGGAATTTTCTTCAGAGCAGACCAGAGCCAACAGCCCCACCAGAAGAGAGCTTCAGGTTTGGGGAA GAGACAACAACTCCCTCTCAGAAGCAGGAGCCGATAGACAAGGAAATGTATCCTTTAGCTTCCCTCAAAT CACTCTTTGGCAACGACCCCTCGTCACAATAAAGATAGGGGGGCAACTAAAGGAAGCTCTATTAGATACA GGAGCAGATGATACAGTATTAGAAGAAATGAATTTGCCAGGAAGATGGAAACCAAAAATGATAGGGGGAA TTGGAGGTTTTATCAAAGTAAAACAGTATGATCAGATACCCATAGAAATCTGTGGACATAAAGCTATAGG TACAGTATTAGTAGGACCTACACCTGTCAACATAATTGGAAGAAATCTGTTGACTCAGATTGGTTGCACT TTAAATTTTCCCATTAGTCCTATTGAAACTGTACCAGTAAAATTAAAGCCAGGAATGGATGGCCCAAAAG TTAAACAATGGCCATTGACAGAAGAAAAAATAAAAGCATTAGTAGAAATTTGTACAGAAATGGAAAAGGA AGGGAAAATTTCAAAAATTGGGCCTGAAAATCCATACAATACTCCAGTATTTGCCATAAGGAAAAAAGAC AGTACTAAATGGAGAAAATTAGTAGATTTCAGAGAACTTAATAAGAGAACTCAAGACTTCCGGGAAGTCC AATTAGGAATACCACATCCCTCAGGGTTAAAAAAGAAAAAATCAGTAACAGTACTGGATGTGGGTGATGC ATATTTTTCAGTTCCCTTAGATGAAGACTTCAGAAAGTATACTGCATTTACCATACCTAGTGTAAACAAT GAGACACCAGGGATTAGGTATCAGTACAATGTGCTTCCACAAGGATGGAAAGGATCACCAGCAATATTCC AAGCTAGCATGACAAAAATCTTAGAGCCTTTTAGAAAACAAAATCCAGACATAGTTATCTATCAATACAT GGATGATCTGTATGTAGGATCTGACCTAGAAATAGGGCAGCATAGAACAAAAATAGAGGAACTGAGAGAA CATCTGTTGAGGTGGGGGTTTTGCACACCAGACAAGAAACATCAGAAGGAACCTCCATTCCTTTGGATGG GTTATGAACTCCATCCCC","title":"ph_parser"},{"location":"hp_reads/","text":"hp_reads involves cleaning up the raw read sequences, as well as other processing steps. Modules to manipulate reads. Use -h after any command for a list of options. sample_reads Subsample reads using seqtk ( documentation ). Input is reads in FASTQ format. Output is sampled reads in FASTQ format. You do not have to have all read options (i.e., read1, read2 AND unpaired reads). You can have a combination of any of those. Usage: haphpipe sample_reads [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ [--outdir] (or): hp_sample_reads [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ [--outdir] Output files: sample_1.fastq sample_2.fastq Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --outdir Output directory (default: current directory). Options: Option Description --nreads Number of reads to sample. If greater than the number of reads in file, all reads will be sampled. --frac Fraction of reads to sample, between 0 and 1. Each read has [frac] --seed Seed for random number generator. Settings: Option Description --quiet Do not write output to console (silence stdout and stderr), default is False. --logfile Append console output to this file. --debug Print commands but do not run, default is False. Example usage: This pulls 1000 reads from these paired end files with a starting seed of 1234. haphpipe sample_reads --fq1 read_1.fastq --fq2 read_2.fastq --nreads 1000 --seed 1234 -- trim_reads Trim reads using Trimmomatic ( documentation ). Input is reads in FASTQ format. Output is trimmed reads in FASTQ format. Usage: haphpipe trim_reads [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ [--outdir] (or): hp_trim_reads [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ [--outdir] Output files: trimmed_1.fastq trimmed_2.fastq trimmed_U.fastq trimmomatic_summary.out Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --outdir Output directory (default: current directory). Options: Option Description --adapter_file Adapter file. --trimmers Trim commands for trimmomatic (default: ['LEADING:3', 'TRAILING:3', 'SLIDINGWINDOW:4:15', 'MINLEN:36']). --encoding Quality score encoding. Settings: Option Description --ncpu Number of CPU to use (default: 1). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: This trims paired end read files 1 and 2. haphpipe trim_reads --fq1 read_1.fastq --fq2 read_2.fastq -- join_reads Join reads using FLASH ( paper ). Input is reads in FASTQ format. Output is joined reads in FASTQ format. Usage: haphpipe join_reads [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ [--outdir] (or): hp_join_reads [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ [--outdir] Output files: joined.fastq notjoined_1.fastq notjoined_2.fastq Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --outdir Output directory (default: current directory). Settings: Option Description --min_overlap The minimum required overlap length between two reads to provide a confident overlap (default: 10). --max_overlap Maximum overlap length expected in approximately 90% of read pairs, longer overlaps are penalized. --allow_outies Also try combining read pairs in the \"outie\" orientation (default: False). --encoding Quality score encoding. Settings: Option Description --ncpu Number of CPU to use (default: 1). --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe join_reads --fq1 trimmed_1.fastq --fq2 trimmed_2.fastq ec_reads Error correction using SPAdes ( documentation ). Input is reads in FASTQ format. Output is error-corrected reads in FASTQ format. Remember that HAPHPIPE is intended for Illumina reads, therefore the error correction is based on Illumina sequencing errors. Usage: haphpipe ec_reads [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ [--outdir] (or): hp_ec_reads [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ [--outdir] Output files: corrected_1.fastq corrected_2.fastq corrected_U.fastq Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --outdir Output directory (default: current directory). Settings: Option Description --ncpu Number of CPU to use (default: 1). --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run, default is False. Example usage: haphpipe ec_reads --fq1 trimmed_1.fastq --fq2 trimmed_2.fastq","title":"Reads"},{"location":"hp_reads/#sample_reads","text":"Subsample reads using seqtk ( documentation ). Input is reads in FASTQ format. Output is sampled reads in FASTQ format. You do not have to have all read options (i.e., read1, read2 AND unpaired reads). You can have a combination of any of those. Usage: haphpipe sample_reads [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ [--outdir] (or): hp_sample_reads [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ [--outdir] Output files: sample_1.fastq sample_2.fastq Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --outdir Output directory (default: current directory). Options: Option Description --nreads Number of reads to sample. If greater than the number of reads in file, all reads will be sampled. --frac Fraction of reads to sample, between 0 and 1. Each read has [frac] --seed Seed for random number generator. Settings: Option Description --quiet Do not write output to console (silence stdout and stderr), default is False. --logfile Append console output to this file. --debug Print commands but do not run, default is False. Example usage: This pulls 1000 reads from these paired end files with a starting seed of 1234. haphpipe sample_reads --fq1 read_1.fastq --fq2 read_2.fastq --nreads 1000 --seed 1234 --","title":"sample_reads"},{"location":"hp_reads/#trim_reads","text":"Trim reads using Trimmomatic ( documentation ). Input is reads in FASTQ format. Output is trimmed reads in FASTQ format. Usage: haphpipe trim_reads [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ [--outdir] (or): hp_trim_reads [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ [--outdir] Output files: trimmed_1.fastq trimmed_2.fastq trimmed_U.fastq trimmomatic_summary.out Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --outdir Output directory (default: current directory). Options: Option Description --adapter_file Adapter file. --trimmers Trim commands for trimmomatic (default: ['LEADING:3', 'TRAILING:3', 'SLIDINGWINDOW:4:15', 'MINLEN:36']). --encoding Quality score encoding. Settings: Option Description --ncpu Number of CPU to use (default: 1). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: This trims paired end read files 1 and 2. haphpipe trim_reads --fq1 read_1.fastq --fq2 read_2.fastq --","title":"trim_reads"},{"location":"hp_reads/#join_reads","text":"Join reads using FLASH ( paper ). Input is reads in FASTQ format. Output is joined reads in FASTQ format. Usage: haphpipe join_reads [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ [--outdir] (or): hp_join_reads [OPTIONS] [SETTINGS] --fq1 FASTQ --fq2 FASTQ [--outdir] Output files: joined.fastq notjoined_1.fastq notjoined_2.fastq Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --outdir Output directory (default: current directory). Settings: Option Description --min_overlap The minimum required overlap length between two reads to provide a confident overlap (default: 10). --max_overlap Maximum overlap length expected in approximately 90% of read pairs, longer overlaps are penalized. --allow_outies Also try combining read pairs in the \"outie\" orientation (default: False). --encoding Quality score encoding. Settings: Option Description --ncpu Number of CPU to use (default: 1). --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). Example usage: haphpipe join_reads --fq1 trimmed_1.fastq --fq2 trimmed_2.fastq","title":"join_reads"},{"location":"hp_reads/#ec_reads","text":"Error correction using SPAdes ( documentation ). Input is reads in FASTQ format. Output is error-corrected reads in FASTQ format. Remember that HAPHPIPE is intended for Illumina reads, therefore the error correction is based on Illumina sequencing errors. Usage: haphpipe ec_reads [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ [--outdir] (or): hp_ec_reads [SETTINGS] --fq1 FASTQ --fq2 FASTQ --fqU FASTQ [--outdir] Output files: corrected_1.fastq corrected_2.fastq corrected_U.fastq Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --outdir Output directory (default: current directory). Settings: Option Description --ncpu Number of CPU to use (default: 1). --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run, default is False. Example usage: haphpipe ec_reads --fq1 trimmed_1.fastq --fq2 trimmed_2.fastq","title":"ec_reads"},{"location":"inout/","text":"Module Input Input Format Output Output File Names sample_reads FASTQ file(s) .fastq.gz or .fastq or .fq Subampled FASTQ file(s) sample_1.fastq sample_2.fastq trim_reads FASTQ file(s) .fastq.gz or .fastq or .fq Trimmed FASTQ file(s) and summary from Trimmomatic text file trimmed_1.fastq trimmed_2.fastq trimmed_U.fastq trimmomatic_summary.out join_reads FASTQ file(s) .fastq.gz or .fastq or .fq FASTQ files for joined reads and unjoined reads joined.fastq notjoined_1.fastq notjoined_2.fastq ec_reads FASTQ file(s) .fastq.gz or .fastq or .fq Error corrected FASTQ file(s) corrected_1.fastq corrected_2.fastq corrected_U.fastq assemble_denovo FASTQ file(s) .fastq.gz or .fastq or .fq De novo assembled contigs FASTA file and de novo summary text file denovo_contigs.fna denovo_summary.txt assemble_amplicons FASTA files and GTF file .fna or .fasta or .fa and .gtf FASTA file with assembled amplicons amplicon_assembly.fna assemble_scaffold FASTA files .fna or .fasta or .fa FASTA files with scaffolded and aligned sequences and FASTA file with assembled amplicons scaffold_aligned.fa scaffold_assembly.fa scaffold_imputed.fa scaffold_padded.out align_reads FASTQ files and FASTA file .fastq.gz or .fastq or .fq and .fna or .fasta or .fa Aligned BAM file and Bowtie2 alignment output summary text file aligned.bam aligned.bt2.out call_variants BAM and FASTA file .bam and .fna or .fasta or .fa VCF file with variants variants.vcf.gz vcf_to_consensus VCF file .vcf FASTA file with consensus sequence consensus.fna refine_assembly FASTQ files and FASTA file .fastq.gz or .fastq or .fq and .fna or .fasta or .fa FASTA file with refined consensus sequence refined.fna finalize_assembly FASTQ files and FASTA file .fastq.gz or .fastq or .fq and .fna or .fasta or .fa FASTA file with final refined consensus sequence and final BAM file with reads aligned to final FASTA file and VCF file with variants relative to final.fna final.fna and final.bam and final.vcf.gz predict_haplo FASTQ files and FASTA file .fastq.gz or .fastq or .fq and .fna or .fasta or .fa PredictHaplo's fasta-like output file best.fa ph_parser PredictHaplo's FAS output file .fas Summary text file with haplotype diversity statistics and FASTA file with haplotype sequences ph_summary.txt and ph_haplotypes.fna pairwise_align FASTA files and GTF file .fna or .fasta or .fa and .gtf JSON file pairwise_aligned.json extract_pairwise JSON file from pairwise_align output .json FASTA output with region extracted to standard out stdout.fasta annotate_from_ref JSON file from pairwise_align output and GTF file .json and .gtf","title":"File Input/Output"},{"location":"install/","text":"HAPHPIPE Installation Instructions HAPHIPE depends on more than a dozen different programs, each of which may itself depend on other programs and libraries. Installing everything separately would be a nightmare, so you should use the package manager \"Bioconda\" to install everything. Bioconda is a popular package manager in the bioinformatics world. See the Helpful Resources section for more information and resources for Bioconda. Here, we will describe where to get Bioconda and how to install it, then how to use Bioconda to install the necessary programs for HAPHPIPE. We will also detail the acquisition and installation of one program, GTAK, that is not handled by Bioconda, and finally the acquisition and installation of HAPHPIPE itself. Here, we describe the procedure for installing HAPHPIPE using the package manager Bioconda (Gr\u00fcning et al. 2018) on the command line. If you are unfamiliar with Bioconda, please see for installation and channel setup. HAPHPIPE is available here and is written in Python 3.7.2 coding language. The installation process begins with the creation of a conda environment that installs the necessary dependencies required by HAPHPIPE. Once the conda environment has been created, it can be activated for use with the command conda activate haphpipe. Due to license restrictions, Bioconda cannot distribute and install GATK (McKenna et al. 2010; Van der Auwera et al. 2013; Poplin et al. 2018) directly. To fully install GATK, you must download a licensed copy of GATK (version 3.8-0) from the Broad Institute (link) . You can then install HAPHPIPE using the single command pip install git+git://github.com/gwcbi/haphpipe.git , which pulls the repository from Github. 1. Install conda Download the conda package: wget https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-x86_64.sh sh Miniconda3-latest-MacOSX-x86_64.sh 2. Set up conda channels These need to be put in as a command in the order they come, as this sets the priority for where packages are pulled from. Therefore, in this order, conda-forge is top priority and explored first when downloading a program. conda config --add channels R conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge 3. Create a conda environment with the following dependencies conda create -n haphpipe \\ python=3.7 \\ future \\ pyyaml \\ biopython \\ seqtk \\ bowtie2 \\ bwa \\ flash \\ freebayes \\ mummer \\ picard \\ trimmomatic \\ samtools=1.9 \\ gatk=3.8 \\ spades \\ blast \\ sierrapy Note: on some HPC systems (including GW's Pegasus), certain dependencies must be installed as a 'User Install'. If this is the case (which will be apparent if you receive availability errors for any of the above packages after installing them through conda) use the following command to install the package: pip install --user package name ```` __4. Activate the environment__ conda activate haphpipe __5. Install GATK__ Due to license restrictions, bioconda cannot distribute and install GATK directly. To fully install GATK, you must download a licensed copy of GATK (version 3.8-0) from the Broad Institute: [https://software.broadinstitute.org/gatk/download/archive](https://software.broadinstitute.org/gatk/download/archive). Register the package using gatk3-register: gatk3-register /path/to/GenomeAnalysisTK-3.8-0-ge9d806836.tar.bz2 This will copy GATK into your conda environment. Alternatively, GATK may be installed directly on the command line with the following commands: ```bash mkdir -p /path/to/gatk_dir wget -O - 'https://software.broadinstitute.org/gatk/download/auth?package=GATK-archive version=3.6-0-g89b7209' | tar -xjvC /path/to/gatk_dir gatk3-register /path/to/gatk_dir/GenomeAnalysisTK.jar NOTE: HAPHPIPE was developed and tested using GATK 3.8. 6. Install HAPHPIPE pip install git+git://github.com/gwcbi/haphpipe.git Upon completion of the installation, you can test it to ensure the repository has been installed completely and correctly by running haphpipe -h (in quick start). Once HAPHPIPE is installed and performing correctly, there is no need to install it again; simply activate the conda environment when needed by executing conda activate haphpipe . If a new version is released in the future, HAPHPIPE can be updated by running the command line pip install --upgrade git+git://github.com/gwcbi/haphpipe.git . At any point, the -h option that follows any HAPHPIPE stage will output a help message that provides a description of the stage and the desired input(s) and output(s). PredictHaplo Installation Instructions Users are required to download PredictHaplo on their own computing system prior to running any of the haplotype stages ( hp_predict_haplo and hp_ph_parser ). Here is how the GW CBI team installed PredictHaplo onto our HPC, which has a slurm scheduling system and uses Lua module files. We cannot help with the installation of this software, but have provided the code that we used here to install PredictHaplo onto our system. Please see their website for contact information if needed. This module loads predicthaplo onto GWU's HPC - Colonial One. See https://bmda.dmi.unibas.ch/software.html cd /path/to/modules/predicthaplo # use gcc 4.9.4, add blas and lapack to library path module load gcc/4.9.4 module load blas/gcc/64 module load lapack/gcc/64 # download source cd archive wget https://bmda.dmi.unibas.ch/software/PredictHaplo-Paired-0.4.tgz cd .. # unzip source and change directory name tar xfvz archive/PredictHaplo-Paired-0.4.tgz mv PredictHaplo-Paired-0.4 0.4 cd 0.4 # install scythestat tar xfvz scythestat-1.0.3.tar.gz cd scythestat-1.0.3 ./configure --prefix=/path/to/modules/predicthaplo/0.4/NEWSCYTHE make install cd .. # compile predicthaplo make If a segfault error occurs during the hp_predict_haplo stage, this is not a characteristic of HAPHPIPE but rather that of PredictHaplo. Sometimes, we have luck if we just rerun the code again or move to an interactive CPU node. We are unsure what causes this error, and we only see it between the local and global reconstruction phases in PredictHaplo. Quick start 1. Activate haphpipe Make sure you have conda running. For students at GW using Colonial One, you need to load the miniconda3 module like such prior to activating the haphpipe conda environemnt: module load miniconda3 . conda activate haphpipe 2. Test that it is loaded correctly haphpipe -h should produce: Program: haphpipe (haplotype and phylodynamics pipeline) Version: 0.8.1 Commands: -- Reads sample_reads subsample reads using seqtk trim_reads trim reads using Trimmomatic join_reads join reads using FLASh ec_reads error correct reads using SPAdes -- Assemble assemble_denovo assemble reads denovo assemble_amplicons assemble contigs to amplicon regions assemble_scaffold assemble contigs to genome align_reads align reads to reference call_variants call variants vcf_to_consensus create consensus sequence from VCF refine_assembly iterative refinement: align - variants - consensus finalize_assembly finalize consensus sequence -- Haplotype predict_haplo assemble haplotypes with PredictHaplo ph_parser parse output from PredictHaplo. -- Annotate pairwise_align align consensus to an annotated reference extract_pairwise extract sequence regions from pairwise alignment annotate_from_ref annotate consensus from reference annotation -- Miscellaneous demo setup demo directory and test data Directory Structure Below is the recommended directory structure for using HAPHPIPE: We recommend creating a separate directory for each sample, as well as a directory for reference files. Windows Users HAPHPIPE is only available for Mac OSX or Linux platforms. We suggest the following options for running HAPHPIPE on a Windows machine: Run HAPHPIPE on your institution's HPC cluster, if available. Utilize the Windows Subsystem for Linux . Run Linux in a virtual machine via VirtualBox","title":"Installation"},{"location":"install/#haphpipe-installation-instructions","text":"HAPHIPE depends on more than a dozen different programs, each of which may itself depend on other programs and libraries. Installing everything separately would be a nightmare, so you should use the package manager \"Bioconda\" to install everything. Bioconda is a popular package manager in the bioinformatics world. See the Helpful Resources section for more information and resources for Bioconda. Here, we will describe where to get Bioconda and how to install it, then how to use Bioconda to install the necessary programs for HAPHPIPE. We will also detail the acquisition and installation of one program, GTAK, that is not handled by Bioconda, and finally the acquisition and installation of HAPHPIPE itself. Here, we describe the procedure for installing HAPHPIPE using the package manager Bioconda (Gr\u00fcning et al. 2018) on the command line. If you are unfamiliar with Bioconda, please see for installation and channel setup. HAPHPIPE is available here and is written in Python 3.7.2 coding language. The installation process begins with the creation of a conda environment that installs the necessary dependencies required by HAPHPIPE. Once the conda environment has been created, it can be activated for use with the command conda activate haphpipe. Due to license restrictions, Bioconda cannot distribute and install GATK (McKenna et al. 2010; Van der Auwera et al. 2013; Poplin et al. 2018) directly. To fully install GATK, you must download a licensed copy of GATK (version 3.8-0) from the Broad Institute (link) . You can then install HAPHPIPE using the single command pip install git+git://github.com/gwcbi/haphpipe.git , which pulls the repository from Github. 1. Install conda Download the conda package: wget https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-x86_64.sh sh Miniconda3-latest-MacOSX-x86_64.sh 2. Set up conda channels These need to be put in as a command in the order they come, as this sets the priority for where packages are pulled from. Therefore, in this order, conda-forge is top priority and explored first when downloading a program. conda config --add channels R conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge 3. Create a conda environment with the following dependencies conda create -n haphpipe \\ python=3.7 \\ future \\ pyyaml \\ biopython \\ seqtk \\ bowtie2 \\ bwa \\ flash \\ freebayes \\ mummer \\ picard \\ trimmomatic \\ samtools=1.9 \\ gatk=3.8 \\ spades \\ blast \\ sierrapy Note: on some HPC systems (including GW's Pegasus), certain dependencies must be installed as a 'User Install'. If this is the case (which will be apparent if you receive availability errors for any of the above packages after installing them through conda) use the following command to install the package: pip install --user package name ```` __4. Activate the environment__ conda activate haphpipe __5. Install GATK__ Due to license restrictions, bioconda cannot distribute and install GATK directly. To fully install GATK, you must download a licensed copy of GATK (version 3.8-0) from the Broad Institute: [https://software.broadinstitute.org/gatk/download/archive](https://software.broadinstitute.org/gatk/download/archive). Register the package using gatk3-register: gatk3-register /path/to/GenomeAnalysisTK-3.8-0-ge9d806836.tar.bz2 This will copy GATK into your conda environment. Alternatively, GATK may be installed directly on the command line with the following commands: ```bash mkdir -p /path/to/gatk_dir wget -O - 'https://software.broadinstitute.org/gatk/download/auth?package=GATK-archive version=3.6-0-g89b7209' | tar -xjvC /path/to/gatk_dir gatk3-register /path/to/gatk_dir/GenomeAnalysisTK.jar NOTE: HAPHPIPE was developed and tested using GATK 3.8. 6. Install HAPHPIPE pip install git+git://github.com/gwcbi/haphpipe.git Upon completion of the installation, you can test it to ensure the repository has been installed completely and correctly by running haphpipe -h (in quick start). Once HAPHPIPE is installed and performing correctly, there is no need to install it again; simply activate the conda environment when needed by executing conda activate haphpipe . If a new version is released in the future, HAPHPIPE can be updated by running the command line pip install --upgrade git+git://github.com/gwcbi/haphpipe.git . At any point, the -h option that follows any HAPHPIPE stage will output a help message that provides a description of the stage and the desired input(s) and output(s).","title":"HAPHPIPE Installation Instructions"},{"location":"install/#predicthaplo-installation-instructions","text":"Users are required to download PredictHaplo on their own computing system prior to running any of the haplotype stages ( hp_predict_haplo and hp_ph_parser ). Here is how the GW CBI team installed PredictHaplo onto our HPC, which has a slurm scheduling system and uses Lua module files. We cannot help with the installation of this software, but have provided the code that we used here to install PredictHaplo onto our system. Please see their website for contact information if needed. This module loads predicthaplo onto GWU's HPC - Colonial One. See https://bmda.dmi.unibas.ch/software.html cd /path/to/modules/predicthaplo # use gcc 4.9.4, add blas and lapack to library path module load gcc/4.9.4 module load blas/gcc/64 module load lapack/gcc/64 # download source cd archive wget https://bmda.dmi.unibas.ch/software/PredictHaplo-Paired-0.4.tgz cd .. # unzip source and change directory name tar xfvz archive/PredictHaplo-Paired-0.4.tgz mv PredictHaplo-Paired-0.4 0.4 cd 0.4 # install scythestat tar xfvz scythestat-1.0.3.tar.gz cd scythestat-1.0.3 ./configure --prefix=/path/to/modules/predicthaplo/0.4/NEWSCYTHE make install cd .. # compile predicthaplo make If a segfault error occurs during the hp_predict_haplo stage, this is not a characteristic of HAPHPIPE but rather that of PredictHaplo. Sometimes, we have luck if we just rerun the code again or move to an interactive CPU node. We are unsure what causes this error, and we only see it between the local and global reconstruction phases in PredictHaplo.","title":"PredictHaplo Installation Instructions"},{"location":"install/#quick-start","text":"1. Activate haphpipe Make sure you have conda running. For students at GW using Colonial One, you need to load the miniconda3 module like such prior to activating the haphpipe conda environemnt: module load miniconda3 . conda activate haphpipe 2. Test that it is loaded correctly haphpipe -h should produce: Program: haphpipe (haplotype and phylodynamics pipeline) Version: 0.8.1 Commands: -- Reads sample_reads subsample reads using seqtk trim_reads trim reads using Trimmomatic join_reads join reads using FLASh ec_reads error correct reads using SPAdes -- Assemble assemble_denovo assemble reads denovo assemble_amplicons assemble contigs to amplicon regions assemble_scaffold assemble contigs to genome align_reads align reads to reference call_variants call variants vcf_to_consensus create consensus sequence from VCF refine_assembly iterative refinement: align - variants - consensus finalize_assembly finalize consensus sequence -- Haplotype predict_haplo assemble haplotypes with PredictHaplo ph_parser parse output from PredictHaplo. -- Annotate pairwise_align align consensus to an annotated reference extract_pairwise extract sequence regions from pairwise alignment annotate_from_ref annotate consensus from reference annotation -- Miscellaneous demo setup demo directory and test data","title":"Quick start"},{"location":"install/#directory-structure","text":"Below is the recommended directory structure for using HAPHPIPE: We recommend creating a separate directory for each sample, as well as a directory for reference files.","title":"Directory Structure"},{"location":"install/#windows-users","text":"HAPHPIPE is only available for Mac OSX or Linux platforms. We suggest the following options for running HAPHPIPE on a Windows machine: Run HAPHPIPE on your institution's HPC cluster, if available. Utilize the Windows Subsystem for Linux . Run Linux in a virtual machine via VirtualBox","title":"Windows Users"},{"location":"modules/","text":"hp_reads Stages to manipulate reads. Use -h after any command for a list of options. sample_reads Subsample reads using seqtk ( seqtk documentation ). Output files: sample_1.fastq sample_2.fastq Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --outdir Output directory (default: current directory). Sample Options: Option Description --nreads Number of reads to sample. If greater than the number of reads in file, all reads will be sampled. --frac Fraction of reads to sample, between 0 and 1. Each read has [frac] probability of being sampled, so the number of sampled reads is not precisely [frac * number of reads]. --seed Seed for random number generator. Settings: Option Description --quiet Do not write output to console (silence stdout and stderr), default is False. --logfile Append console output to this file. --debug Print commands but do not run, default is False. trim_reads Trim reads using Trimmomatic ( Trimmomatic documentation ). Output files: trimmed_1.fastq trimmed_2.fastq trimmed_U.fastq trimmomatic_summary.out Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --outdir Output directory (default: current directory). Trimmomatic Options: Option Description --adapter_file Adapter file. --trimmers Trim commands for trimmomatic (default: ['LEADING:3', 'TRAILING:3', 'SLIDINGWINDOW:4:15', 'MINLEN:36']). --encoding Quality score encoding. Settings: Option Description --ncpu Number of CPU to use (default: 1). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). join_reads Join reads using FLASH ( FLASH documentation ). Output files: joined.fastq notjoined_1.fastq notjoined_2.fastq Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --outdir Output directory (default: current directory). FLASH Settings: Option Description --min_overlap The minimum required overlap length between two reads to provide a confident overlap (default: 10). --max_overlap Maximum overlap length expected in approximately 90% of read pairs, longer overlaps are penalized. --allow_outies Also try combining read pairs in the \"outie\" orientation (default: False). --encoding Quality score encoding. Settings: Option Description --ncpu Number of CPU to use (default: 1). --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). ec_reads Error correct reads using spades ( Spades documentation ). Output files: corrected_1.fastq corrected_2.fastq corrected_U.fastq Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --outdir Output directory (default: current directory). Settings: Option Description --ncpu Number of CPU to use (default: 1). --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run, default is False. hp_assemble Stages to assemble consensus sequence(s). Use -h after any command for a list of options. assemble_denovo Assemble reads using denovo assembly. Output files: denovo_contigs.fna denovo_summary.txt Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --outdir Output directory (default: current directory). Assembly Options: Option Description --no_error_correction Do not perform error correction (default: False) --subsample Use a subsample of reads for assembly --seed Seed for random number generator (ignored if not subsampling) Settings: Option Description --ncpu Number of CPU to use (default: 1). --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). assemble_amplicons Assemble contigs using reference and amplicon regions. Output files: amplicon_assembly.fna Input/Output Arguments: Option Description --contigs_fa Fasta file with assembled contigs. --ref_fa Fasta file with reference genome to scaffold against. --ref_gtf GTF format file containing amplicon regions. --outdir Output directory (default: current directory). Scaffold Options: Option Description --sample_id Sample ID (default: sampleXX). --padding Bases to include outside reference annotation (default: 50). Settings: Option Description --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). assemble_scaffold Scaffold contigs using reference sequence. Output files: scaffold_aligned.fa scaffold_assembly.fa scaffold_imputed.fa scaffold_padded.out Input/Output Arguments: Option Description --contigs_fa Fasta file with assembled contigs. --ref_fa Fasta file with reference genome to scaffold against. --outdir Output directory (default: current directory). Scaffold Options: Option Description --seqname Name to append to scaffold sequence (default: sample01). Settings: Option Description --keep_tmp Additional options (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). align_reads Align reads to reference. Output files: aligned.bam aligned.bt2.out Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --ref_fa. Reference fasta file. --outdir Output directory (default: current directory). Alignment Options: Option Description --bt2_preset {very-fast, fast, sensitive,very-sensitive,very-fast-local,fast-local,sensitive-local,very-sensitive-local} --sample_id Sample ID. Used as read group ID in BAM (default: sampleXX). --no_realign Do not realign indels (default: False). --remove_duplicates Remove duplicates from final alignment. Otherwise duplicates are marked but not removed (default: False). --encoding {Phred+33,Phred+64} Quality score encoding. Settings: Option Description --ncpu Number of CPUs to use (default: 1). --xmx Maximum heap size for Java VM, in GB (default: 32). --keep_tmp Additional options (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). call_variants Call variants. Output files: variants.vcf.gz Input/Output Arguments: Option Description --aln_bam Alignment file. --ref_fa Reference fasta file. --outdir Output directory (default: False). Variant Calling Options: Option Description --emit_all Output calls for all site (default: False). --min_base_qual Minimum base quality required to consider a base for calling (default: 15). Settings: Option Description --ncpu Number of CPUs to use (default: 1). --xmx Maximum heap size for Java VM, in GB (default: 32). --keep_tmp Additional options (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). vcf_to_consensus Create consensus sequence from VCF. Output files: consensus.fna Input/Output Arguments: Option Description --vcf VCF file (created with all sites). --outdir Output directory (default: False). --sampidx Index for sample if multi-sample VCF (default: 0). Variant Options: Option Description --min_DP Minimum depth to call site (default: 1). --major Allele fraction to make unambiguous call (default: 0.5). --minor Allele fraction to make ambiguous call (default: 0.2). Settings: Option Description --keep_tmp Additional options (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. refine_assembly Three step assembly refinement: align reads, call variants, and update reference. Output files: refined.fna Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --ref_fa. Reference fasta file. --outdir Output directory (default: False). Refinement Options: Option Description --max_step Maximum number of refinement steps (default: 1). --subsample Use a subsample of reads for refinement. --seed Seed for random number generator (ignored if not subsampling). --sample_id Sample ID. Used as read group ID in BAM (default: sampleXX). Settings: Option Description --ncpu Number of CPUs to use (default: 1). --xmx Maximum heap size for Java VM, in GB (default: 32). --keep_tmp Additional options (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). finalize_assembly Finalize consensus sequence, align all reads to consensus, and call variants in dataset. Output files: final.fna final.ban final.vcf.gz Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --ref_fa Consensus fasta file. --outdir Output directory (default: current directory). Consensus Options: Option Description --bt2_preset {very-fast,fast,sensitive,very-sensitive,very-fast-local,fast-local,sensitive-local,very-sensitive-local} Bowtie2 preset to use (default: very-sensitive). --sample_id Sample ID (default: sampleXX). Settings: Option Description --ncpu Number of CPU to use (default: 1). --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). hp_annotate Stages to annotate consensus sequence(s). Use -h after any command for a list of options. pairwise_align Align amplicons to reference. Output files: pairwise_aligned.json Input/Output Arguments: Option Description --amplicons_fa Fasta file with assembled amplicons. --ref_fa Reference fasta file. --ref_gtf GTF format file containing amplicon regions. Primary and alternate coding regions should be provided in the attribute field (for amino acid alignment). --outdir Output directory (default: False). Settings: Option Description --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False). extract_pairwise Extract sequence regions from pairwise alignment. Output files: stdout.fasta Input/Output Arguments: Option Description --align_json JSON file describing alignment (output of pairwise_align stage). --outfile Output file (default: stdout). Options: Option Description --outfmt Format for output: nuc_fa, aln_fa, amp_gtf, ost, or prot_fa (default: nuc_fa). --refreg Reference region. String format is ref:start-stop. For example, the region string to extract pol when aligned to HXB2 is HIV_B.K03455.HXB2:2085-5096. Settings: Option Description --debug Print commands but do not run (default: False). hp_annotate_from_ref Extract sequence regions from pairwise alignment. Output files: Input/Output Arguments: Option Description --align_json JSON file describing alignment (output of pairwise_align stage). --ref_gtf GTF format file containing amplicon regions. --outfile Output file (default: stdout). Settings: Option Description --debug Print commands but do not run (default: False).","title":"Modules"},{"location":"modules/#hp_reads","text":"Stages to manipulate reads. Use -h after any command for a list of options.","title":"hp_reads"},{"location":"modules/#sample_reads","text":"Subsample reads using seqtk ( seqtk documentation ). Output files: sample_1.fastq sample_2.fastq Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --outdir Output directory (default: current directory). Sample Options: Option Description --nreads Number of reads to sample. If greater than the number of reads in file, all reads will be sampled. --frac Fraction of reads to sample, between 0 and 1. Each read has [frac] probability of being sampled, so the number of sampled reads is not precisely [frac * number of reads]. --seed Seed for random number generator. Settings: Option Description --quiet Do not write output to console (silence stdout and stderr), default is False. --logfile Append console output to this file. --debug Print commands but do not run, default is False.","title":"sample_reads"},{"location":"modules/#trim_reads","text":"Trim reads using Trimmomatic ( Trimmomatic documentation ). Output files: trimmed_1.fastq trimmed_2.fastq trimmed_U.fastq trimmomatic_summary.out Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --outdir Output directory (default: current directory). Trimmomatic Options: Option Description --adapter_file Adapter file. --trimmers Trim commands for trimmomatic (default: ['LEADING:3', 'TRAILING:3', 'SLIDINGWINDOW:4:15', 'MINLEN:36']). --encoding Quality score encoding. Settings: Option Description --ncpu Number of CPU to use (default: 1). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False).","title":"trim_reads"},{"location":"modules/#join_reads","text":"Join reads using FLASH ( FLASH documentation ). Output files: joined.fastq notjoined_1.fastq notjoined_2.fastq Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --outdir Output directory (default: current directory). FLASH Settings: Option Description --min_overlap The minimum required overlap length between two reads to provide a confident overlap (default: 10). --max_overlap Maximum overlap length expected in approximately 90% of read pairs, longer overlaps are penalized. --allow_outies Also try combining read pairs in the \"outie\" orientation (default: False). --encoding Quality score encoding. Settings: Option Description --ncpu Number of CPU to use (default: 1). --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False).","title":"join_reads"},{"location":"modules/#ec_reads","text":"Error correct reads using spades ( Spades documentation ). Output files: corrected_1.fastq corrected_2.fastq corrected_U.fastq Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --outdir Output directory (default: current directory). Settings: Option Description --ncpu Number of CPU to use (default: 1). --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run, default is False.","title":"ec_reads"},{"location":"modules/#hp_assemble","text":"Stages to assemble consensus sequence(s). Use -h after any command for a list of options.","title":"hp_assemble"},{"location":"modules/#assemble_denovo","text":"Assemble reads using denovo assembly. Output files: denovo_contigs.fna denovo_summary.txt Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --outdir Output directory (default: current directory). Assembly Options: Option Description --no_error_correction Do not perform error correction (default: False) --subsample Use a subsample of reads for assembly --seed Seed for random number generator (ignored if not subsampling) Settings: Option Description --ncpu Number of CPU to use (default: 1). --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False).","title":"assemble_denovo"},{"location":"modules/#assemble_amplicons","text":"Assemble contigs using reference and amplicon regions. Output files: amplicon_assembly.fna Input/Output Arguments: Option Description --contigs_fa Fasta file with assembled contigs. --ref_fa Fasta file with reference genome to scaffold against. --ref_gtf GTF format file containing amplicon regions. --outdir Output directory (default: current directory). Scaffold Options: Option Description --sample_id Sample ID (default: sampleXX). --padding Bases to include outside reference annotation (default: 50). Settings: Option Description --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False).","title":"assemble_amplicons"},{"location":"modules/#assemble_scaffold","text":"Scaffold contigs using reference sequence. Output files: scaffold_aligned.fa scaffold_assembly.fa scaffold_imputed.fa scaffold_padded.out Input/Output Arguments: Option Description --contigs_fa Fasta file with assembled contigs. --ref_fa Fasta file with reference genome to scaffold against. --outdir Output directory (default: current directory). Scaffold Options: Option Description --seqname Name to append to scaffold sequence (default: sample01). Settings: Option Description --keep_tmp Additional options (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False).","title":"assemble_scaffold"},{"location":"modules/#align_reads","text":"Align reads to reference. Output files: aligned.bam aligned.bt2.out Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --ref_fa. Reference fasta file. --outdir Output directory (default: current directory). Alignment Options: Option Description --bt2_preset {very-fast, fast, sensitive,very-sensitive,very-fast-local,fast-local,sensitive-local,very-sensitive-local} --sample_id Sample ID. Used as read group ID in BAM (default: sampleXX). --no_realign Do not realign indels (default: False). --remove_duplicates Remove duplicates from final alignment. Otherwise duplicates are marked but not removed (default: False). --encoding {Phred+33,Phred+64} Quality score encoding. Settings: Option Description --ncpu Number of CPUs to use (default: 1). --xmx Maximum heap size for Java VM, in GB (default: 32). --keep_tmp Additional options (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False).","title":"align_reads"},{"location":"modules/#call_variants","text":"Call variants. Output files: variants.vcf.gz Input/Output Arguments: Option Description --aln_bam Alignment file. --ref_fa Reference fasta file. --outdir Output directory (default: False). Variant Calling Options: Option Description --emit_all Output calls for all site (default: False). --min_base_qual Minimum base quality required to consider a base for calling (default: 15). Settings: Option Description --ncpu Number of CPUs to use (default: 1). --xmx Maximum heap size for Java VM, in GB (default: 32). --keep_tmp Additional options (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False).","title":"call_variants"},{"location":"modules/#vcf_to_consensus","text":"Create consensus sequence from VCF. Output files: consensus.fna Input/Output Arguments: Option Description --vcf VCF file (created with all sites). --outdir Output directory (default: False). --sampidx Index for sample if multi-sample VCF (default: 0). Variant Options: Option Description --min_DP Minimum depth to call site (default: 1). --major Allele fraction to make unambiguous call (default: 0.5). --minor Allele fraction to make ambiguous call (default: 0.2). Settings: Option Description --keep_tmp Additional options (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file.","title":"vcf_to_consensus"},{"location":"modules/#refine_assembly","text":"Three step assembly refinement: align reads, call variants, and update reference. Output files: refined.fna Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --ref_fa. Reference fasta file. --outdir Output directory (default: False). Refinement Options: Option Description --max_step Maximum number of refinement steps (default: 1). --subsample Use a subsample of reads for refinement. --seed Seed for random number generator (ignored if not subsampling). --sample_id Sample ID. Used as read group ID in BAM (default: sampleXX). Settings: Option Description --ncpu Number of CPUs to use (default: 1). --xmx Maximum heap size for Java VM, in GB (default: 32). --keep_tmp Additional options (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False).","title":"refine_assembly"},{"location":"modules/#finalize_assembly","text":"Finalize consensus sequence, align all reads to consensus, and call variants in dataset. Output files: final.fna final.ban final.vcf.gz Input/Output Arguments: Option Description --fq1 Fastq file with read 1. --fq2 Fastq file with read 2. --fqU Fastq file with unpaired reads. --ref_fa Consensus fasta file. --outdir Output directory (default: current directory). Consensus Options: Option Description --bt2_preset {very-fast,fast,sensitive,very-sensitive,very-fast-local,fast-local,sensitive-local,very-sensitive-local} Bowtie2 preset to use (default: very-sensitive). --sample_id Sample ID (default: sampleXX). Settings: Option Description --ncpu Number of CPU to use (default: 1). --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False).","title":"finalize_assembly"},{"location":"modules/#hp_annotate","text":"Stages to annotate consensus sequence(s). Use -h after any command for a list of options.","title":"hp_annotate"},{"location":"modules/#pairwise_align","text":"Align amplicons to reference. Output files: pairwise_aligned.json Input/Output Arguments: Option Description --amplicons_fa Fasta file with assembled amplicons. --ref_fa Reference fasta file. --ref_gtf GTF format file containing amplicon regions. Primary and alternate coding regions should be provided in the attribute field (for amino acid alignment). --outdir Output directory (default: False). Settings: Option Description --keep_tmp Keep temporary directory (default: False). --quiet Do not write output to console (silence stdout and stderr) (default: False). --logfile Append console output to this file. --debug Print commands but do not run (default: False).","title":"pairwise_align"},{"location":"modules/#extract_pairwise","text":"Extract sequence regions from pairwise alignment. Output files: stdout.fasta Input/Output Arguments: Option Description --align_json JSON file describing alignment (output of pairwise_align stage). --outfile Output file (default: stdout). Options: Option Description --outfmt Format for output: nuc_fa, aln_fa, amp_gtf, ost, or prot_fa (default: nuc_fa). --refreg Reference region. String format is ref:start-stop. For example, the region string to extract pol when aligned to HXB2 is HIV_B.K03455.HXB2:2085-5096. Settings: Option Description --debug Print commands but do not run (default: False).","title":"extract_pairwise"},{"location":"modules/#hp_annotate_from_ref","text":"Extract sequence regions from pairwise alignment. Output files: Input/Output Arguments: Option Description --align_json JSON file describing alignment (output of pairwise_align stage). --ref_gtf GTF format file containing amplicon regions. --outfile Output file (default: stdout). Settings: Option Description --debug Print commands but do not run (default: False).","title":"hp_annotate_from_ref"},{"location":"phylo/","text":"hp_phylo includes phylogenomics stages. multiple_align Align consensus sequences using MAFFT ( documentation ). Input can be a list of directories which contain final.fna files or a fasta file, or both (in which case the sequences in the FASTA file are combined with the final.fna files retreived before the alignment. Sequences will be separated by amplicons using a supplied GTF file before alignment (unless the --alignall option is specified). This module may also be used to separate files by amplicons (without aligning) by specifying the --fastaonly option. Alignments are by default outputted as FASTA files, although PHYLIP ( --phylipout ) or CLUSTAL ( --clustalout ) output options are also available. Many options from MAFFT are available in this module. Please refer to the MAFFT documentation above for information about these options. Usage: haphpipe multiple_align [MAFFT OPTIONS] [HAPHPIPE OPTIONS] --seqs FASTA --dir_list TXT --ref_gtf GTF [--outdir] (or): hp_multiple_align [MAFFT OPTIONS] [HAPHPIPE OPTIONS] --seqs FASTA --dir_list TXT --ref_gtf GTF [--outdir] Output files: alignment files in FASTA format (default), one per amplicon (or one alignment.fasta file if using --alignall option) Note: MAFFT stores intermediate files in a temporary directory located in /tmp. More information is available here . Input/Output Arguments: Option Description --seqs SEQS FASTA file with sequences to be aligned --dir_list DIR_LIST List of directories which include a final.fna file,one on each line --ref_gtf REF_GTF Reference GTF file --out_align OUT_ALIGN Name for alignment file --nuc Assume nucleotide (default: False) --amino Assume amino (default: False) --clustalout Clustal output format (default: False) --phylipout PHYLIP output format (default: False) --inputorder Output order same as input (default: False) --reorder Output order aligned (default: False) --treeout Guide tree is output to the input.tree file (default:False) --quiet_mafft Do not report progress (default: False) --outdir OUTDIR Output directory MAFFT Options: Option Description --algo ALGO Use different algorithm in command: linsi, ginsi, einsi, fftnsi, fftns, nwns, nwnsi --auto Automatically select algorithm (default: False) --sixmerpair Calculate distance based on shared 6mers, on by default (default: False) --globalpair Use Needleman-Wunsch algorithm (default: False) --localpair Use Smith-Waterman algorithm (default: False) --genafpair Use local algorithm with generalized affine gap cost (default: False) --fastapair Use FASTA for pairwise alignment (default: False) --weighti WEIGHTI Weighting factor for consistency term --retree RETREE Number of times to build guide tree --maxiterate MAXITERATE Number of cycles for iterative refinement --noscore Do not check alignment score in iterative alignment (default: False) --memsave Use Myers-Miller algorithm (default: False) --parttree Use fast tree-building method with 6mer distance (default: False) --dpparttree Use PartTree algorithm with distances based on DP (default: False) --fastaparttree Use PartTree algorithm with distances based on FASTA (default: False) --partsize PARTSIZE Number of partitions for PartTree --groupsize GROUPSIZE Max number of sequences for PartTree MAFFT Parameters: Option Description --lop LOP Gap opening penalty --lep LEP Offset value --lexp LEXP Gap extension penalty --LOP LOP Gap opening penalty to skip alignment --LEXP LEXP Gap extension penalty to skip alignment --bl BL BLOSUM matrix: 30, 45, 62, or 80 --jtt JTT JTT PAM number 0 --tm TM Transmembrane PAM number 0 --aamatrix AAMATRIX Path to user-defined AA scoring matrix --fmodel Incorporate AA/nuc composition info into scoring matrix (default: False) Options: Option Description --ncpu NCPU Number of CPU to use (default: 1) --quiet Do not write output to console (silence stdout and stderr) (default: False) --logfile LOGFILE Name for log file (output) --debug Print commands but do not run (default: False) --fastaonly Output fasta files separated by region but do not align (default: False) --alignall Do not separate files by region, align entire file (default: False) Example usage: haphpipe multiple_align --dir_list demo_sra_list.txt --ref_gtf HIV_B.K03455.HXB2.gtf --phylipout --logfile demo_multiple_align.log build_tree Phylogeny reconstruction with RAxML ( documentation ). Input is an alignment (FASTA or PHYLIP format). Output is a tree file (TRE format). Usage: haphpipe build_tree [RAxML OPTIONS] [HAPHPIPE OPTIONS] --seqs FASTA --output_name TXT [--outdir] (or): hpbuild_tree [RAxML OPTIONS] [HAPHPIPE OPTIONS] --seqs FASTA --output_name TXT [--outdir] Output files: Input/Output Arguments: RAxML Options: Options:","title":"Phylo"},{"location":"phylo/#multiple_align","text":"Align consensus sequences using MAFFT ( documentation ). Input can be a list of directories which contain final.fna files or a fasta file, or both (in which case the sequences in the FASTA file are combined with the final.fna files retreived before the alignment. Sequences will be separated by amplicons using a supplied GTF file before alignment (unless the --alignall option is specified). This module may also be used to separate files by amplicons (without aligning) by specifying the --fastaonly option. Alignments are by default outputted as FASTA files, although PHYLIP ( --phylipout ) or CLUSTAL ( --clustalout ) output options are also available. Many options from MAFFT are available in this module. Please refer to the MAFFT documentation above for information about these options. Usage: haphpipe multiple_align [MAFFT OPTIONS] [HAPHPIPE OPTIONS] --seqs FASTA --dir_list TXT --ref_gtf GTF [--outdir] (or): hp_multiple_align [MAFFT OPTIONS] [HAPHPIPE OPTIONS] --seqs FASTA --dir_list TXT --ref_gtf GTF [--outdir] Output files: alignment files in FASTA format (default), one per amplicon (or one alignment.fasta file if using --alignall option) Note: MAFFT stores intermediate files in a temporary directory located in /tmp. More information is available here . Input/Output Arguments: Option Description --seqs SEQS FASTA file with sequences to be aligned --dir_list DIR_LIST List of directories which include a final.fna file,one on each line --ref_gtf REF_GTF Reference GTF file --out_align OUT_ALIGN Name for alignment file --nuc Assume nucleotide (default: False) --amino Assume amino (default: False) --clustalout Clustal output format (default: False) --phylipout PHYLIP output format (default: False) --inputorder Output order same as input (default: False) --reorder Output order aligned (default: False) --treeout Guide tree is output to the input.tree file (default:False) --quiet_mafft Do not report progress (default: False) --outdir OUTDIR Output directory MAFFT Options: Option Description --algo ALGO Use different algorithm in command: linsi, ginsi, einsi, fftnsi, fftns, nwns, nwnsi --auto Automatically select algorithm (default: False) --sixmerpair Calculate distance based on shared 6mers, on by default (default: False) --globalpair Use Needleman-Wunsch algorithm (default: False) --localpair Use Smith-Waterman algorithm (default: False) --genafpair Use local algorithm with generalized affine gap cost (default: False) --fastapair Use FASTA for pairwise alignment (default: False) --weighti WEIGHTI Weighting factor for consistency term --retree RETREE Number of times to build guide tree --maxiterate MAXITERATE Number of cycles for iterative refinement --noscore Do not check alignment score in iterative alignment (default: False) --memsave Use Myers-Miller algorithm (default: False) --parttree Use fast tree-building method with 6mer distance (default: False) --dpparttree Use PartTree algorithm with distances based on DP (default: False) --fastaparttree Use PartTree algorithm with distances based on FASTA (default: False) --partsize PARTSIZE Number of partitions for PartTree --groupsize GROUPSIZE Max number of sequences for PartTree MAFFT Parameters: Option Description --lop LOP Gap opening penalty --lep LEP Offset value --lexp LEXP Gap extension penalty --LOP LOP Gap opening penalty to skip alignment --LEXP LEXP Gap extension penalty to skip alignment --bl BL BLOSUM matrix: 30, 45, 62, or 80 --jtt JTT JTT PAM number 0 --tm TM Transmembrane PAM number 0 --aamatrix AAMATRIX Path to user-defined AA scoring matrix --fmodel Incorporate AA/nuc composition info into scoring matrix (default: False) Options: Option Description --ncpu NCPU Number of CPU to use (default: 1) --quiet Do not write output to console (silence stdout and stderr) (default: False) --logfile LOGFILE Name for log file (output) --debug Print commands but do not run (default: False) --fastaonly Output fasta files separated by region but do not align (default: False) --alignall Do not separate files by region, align entire file (default: False) Example usage: haphpipe multiple_align --dir_list demo_sra_list.txt --ref_gtf HIV_B.K03455.HXB2.gtf --phylipout --logfile demo_multiple_align.log","title":"multiple_align"},{"location":"phylo/#build_tree","text":"Phylogeny reconstruction with RAxML ( documentation ). Input is an alignment (FASTA or PHYLIP format). Output is a tree file (TRE format). Usage: haphpipe build_tree [RAxML OPTIONS] [HAPHPIPE OPTIONS] --seqs FASTA --output_name TXT [--outdir] (or): hpbuild_tree [RAxML OPTIONS] [HAPHPIPE OPTIONS] --seqs FASTA --output_name TXT [--outdir] Output files: Input/Output Arguments: RAxML Options: Options:","title":"build_tree"}]}